text,maj_label,doccano_art_id,sentence_id,label_anno1,comments_anno1,label_anno2,comments_anno2,label_anno3,comments_anno3,label_anno4,comments_anno4,previous_sentence_section,previous_sentence,next_sentence_section,next_sentence
"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",POS,0,2,POS,j'hésite entre POS et FACT,POS,,POS,,,,abstract,"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",abstract,"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing."
"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.",FACT,0,3,FACT,,FACT,,POS,,,,abstract,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",Introduction,"One of the problems relating to sign language recognition is the lack of appropriate datasets for algorithm training, since most datasets are recorded for academic purposes and as such, they concentrate in human learning rather than machine learning."
"This way we can extract a dataset with a lot of examples for every handshape, palm orientation and hand location out of the video collections.",POS,0,8,POS,pas sûre,NC,,POS,,,,Introduction,"Thus, we developed a system in the direction of ""phonological"" features recognition.",Datasets,For the purposes of the project two collections of single gloss videos were used as datasets.
"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.",FACT,1,4,FACT,,FACT,,POS,,,,abstract,This degrades translation accuracy.,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points."
1,NC,1,6,NC,problème phrase,NC,,NC,,,,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.",Introduction,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008)."
"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",NC,1,10,NC,,NC,,NC,"Ce n'est pas un claim as such, mais je me rends compte qu'en citant de cette manière, on influence la suite alors qu'on ne parle pas de la recherche menée, mais de l'état de l'art",,,Introduction,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",Introduction,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015)."
"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",NC,1,22,NC,,NC,"analyse tirée d'un exemple (example = not a claim), mais servant de base à une hypothèse de travail",NC,,,,Introduction,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",Introduction,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting."
"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",POS,1,23,POS,,POS,,PROSP,Hypothèse =? prospective ?,,,Introduction,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",Introduction,"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b)."
"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",FACT,1,26,FACT,,NC,"précisions sur la méthode développée, déjà introduite précédemment, mais on reste dans l'introduction ...",FACT,,,,Introduction,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",Introduction,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4)."
"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",NC,1,27,FACT,,NC,,NC,,,,Introduction,"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2)."
"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",POS,1,29,FACT#POS,"""we perform expe..."" = FACT (?)",FACT#POS,,POS,,,,Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",Introduction,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario."
"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",POS,1,30,FACT#POS,,FACT#POS,,POS,,,,Introduction,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",Synchronous Context-Free Grammars,"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007)."
"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",NC,1,38,NC#NEG,,NC,,,,,,Synchronous Context-Free Grammars,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",Synchronous Context-Free Grammars,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007)."
"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",NC,1,60,NC,difficile de savoir si c'est une connaissance établie ou une nouvelle hypothèse de leur part (sans connaissance du domaine),POS,claim implicite ?,,,,,Triangulation with Syntactic Matching,"In the previous section, we explained about the standard triangulation method and mentioned that the pivot-side ambiguity causes incorrect estimation of translation probability and the translation accuracy might decrease.",Triangulation with Syntactic Matching,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching."
"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",POS,1,66,POS,,NC,,,,,,Exact Matching of Parse Subtrees,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",Exact Matching of Parse Subtrees,Therefore it is important to create TMs that are both reliabile and have high coverage.
Therefore it is important to create TMs that are both reliabile and have high coverage.,POS,1,67,POS,,NC,,,,,,Exact Matching of Parse Subtrees,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",Partial Matching of Parse Subtrees,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees."
"Cascade is one immediate method connecting two TMs, and NMT cascade translation shows the medium performance in this experiment.",POS,1,112,POS,,NC,,,,,,Comparison with Neural MT:,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training.",Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker."
"This may be because we used only 1 LSTM layer for encoder/decoder, or because the amount of parallel corpora or language pairs were not sufficient.",NEG,1,114,NEG,,POS,"cause d'un résultat, et non sa conséquence (conséquence directe d'un résultat = claim), quand même considéré comme une analyse directe de résultat ?",,,,,Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker.",Comparison with Neural MT:,"Thus, we can posit that while zero-shot translation has demonstrated reasonable results in some settings, successful zero-shot translation systems are far from trivial to build, and pivot-based symbolic MT systems such as PBMT or Hiero may still be a competitive alternative."
"To estimate translation probabilities, we introduced heuristic that has no guarantee to be optimal.",FACT,1,118,FACT,,FACT#NEG,,,,,,Conclusion,"In experiments, we found that these triangulated models are effective in particular when allowing partial matching.",Conclusion,"Therefore in the future, we plan to explore more refined estimation methods that utilize machine learning."
"The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture.",NC,2,3,NC,,POS,working hypothesis,,,,,abstract,"This graph is used to prune the action space, enabling more efficient exploration.",abstract,"In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives."
We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.,NC,2,5,NC,,FACT,link to code/data -> to keep in FACT ? could be useful for an overview of the contributions,,,,,abstract,"In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives.",Introduction,Natural language communication can be used to affect change in the real world.
We introduce three contributions to text-based game playing to deal with the combinatorially large state and action spaces.,NC,2,18,NC,,FACT,,,,,,Introduction,"Such games have historically proven to be difficult to play for AI agents, and the more complex variants such as Zork still remain firmly out of the reach of existing approaches.",Introduction,"First, we show that a state representation in the form of a knowledge graph gives us the ability to effectively prune an action space."
The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.,NC,2,21,NC,,POS,,,,,,Introduction,A knowledge graph captures the relationships between entities as a directed graph.,Introduction,"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair."
"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair.",FACT,2,22,FACT,,FACT#POS,,,,,,Introduction,The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.,Introduction,"This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018;Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs."
"This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018;Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs.",NC,2,23,NC,,POS,,,,,,Introduction,"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair.",Introduction,"Finally, we take initial steps toward framing the POMDP as a questionanswering (QA) problem wherein a knowledgegraph can be used to not only prune actions but to answer the question of what action is most appropriate."
We provide results on ablative experiments comparing our knowledge-graph based approach approaches to strong baselines.,NC,2,27,NC,,FACT,,,,,,Introduction,We show how pre-training certain parts of our KG-DQN network using existing QA methods improves performance and allows knowledge to be transferred from different games.,Introduction,Results show that incorporating a knowledge-graph into a reinforcement learning agent results in converges to the highest reward more than 40% faster than the best baseline.
"In our approach, our agent learns a knowledge graph, stored as a set of RDF triples, i.e.",NC,2,50,NC,phrase incomplète,NC,,,,,,Knowledge Graph DQN,"In this section we introduce our knowledge graph representation, action pruning and deep Qnetwork architecture.",Knowledge Graph Representation,"3-tuples of subject, relation, object ."
"For example, from a phrase such as ""There is an exit to the north"" one can infer a has relation between the current The knowledge graph is updated after every agent action (see Figure 1).",NC,2,54,NC,1re partie est une phrase coupée ?,NC,,,,,,Knowledge Graph Representation,OpenIE is not optimized to the regularities of text adventure games and there are a lot of relations that can be inferred from the typical structure of descriptive texts.,Knowledge Graph Representation,The update rules are defined such that there are portions of the graph offering short and long-term context.
We intend for the update rules to be applied to text-based games in different domains and so only hand-craft a minimal set of rules that we believe apply generally.,POS,2,58,POS,,PROSP,,,,,,Knowledge Graph Representation,Other relations persist after each action.,Knowledge Graph Representation,They are: • Linking the current room type (e.g.
Treating the problem as question-answering will not replace the need for exploration in text-adventure games.,POS,2,106,POS,,NEG,,,,,,Game Play as Question Answering,"When appropriately trained, the agent may be able to answer the question for itself and select a good next move to execute.",Game Play as Question Answering,"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration."
"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.",POS,2,107,POS,,PROSP,,,,,,Game Play as Question Answering,Treating the problem as question-answering will not replace the need for exploration in text-adventure games.,Game Play as Question Answering,"To teach the agent to answer the question of what action is best to take given an observation, we use an offline, pre-training approach."
"To teach the agent to answer the question of what action is best to take given an observation, we use an offline, pre-training approach.",NC,2,108,NC,,FACT,,,,,,Game Play as Question Answering,"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.",Game Play as Question Answering,"The data for the pre-training approach is generated using an oracle, an agent capable of finishing a game perfectly in the least number of steps possible."
"We then use the DrQA (Chen et al., 2017) question-answering technique to train a paired question encoder and an answer encoder that together predict the answer (action) from the question (text observation).",NC,2,113,NC,,FACT,,,,,,Game Play as Question Answering,and the oracle's correct action is the answer.,Game Play as Question Answering,The weights from the SB-LSTM in the document encoder in the DrQA system are then used to initialize the weights of the SB-LSTM.
Our approach to question-answering in the context of text adventure game playing thus represents a form of transfer learning.,NC,2,122,NC,,FACT,,,,,,Game Play as Question Answering,"For pre-training to work, the agent must develop a general question-answering competence that can transfer to new quests.",Experiments,"We conducted experiments in the TextWorld framework (Côté et al., 2018) using their ""home"" theme."
Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map.,NC,2,167,NC,,POS,,,,,,Results and Discussion,"To help benchmark our agent's performance, we observed people unaffiliated with the research playing through the same TextWorld ""home"" quests as the other models.",Results and Discussion,"When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible."
"When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible.",NC,2,168,NC,,POS,,,,,,Results and Discussion,Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map.,Results and Discussion,Also note that none of the deep reinforcement learning agents received instructions.
We don't show BOW-DQN because it is strictly inferior to LSTM-DQN in all situations).,POS,2,171,POS,,NC,,,,,,Results and Discussion,"On both small and large maps, all versions of KG-DQN tested converge faster than baselines (see Figure 3 for the small game and Figure 4 for the large game).",Results and Discussion,KG-DQN converges 40% faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five.
"On the large game, no Table 3: Average number of steps (and standard deviation) taken to complete the small game.",POS,2,173,POS,légende de table en plein milieu/coupe phrase,NC,,,,,,Results and Discussion,KG-DQN converges 40% faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five.,Results and Discussion,"agents achieve the maximum reward of 10, and the LSTM-DQN requires more than 300 episodes to converge at the same level as KG-DQN."
"Differences between LSTM-DQN and full KG-DQN are not statistically significant, p = 0.199 on an independent Ttest.",POS,2,178,POS,,NEG#POS,,,,,,Results and Discussion,Full KG-DQN requires an equivalent number of steps in the small game (Table 3) and in the large game (Table 4).,Results and Discussion,The ablated versions of KG-DQN-unpruned KG-DQN and non-pre-trained KG-DQN-require many more steps to complete quests.
TextWorld's reward function allows for a lot of exploration of the environment without penalty so it is possible for a model that has converged on reward to complete quests in as few as five steps or in many hundreds of steps.,NC,2,180,NC,,POS,,,,,,Results and Discussion,The ablated versions of KG-DQN-unpruned KG-DQN and non-pre-trained KG-DQN-require many more steps to complete quests.,Results and Discussion,"From these results, we conclude that the pre-training using our questionanswering paradigm is allowing the agent to find a general understanding of how to pick good actions even when the agent has never seen the final"
"While the knowledge graph allows the agent to reach optimal reward more quickly, it doesn't ensure a high quality solution to quests.",POS,2,184,POS,,NEG,,,,,,Conclusions,We speculate that this is because the knowledge graph provides a persistent memory of the world as it is being explored.,Conclusions,Action pruning using the knowledge graph and pre-training of the embeddings used in the deep Q-network result in shorter action sequences needed to complete quests.
The insight into pre-training portions of the agent's architecture is based on converting textadventure game playing into a question-answering activity.,NC,2,186,NC,,POS,,,,,,Conclusions,Action pruning using the knowledge graph and pre-training of the embeddings used in the deep Q-network result in shorter action sequences needed to complete quests.,Conclusions,"That is, at every step, the agent is asking-and trying to answer-what is the most important thing to try."
"The pre-training acts as a form of transfer learning from different, but re-lated games.",NC,2,188,NC,,POS,,,,,,Conclusions,"That is, at every step, the agent is asking-and trying to answer-what is the most important thing to try.",Conclusions,"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required."
"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.",POS,2,189,POS,,NEG,,,,,,Conclusions,"The pre-training acts as a form of transfer learning from different, but re-lated games.",Conclusions,"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language."
"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language.",POS,2,190,POS,,PROSP,,,,,,Conclusions,"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.",Conclusions,"Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language."
"Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language.",POS,2,191,POS,,PROSP,,,,,,Conclusions,"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language.",,
"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",POS,3,0,POS,,NC,,,,,,,,abstract,"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation."
"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation.",POS,3,1,POS,,NC,,,,,,abstract,"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",abstract,"In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time."
"We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.",FACT,3,4,FACT,,POS,,,,,,abstract,"Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process.",INTRODUCTION,"Online automatic speech recognition (ASR), which immediately recognizes incomplete speeches as humans do, is emerging as a core element of diverse ASR-based services such as teleconferences, AI secretaries, or AI booking services."
"Published in ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 6-11 June 2021 in Toronto, Ontario, Canada.",NC,3,8,NC,ligne de conf,NC,,,,,,INTRODUCTION,"However, of course, online ASR models [1,2] targeting real-time inference have concerns about performance degradation compared to traditional Copyright 2021 IEEE.",INTRODUCTION,Personal use of this material is permitted.
The performance degradation with small occurs since accessible input information is very limited and training models with small restricts head diversity severely.,NC,3,97,NC,,NC,explication d'un résultat,,,,,Trade-off between Performance and Latency,Our model outperforms baselines and is still faster than MMA without HSD even though there are small increases in relative latency compared to HeadDrop except for the case with extremely small text .,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small ."
We suggest the method to learn alignments with considering other heads' alignments by modifying expected alignments for all the heads of each layer to select an input frame within a fixed size window.,POS,3,99,POS,,FACT,,,,,,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small .",CONCLUSION,Our approach improves performance with only a small increase in latency by regularizing the intra-layer difference of boundaries effectively from the training phase.
"However, Tanaka (2011) ar gues against Merchant's dichotomy in voice mis match between VP ellipsis and Pseudogapping, reporting that voice mismatch in both types of e llipsis is permissible or not while interacting wi th what Kehler (2000) calls discourse coherence relations between ellipsis and antecedent clause s. Departing from Kehler's (2000) insight, we s uggest that vP undergoes ellipsis in a resemblan ce discourse relation, but VP does so in a cause/ effect discourse relation.",NC,4,3,NC,,POS,caractères espace --> pb au niveau du XML ?,,,,,abstract,"reports that VP ellipsis as an elision o f smaller size VP allows voice mismatch, but Ps eudogapping and Sluicing as an elision of bigge r size vP/TP do not.",abstract,"Given the asymmetry i n the size of ellipsis in tandem with discourse re lations, we argue that since Accusative as well as Nominative Case is checked outside VP, the VP to be elided can meet the identity condition on ellipsis with its antecedent VP as the object element in the former and the subject one in the latter or vice versus have not been Case-checke d yet, thus being identical in terms of Case-feat ure at the point of derivation building a VP."
This paper examines the very issue of voice mismatch in the above three types of ellipsis in English.,NC,4,14,NC,,FACT,,,,,,Introduction,"(3) Pseudogapping *Roses were brought by some, and others did bring lilies.",Introduction,"The next section reviews Merchant's (2007Merchant's ( , 2008) ) analysis of voice mismatch in ellipsis by postulating the functional category of Voice in the syntactic structure of a clause, and the subsequent rebuttal of Merchant's analysis by Tanaka (2011)."
"It seems clear that voice mismatch is disallowed only in some of elliptical structures like VP ellipsis, Pseudogapping and Sluicing.",NC,4,21,NC,,POS,claim tiré d'un exemple mais servant de point de départ aux auteurs,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,Consider the examples in (4) and (5).,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Unlike in the ellipsis structure of (4), voice mismatch is permissible in the non-elliptical structure of ( 5)."
"These results of the experiment show that voice mismatch in VP ellipsis is not always permissible, unlike what Merchant (2008) argues.",NC,4,71,NC,,NC,rapport détaillé des résultats d'un autre papier,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"No effect of coherence (i.e., discourse relation) is found in the big elliptical conditions (conditions (a-d) above).",No asymmetry in voice match between VP ellipsis and Pseudogapping,"Instead, discourse relations are a determining factor in ruling in or out voice mismatch in VP ellipsis."
"The conclusion drawn from the review of Merchant (2007Merchant ( , 2008) ) and Tanaka (2011) is that the former analysis based on the different sizes of ellipsis for VP ellipsis and Pseudogapping overgenerates and under-generates.",NC,4,73,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Instead, discourse relations are a determining factor in ruling in or out voice mismatch in VP ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"It over-predicts that all the examples involving mismatch in VP are acceptable, and at the same time it cannot predict that some of those involving voice mismatch in VP ellipsis are unacceptable."
"It over-predicts that all the examples involving mismatch in VP are acceptable, and at the same time it cannot predict that some of those involving voice mismatch in VP ellipsis are unacceptable.",NC,4,74,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"The conclusion drawn from the review of Merchant (2007Merchant ( , 2008) ) and Tanaka (2011) is that the former analysis based on the different sizes of ellipsis for VP ellipsis and Pseudogapping overgenerates and under-generates.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"In the next section, building on Kehler's (2000) insight into discourse relations between ellipsis and antecedent clauses, we argue that sizes of ellipsis for both VP ellipsis and Pseudogapping interact with such discourse relations."
"We depart from Kehler (2000), suggesting that a cause/effect relation as well as a resemblance relation requires syntactic identity in ellipsis, but that they are distinguished in terms of the category that undergoes ellipsis.",NC,4,99,NC,,POS,claim principal d'une argumentation qui continue sur plusieurs lignes ... est-ce qu'on garde seulement le claim initial ?,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"(Kehler 2000: 551, example 34) Kehler (2000: 543-46) ascribes this contrast to the fact that cause/effect relations require only semantic identity, which tolerates voice mismatch, while resemblance relations require syntactic identity in addition to semantic identity.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis."
"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis.",NC,4,100,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"We depart from Kehler (2000), suggesting that a cause/effect relation as well as a resemblance relation requires syntactic identity in ellipsis, but that they are distinguished in terms of the category that undergoes ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"By contrast, when a causeeffect relation holds, the smaller category VP can be elided, as schematized below: (28) a. vP ellipsis in ""parallel resemblance (or contrast) relations"" [ TP < vP [ VP ] >]... [ TP [ vP [ VP ] ]] b. VP ellipsis in ""non-parallel cause-effect relations"" [ TP [ vP < VP > ]]... [ TP [ vP [ VP ] ]] The difference between the two types of relations in terms of the category of ellipsis is justified on the basis of the following reasoning."
"By contrast, when a causeeffect relation holds, the smaller category VP can be elided, as schematized below: (28) a. vP ellipsis in ""parallel resemblance (or contrast) relations"" [ TP < vP [ VP ] >]... [ TP [ vP [ VP ] ]] b. VP ellipsis in ""non-parallel cause-effect relations"" [ TP [ vP < VP > ]]... [ TP [ vP [ VP ] ]] The difference between the two types of relations in terms of the category of ellipsis is justified on the basis of the following reasoning.",NC,4,101,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"First, a parallel resemblance relation relates two clauses/sentences; the ellipsis clause and its antecedent clause."
The ideas we rely on are summarized below: (29) Identity condition on VP or vP ellipsis: a. Case/case mismatch (between the copy of the survivor/remnant and its correlate) is not allowed for ellipsis (as part of syntactic isomorphism in ellipsis).,NC,4,115,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Given the asymmetry between resemblance and cause/effect relations in terms of the size of ellipsis, we are now in a position to account for their contrast in voice mismatch when a verbal domain (VP or vP) undergoes ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP."
"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP.",NC,4,116,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,The ideas we rely on are summarized below: (29) Identity condition on VP or vP ellipsis: a. Case/case mismatch (between the copy of the survivor/remnant and its correlate) is not allowed for ellipsis (as part of syntactic isomorphism in ellipsis).,No asymmetry in voice match between VP ellipsis and Pseudogapping,c. vP undergoes 'VP ellipsis' in a resemblance relation.
c. vP undergoes 'VP ellipsis' in a resemblance relation.,NC,4,117,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP.",No asymmetry in voice match between VP ellipsis and Pseudogapping,The key ingredient we rely on in this analysis is Case/case (mis)match in ellipsis.
The key ingredient we rely on in this analysis is Case/case (mis)match in ellipsis.,NC,4,118,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,c. vP undergoes 'VP ellipsis' in a resemblance relation.,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Simply stated, Case/case mismatch is not allowed between a survivor/remnant and its antecedent constituent (or correlate)."
"In other words, voice mismatch for vP ellipsis in a resemblance relation is not permissible, because it always invites Case mismatch between an object element and its corresponding subject or vice versus, ultimately infringing on the syntactic isomorphism on ellipsis.",NC,4,135,NC,,POS,,,,,,(30) ..[ antecedent constituent A' ] ...[ ellipsis constituent,"Therefore, there is bound to arise a Case mismatch in both Pseudogapping and VP ellipsis in a resemblance relation that holds for (33) and (34).",(30) ..[ antecedent constituent A' ] ...[ ellipsis constituent,We now turn to the examples where a VPinternal element is assigned not structural Case but inherent case.
"We saw that passive-active alternation (i.e., voice mismatch) in the antecedent and ellipsis pair is permissible in a cause/effect relation.",POS,4,158,POS,,NC,,,,,,Consequences,"Chomsky (1995) Why is there a contrast between passives, on the one hand, and unaccusatives and middles, on the other hand?",Consequences,"However, neither causative-unaccusative nor transitive-middle alternation in the antecedent and ellipsis pair is allowed."
"However, neither causative-unaccusative nor transitive-middle alternation in the antecedent and ellipsis pair is allowed.",POS,4,159,POS,,NC,,,,,,Consequences,"We saw that passive-active alternation (i.e., voice mismatch) in the antecedent and ellipsis pair is permissible in a cause/effect relation.",Consequences,"We suggest on the basis of the following do so replacement that in English, passives involve syntactic movement, but neither unaccusatives nor middles do so."
"This account implies that passive verbs are potentially transitive verbs, thus being able to meet the identity condition on ellipsis with transitive verbs.",POS,4,166,POS,,NC,,,,,,Consequences,"Stroik (2001), among others) cannot replace a VP that contains a gap left behind by A or A'-movement.",Consequences,"However, unaccusative and middle verbs are in fact intransitive verbs, thus not being able to meet the identity condition on ellipsis with causative or transitive verbs."
"However, unaccusative and middle verbs are in fact intransitive verbs, thus not being able to meet the identity condition on ellipsis with causative or transitive verbs.",POS,4,167,POS,,NC,,,,,,Consequences,"This account implies that passive verbs are potentially transitive verbs, thus being able to meet the identity condition on ellipsis with transitive verbs.",Consequences,"This is how we account for the unacceptability of ( 38), (42), and (43)."
We took Tanaka's rebuttal of Merchant's dichotomy in voice mismatch between VP and Pseudogapping to be valid.,NC,4,177,NC,,POS,,,,,,Conclusion,"In this paper, we first started with reviewing Merchant's (2008) analysis of voice mismatch in ellipsis constructions and Tanaka's (2011) reply to this analysis.",Conclusion,"Departing from Kehler's (2000) insight that the distinction between resemblance vs. cause/effect discourse coherence relations rather than between VP and Pseudogapping come into place in apparent voice mismatch, we argued that VP undergoes ellipsis in a resemblance relation, whereas vP does so in a cause/effect relation."
"On the basis, we employ a recurrent network to eliminate the fakes.",NC,5,3,NC,,FACT,,,,,,abstract,"In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features.",abstract,Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.
"To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy.",POS,5,18,POS,,FACT,,,,,,Introduction,"However, it is difficult to determine which words are pseudo-related in a specific case, and when they will ""jump out"" to mislead the generation of latent features during testing.",Introduction,"In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network Figure 1: Self-regulated learning scheme is equipped with a memory suppressor to eliminate the fakes."
"In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network Figure 1: Self-regulated learning scheme is equipped with a memory suppressor to eliminate the fakes.",NC,5,19,NC,,FACT,,,,,,Introduction,"To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy.",Introduction,"Detailed experiments on event detection show that our proposed method achieves a substantial performance gain, and is capable of robust domain adaptation."
"To be honest, the models use two different kinds of recurrent units.",NC,5,126,NC,,NEG,,,,,,Trigger identification,This is proven by the fact that SELF (Bi-LSTM+GAN) outperforms Nguyen et al (2016)'s Bi-RNN.,Trigger identification,"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM."
"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM.",NC,5,127,NC,,NEG,,,,,,Trigger identification,"To be honest, the models use two different kinds of recurrent units.",Trigger identification,"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015)."
"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015).",POS,5,128,POS,,NC,,,,,,Trigger identification,"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM.",Trigger identification,This allows a fair comparison between Bi-RNN and SELF.
This allows a fair comparison between Bi-RNN and SELF.,NC,5,129,NC,,POS,,,,,,Trigger identification,"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015).",Event classification,Table 2 shows the performance of multi-class classification.
"SELF inherits the merits of the RNN models, classifying the events with higher recall.",NC,5,139,NC,,POS,,,,,,Event classification,"Let us turn to the structurally more complicated models, SELF and Hybrid.",Event classification,"Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features."
"Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features.",NC,5,140,NC,,POS,,,,,,Event classification,"SELF inherits the merits of the RNN models, classifying the events with higher recall.",Event classification,"So that it outperforms other RNNs, with improvements of no less than 4.5% precision and 1.7% recall."
"However it is built using a single-channel architecture, concatenating the RNN and the CNN.",NC,5,146,NC,,NEG,,,,,,Event classification,Multi-angled cognition enables Hybrid to be more precise.,Event classification,"This results in twofold accumulation of feature information, causing a serious overfitting problem."
"This results in twofold accumulation of feature information, causing a serious overfitting problem.",POS,5,147,POS,,NEG,,,,,,Event classification,"However it is built using a single-channel architecture, concatenating the RNN and the CNN.",Event classification,"Therefore, Hybrid is localized to much higher precision but substantially lower recall."
"For Hybrid, as illustrated in  Figure 2, the gap becomes much wider (from 9% to 19.7%) when the binary classification task (trigger identification) is shifted to multi-class classification (event detection).",POS,5,150,POS,,NEG,,,,,,Event classification,Overfitting results in enlargement of the gap between precision and recall when the task changes to be more difficult.,Event classification,"By contrast, other work shows a nearly constant gap."
"By contrast, other work shows a nearly constant gap.",NC,5,151,NC,,NEG,,,,,,Event classification,"For Hybrid, as illustrated in  Figure 2, the gap becomes much wider (from 9% to 19.7%) when the binary classification task (trigger identification) is shifted to multi-class classification (event detection).",Event classification,"In particular, SELF yields a minimum gap in each task, which changes negligibly from 3.5% to 3.4%."
"However, SELF fails to recall the pronouns that act as a trigger.",POS,5,202,POS,,NEG,,,,,,Recall and Missing,We observe that Bi-RNN and Hybrid seldom pick them up.,Recall and Missing,This is because they occur in spoken language much more frequently than they occur in written language.
This is because they occur in spoken language much more frequently than they occur in written language.,NC,5,203,NC,,POS,,,,,,Recall and Missing,"However, SELF fails to recall the pronouns that act as a trigger.",Recall and Missing,The lack of narrative content makes it difficult to learn the relationship between the pronouns and the events.
The lack of narrative content makes it difficult to learn the relationship between the pronouns and the events.,NC,5,204,NC,,NEG,,,,,,Recall and Missing,This is because they occur in spoken language much more frequently than they occur in written language.,Recall and Missing,Some real examples collected from ACE are shown in Table 7.
We use a self-regulated learning approach to improve event detection.,NC,5,220,NC,,FACT,,,,,,Related Work,"We follow the work to create spurious features, but use them to regulate the self-learning process in a single-task situation.",Conclusion,"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space."
"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space.",NC,5,221,NC,,FACT,,,,,,Conclusion,We use a self-regulated learning approach to improve event detection.,Conclusion,"In this study, the performance of the discriminator in the adversarial network is left to be evaluated."
"In this study, the performance of the discriminator in the adversarial network is left to be evaluated.",PROSP,5,222,PROSP,,NEG,,,,,,Conclusion,"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space.",Conclusion,"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition."
"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition.",PROSP,5,223,PROSP,,POS,,,,,,Conclusion,"In this study, the performance of the discriminator in the adversarial network is left to be evaluated.",Conclusion,"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other."
"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other.",PROSP,5,224,PROSP,,POS,,,,,,Conclusion,"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition.",Conclusion,"Besides, the global features extracted in Li et al (2013)'s work are potentially useful for detecting the event instances referred by pronouns, although involve noises."
"Besides, the global features extracted in Li et al (2013)'s work are potentially useful for detecting the event instances referred by pronouns, although involve noises.",PROSP,5,225,PROSP,,NC,,,,,,Conclusion,"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other.",Conclusion,"Therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
"We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling.",FACT,6,3,FACT#POS,,FACT,,,,,,abstract,Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations.,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.
These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.,NC,6,5,NC,,POS,,,,,,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.,Introduction,The need for effective regularization methods for RNNs has seen extensive focus in recent years.
"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",NC,6,25,NC,,FACT,,,,,,Introduction,"Norm stabilization (Krueger & Memisevic, 2015) penalizes the model when the norm of an RNN's hidden state changes substantially between timesteps, achieving strong results in character language modeling on and phoneme recognition.",Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results."
"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",NC,6,26,NC,,POS,,,,,,Introduction,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model."
"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",NC,6,27,NC,,POS,,,,,,Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",Compared to other invasive regularization techniques,This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.
This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.,NC,6,28,NC,,POS,,,,,,Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",Activation Regularization,"L 2 activation regularization (AR) While L 2 regularization is traditionally used on the weights of machine learning models (L 2 weight decay), it could also be used on the activations."
"To understand the potential of AR and TAR, we investigate their impact on language model perplexity when used independently in Table 1 (AR) and Table 2 (TAR).",NC,6,56,NC,,FACT,,,,,,Model,"All models use weight tying between the embedding and softmax layer (Inan et al., 2016;Press & Wolf, 2016).",Evaluating AR and TAR independently on PTB:,"While both result in a substantial reduction in perplexity, AR results in the strongest improvement of 5.3, while TAR only achieves 4.3."
"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",POS,6,65,NC#POS,,POS,,,,,,Evaluating AR and TAR independently on PTB:,Comparing to state-of-the-art PTB: In Table 5 we summarize the current state of the art models in language modeling over the Penn Treebank.,Evaluating AR and TAR independently on PTB:,"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation."
"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation.",POS,6,66,POS,,NC,,,,,,Evaluating AR and TAR independently on PTB:,"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",Evaluating AR and TAR independently on PTB:,"This would likely result in the RHN being slower than the larger LSTM model during both training and prediction, especially when factoring in optimized LSTM implementations such as NVIDIA's cuDNN LSTM."
It is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not compare their NAS cell results to a standard or variational LSTM cell that was subjected to the same extensive hyperparameter search.,POS,6,70,POS,,NC,,,,,,Evaluating AR and TAR independently on PTB:,"While Zoph & Le (2016) do not report any of the hyperparameters or what type of dropout they used for their Penn Treebank result, they do note that they performed an extensive hyperparameter search over learning rate, weight initialization, dropout rates, and decay epoch in order to produce their best performing model.",Evaluating AR and TAR independently on PTB:,"Our largest LSTM results are 3 perplexity higher in comparison but have not undergone extensive hyperparameter search, do not use additional regularization techniques such as recurrent or embedding dropout, and do not use a custom RNN cell."
"While simple to implement, activity regularization and temporal activity regularization are com-",NC,6,89,NC,phrase incomplète,NC,,,,,,Conclusion,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Sample generated text,"For generating text samples, words were sampled using the standard generation script contained in the PyTorch word level language modeling example."
Unsupervised parsing solutions are simultaneously an attractive yet troublesome method for handling low-data scenarios.,POS,7,5,POS,,NC,,,,,,abstract,"With less than 24 hours of total annotation, we obtain 7% and 17% absolute improvement in unlabeled dependency scores for English and Spanish, respectively, compared to the same parser using only universal grammar constraints.",Introduction,"The performance of unsupervised parsers has increased dramatically in recent years (Klein and Manning, 2004;Naseem et al., 2010), making them a potentially viable option for constructing labeled corpora on limited budgets."
"Allowing partial annotations dramatically increases the speed at which annotators can work, while simultaneously reducing error rates.",POS,7,31,POS,,NC,,,,,,Graph Fragment Language,Many words and phrases are underspecified.,Graph Fragment Language,These two effects both arise from being able to leave difficult or tedious portions of a sentence unspecified.
These two effects both arise from being able to leave difficult or tedious portions of a sentence unspecified.,POS,7,32,POS,,NC,,,,,,Graph Fragment Language,"Allowing partial annotations dramatically increases the speed at which annotators can work, while simultaneously reducing error rates.",Filling in Partial Dependencies,A partial annotation produces a set of dependency tree fragments.
"Indeed, Fill+Parse method produces better results for our datasets than Fill-then-Parse (see Section 4.2).",NC,7,41,NC,,POS,,,,,,Filling in Partial Dependencies,Fill-then-Parse obscures this distinction and not knowing how trustworthy an arc is can lead to additional errors.,Simulated Cost Comparison,Many factors influence the cost of creating a corpus.
"The actual cost of finding and paying annotators is the most obvious factor, and it will typically be higher for a lowresource language or highly specialized domain.",POS,7,44,POS,,NC,,,,,,Simulated Cost Comparison,Our goal is to minimize cost relative to the performance of a parser trained with the corpus.,Simulated Cost Comparison,Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.
Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.,PROSP,7,45,PROSP,,NC,,,,,,Simulated Cost Comparison,"The actual cost of finding and paying annotators is the most obvious factor, and it will typically be higher for a lowresource language or highly specialized domain.",Simulated Cost Comparison,"Given a partial annotation scheme like GFL, an additional cost factor is that of obtaining a particular level of completion for each sentence."
"Given a partial annotation scheme like GFL, an additional cost factor is that of obtaining a particular level of completion for each sentence.",POS,7,46,POS,,NC,,,,,,Simulated Cost Comparison,Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.,Simulated Cost Comparison,"Consider that for any sentence there are both 'low-hanging fruit' dependencies such as determiner attachment, and more difficult dependencies such as preposition attachment and long-distance relations."
"To this end, we trained our own POS taggers using type label annotations (Garrette and Baldridge, 2013)  We trained taggers for all languages using a limited amount of the available gold data-ensuring that the accuracy is comparable with low-resource human-sourced taggers.",FACT,7,84,FACT,,NC,,,,,,POS-Tagging,Our goal is to minimize real-world costs associated with producing a finished parsing model.,POS-Tagging,"We extract types from the corpus, rank them by frequency, and take the most frequent types to train the tagger."
Results based on actual annotation are the most important as they provide our best measure of performance under a realistic annotation setting.,POS,7,115,POS,,NC,,,,,,Experiments and Discussion,We consider both simulated and actual partial annotations.,Experiments and Discussion,"However, our Spanish annotators had only six hours each, and there was no inter-annotator communication or creation of annotation conventions, and no attempt to have them adopt the conventions in the gold-standard AnCora dependencies we evaluate against."
"To investigate the impact of POS taggers on parsing results, we conducted two series of experiments using POS tags trained by our own tagger as discussed in Section 2.6 (Predicted Tag) and gold POS tags extracted from treebank (Gold Tag).",FACT,7,134,FACT,,NC,,,,,,Annotator-sourced partial dependencies,Table 5 gives semi-supervised parsing results on the English and Spanish treebanks for sentences with 10 or fewer words.,Annotator-sourced partial dependencies,We compare against a right-branching baseline and the Gibbs parser of Mielens et al.
All the parsing methods handily beat the rightbranching baseline.,NC,7,136,NC,,POS,,,,,,Annotator-sourced partial dependencies,We compare against a right-branching baseline and the Gibbs parser of Mielens et al.,Annotator-sourced partial dependencies,"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags."
"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags.",NC,7,137,NC,,POS,,,,,,Annotator-sourced partial dependencies,All the parsing methods handily beat the rightbranching baseline.,Annotator-sourced partial dependencies,"This shows the effectiveness of ConvexMST, but highlights its brittleness with respect to tagging errors: bad tags lead to poor guidance from language universals."
"This shows the effectiveness of ConvexMST, but highlights its brittleness with respect to tagging errors: bad tags lead to poor guidance from language universals.",POS,7,138,POS,,NEG#POS,,,,,,Annotator-sourced partial dependencies,"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags.",Annotator-sourced partial dependencies,ConvexMST-GFL easily beats both these approaches: it exploits partial annotations much more effectively than the Gibbs parser and learns effectively without language universals.
"Our goal was to understand the overall behavior of different methods given the same free-wheeling, diverse annotations; it is likely that higher numbers would have been achieved had we guided annotators to use corpus conventions, or used full annotations provided by our annotators as the evaluation set.",PROSP,7,146,PROSP,,POS,,,,,,Annotator-sourced partial dependencies,It is important to recall that the GFL annotations have no specific conformity to the gold standards of either original corpus.,Annotator-sourced partial dependencies,"The former defeats the spirit of our exercise, and we did not have sufficient budget for the latter."
"(Again, keep in mind that we are considering a ""cold start"" to this process, so there can be no gold standard for checking annotator quality.)",NC,7,154,NC,,NEG,,,,,,Annotator-sourced partial dependencies,The next obvious steps would be to use active learning and to detect disagreement in annotators to either drop some or intervene improve their quality.,Annotator-sourced partial dependencies,"Comparison to Full Annotation To this point, all performance comparisons have been between different parse feature sets; we have demonstrated that the GFL features are complimentary to the UG features, and that when standing alone the GFL features are stronger than the UG features."
"However, the theoretical wall clock time of the group of annotators could be as low as two hours if the sessions were run in parallel.",NEG,7,160,NEG,,POS,,,,,,Annotator-sourced partial dependencies,"In comparison, the other annotators were able to partially annotate the same section in roughly two hours each -a total of 24 hours.",Annotator-sourced partial dependencies,These different training sets were once again used to train ConvexMST models that were evaluated on a held out test set.
"In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario).",PROSP,7,164,PROSP,,NEG,,,,,,Annotator-sourced partial dependencies,It should be noted that this comparison does not weight the results using the extrinsic costs associated with the production of the training data.,Annotator-sourced partial dependencies,This makes the performance per unit cost for partial annotators even higher than of these extrinsic cost effects.
This makes the performance per unit cost for partial annotators even higher than of these extrinsic cost effects.,POS,7,165,POS,,NEG,,,,,,Annotator-sourced partial dependencies,"In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario).",Longer Sentences,We also evaluated ConvexMST with longer sentences: those with 20 words or less.
"For this, the right-branching baseline is 25.8%.",POS,7,167,POS,,NC,,,,,,Longer Sentences,We also evaluated ConvexMST with longer sentences: those with 20 words or less.,Longer Sentences,"When using all the annotations on the common set for all annotators, the scores for ConvexMST with UG, GFL, and GFL+UG are 47.6%, 54.4%, and 55.3%, respectively."
POS-Tagging Impact We thought it important to consider imperfect POS-taggings because this entire framework is based off of the assumption that the user is working from essentially no pre-existing resources.,POS,7,170,POS,,NC,,,,,,Longer Sentences,"The values are worse than for shorter sentences, as expected, but the pattern observed in Table 5 still holds: GFL annotations best UG alone, and their combination is the best of all.",Discussion & Error Analysis,"Assuming the availability of gold-standard POS tags is antithetical to this idea, and is one way in which direct supervision can show up in otherwise unsupervised (or indirectly supervised) systems."
"However, more unlikely errors can cause more dramatic effects, as shown in Figure 6.",POS,7,173,POS,,NEG,,,,,,Discussion & Error Analysis,"Many tagger errors are not likely to cause major problems during parsing; for instance mislabeling pronouns as nouns, or adverbs as adjectives, is unlikely to lead to major structural issues.",Discussion & Error Analysis,"Here, the phrase 'beating politically' (gold tags 'NOUN ADV') is mistagged as 'ADJ VERB', leading to the attachment of 'politically' to the root word and the reorganization of a substantial chunk of the sentence."
"Weighting Constraint Violations For feature sets with both GFL and UG-based constraints, a weighting factor can bias the parser towards being more likely to respect either GFL or UG constraints.",POS,7,175,POS,,NEG,,,,,,Discussion & Error Analysis,"Here, the phrase 'beating politically' (gold tags 'NOUN ADV') is mistagged as 'ADJ VERB', leading to the attachment of 'politically' to the root word and the reorganization of a substantial chunk of the sentence.",Discussion & Error Analysis,"We experimented with this, and found that for the  datasets we considered, the best results were obtained when we weighted violations of GFL constraints as worse than violations of UG constraints."
"We experimented with this, and found that for the  datasets we considered, the best results were obtained when we weighted violations of GFL constraints as worse than violations of UG constraints.",POS,7,176,FACT#POS,,POS,,,,,,Discussion & Error Analysis,"Weighting Constraint Violations For feature sets with both GFL and UG-based constraints, a weighting factor can bias the parser towards being more likely to respect either GFL or UG constraints.",Discussion & Error Analysis,"This result is not entirely unexpected given the relative performances of the constraints on their own, but it provides more evidence that direct supervision even in small amounts can beat indirect supervision."
"We demonstrate this with actual annotations produced for English and Spanish, using annotators with a range of experience.",POS,7,180,POS,,FACT,,,,,,5,"The ConvexMST method we adapt from Grave and Elhadad easily combines constraints from both language universals and partial annotations, providing greater robustness from starting annotation until one runs out of budget or time.",5,"Overall, we present a case for working in realistic settings by paying close attention the various sources of annotation and tracking the real costs associated with that supervision."
"Overall, we present a case for working in realistic settings by paying close attention the various sources of annotation and tracking the real costs associated with that supervision.",POS,7,181,POS,,FACT,,,,,,5,"We demonstrate this with actual annotations produced for English and Spanish, using annotators with a range of experience.",5,"We believe that overreliance on creeping supervision of this type may lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models, and are encouraged by recent work on character-based models by Gillick et al."
"This approach has the advantage that it does not require explicit feature engineering, alignments, and a sound transition matrix.",FACT,8,40,FACT,,NC,should we consider it a fact ?,,,,,Convolutional networks,This article is the first to apply convolutional networks (ConvNets) to phonemes by treating each phoneme as a vector of binary valued phonetic features.,Convolutional networks,The approach requires cognacy statements and phonetic descriptions of sounds used to transcribe the words.
The approach requires cognacy statements and phonetic descriptions of sounds used to transcribe the words.,FACT,8,41,FACT,,NC,,,,,,Convolutional networks,"This approach has the advantage that it does not require explicit feature engineering, alignments, and a sound transition matrix.",Convolutional networks,The cognacy statements can be obtained from etymological dictionaries and the quality of the phonemes can be obtained from Ladefoged and Maddieson (1998).
The Manhattan ConvNet competes with PMI and orthographic models at cross-concept cognate identification task.,NC,8,141,NC,,POS,,,,,,Cross-Family experiments,The results of our experiments are given in table 5,Discussion,The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all the three language families.
The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all the three language families.,NC,8,142,NC,,POS,,,,,,Discussion,The Manhattan ConvNet competes with PMI and orthographic models at cross-concept cognate identification task.,Discussion,"In terms of averaged F-scores, Manhattan ConvNet performs slightly better than orthographic model and only performs worse than the other models at Austronesian language family."
The Manhattan ConvNet does not turn up as the best system across all the evaluation metrics in a single language family.,POS,8,145,POS,,POS,exemple de résultat négatif (ici catégorisé en POS),,,,,Discussion,The Manhattan ConvNet shows mixed performance at the task of cross-family cognate identification.,Discussion,The ConvNet performs better than PMI but is not as good as Orthographic measures at Indo-European language family.
"However, ConvNets do not perform as well as orthographic and PMI systems.",POS,8,152,POS,,POS,exemple de résultat négatif (ici catégorisé comme POS),,,,,Discussion,The Orthographic system and PMI system show similar performance at the Austronesian crossconcept task.,Discussion,The reason for this could be due to the differential transcriptions in the database.
The reason for this could be due to the differential transcriptions in the database.,POS,8,153,POS,,POS,explication de résultat,,,,,Discussion,"However, ConvNets do not perform as well as orthographic and PMI systems.",Conclusion,"In this article, we explored the use of phonetic feature convolutional networks for the task of pairwise cognate identification."
We are currently working towards building a larger database of word lists in IPA transcription.,PROSP,8,157,PROSP,,PROSP,PROSP mais déjà commencé dans le présent,,,,,Conclusion,"In the future, we intend to work directly with speech recordings and include language relatedness information into ConvNets to improve the performance.",,
"Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.",NC,9,4,NC,,FACT,,,,,,abstract,"In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents.",abstract,"We evaluate our method on question answering, obtaining state-of-the-art results."
"We evaluate our method on question answering, obtaining state-of-the-art results.",POS,9,5,FACT#POS,,POS,,,,,,abstract,"Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.",INTRODUCTION,"Information retrieval is an important component for many natural language processing tasks, such as question answering (Voorhees et al., 1999) or fact checking (Thorne et al., 2018)."
"Said otherwise, we assume that attention activations are a good proxy for the relevance of documents.",NC,9,27,NC,,POS,,,,,,INTRODUCTION,"More precisely, we use a sequence-to-sequence model as the reader, and use the attention activations over the input documents as synthetic labels to train the retriever.",INTRODUCTION,We then train the retriever to reproduce the ranking of documents corresponding to that metric.
"We make the following contributions: • First, we show that attention scores from a sequence-to-sequence reader model are a good measure of document relevance (Sec.",POS,9,29,FACT#POS,,POS,,,,,,INTRODUCTION,We then train the retriever to reproduce the ranking of documents corresponding to that metric.,INTRODUCTION,"3.2) ; • Second, inspired by knowledge distillation, we propose to iteratively train the retriever from these activations, and compare different loss functions (Sec."
"3.4) ; • Finally, we evaluate our method on three question-answering benchmarks, obtaining stateof-the-art results (Sec.",POS,9,31,FACT#POS,,POS,,,,,,INTRODUCTION,"3.2) ; • Second, inspired by knowledge distillation, we propose to iteratively train the retriever from these activations, and compare different loss functions (Sec.",INTRODUCTION,Our code is available at: github.com/facebookresearch/FiD.
Our code is available at: github.com/facebookresearch/FiD.,NC,9,32,NC,,FACT,,,,,,INTRODUCTION,"3.4) ; • Finally, we evaluate our method on three question-answering benchmarks, obtaining stateof-the-art results (Sec.",RELATED WORK,We briefly review information retrieval based on machine learning.
"In addition to initializing our method with documents retrieved with BM25 and BERT, we also train a system by starting from DPR documents.",FACT,9,206,FACT,,NC,,,,,,RESULTS,"In Table 2, we report the performance of our approach, as well as existing state-of-the-art systems on TriviaQA and NaturalQuestions.",RESULTS,"First, we observe that our method improve the performance over the state-of-the-art, even when starting from BM25 documents."
