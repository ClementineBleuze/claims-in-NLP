text,doc_id,paper_title,paper_structure,year,prev_text,prev_section,next_text,next_section,label
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 1/6
==========================================================================================
Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows.",457835,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,,,A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset.,abstract,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 2/6
==========================================================================================
A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset.",457836,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows.,abstract,"This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data.",abstract,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 3/6
==========================================================================================
This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data.",457837,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset.,abstract,We propose an unsupervised approach to training QA models with generated pseudotraining data.,abstract,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 4/6
==========================================================================================
We propose an unsupervised approach to training QA models with generated pseudotraining data.",457838,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data.",abstract,"We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships.",abstract,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 5/6
==========================================================================================
We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships.",457839,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We propose an unsupervised approach to training QA models with generated pseudotraining data.,abstract,"Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving stateof-the-art performance on SQuAD for unsupervised QA.",abstract,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
0. abstract -- 6/6
==========================================================================================
Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving stateof-the-art performance on SQuAD for unsupervised QA.",457840,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships.",abstract,Question Answering aims to answer a question based on a given knowledge source.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 1/40
==========================================================================================
Question Answering aims to answer a question based on a given knowledge source.",457841,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving stateof-the-art performance on SQuAD for unsupervised QA.",abstract,"Recent advances have driven the performance of QA systems to above or near-human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) thanks to pretrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019).",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 2/40
==========================================================================================
Recent advances have driven the performance of QA systems to above or near-human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) thanks to pretrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019).",457842,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Question Answering aims to answer a question based on a given knowledge source.,Introduction,"Fine-tuning these language models, however, requires largescale data for fine-tuning.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 3/40
==========================================================================================
Fine-tuning these language models, however, requires largescale data for fine-tuning.",457843,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Recent advances have driven the performance of QA systems to above or near-human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) thanks to pretrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019).",Introduction,Creating a dataset for every new domain is extremely costly and practically infeasible.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 4/40
==========================================================================================
Creating a dataset for every new domain is extremely costly and practically infeasible.",457844,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Fine-tuning these language models, however, requires largescale data for fine-tuning.",Introduction,The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 Equal contribution 2 Work done during internship at the AWS AI Labs desirable.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 5/40
==========================================================================================
The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 Equal contribution 2 Work done during internship at the AWS AI Labs desirable.",457845,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Creating a dataset for every new domain is extremely costly and practically infeasible.,Introduction,"This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017;Dhingra et al., 2018;Wang et al., 2018;Alberti et al., 2019).",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 6/40
==========================================================================================
This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017;Dhingra et al., 2018;Wang et al., 2018;Alberti et al., 2019).",457846,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 Equal contribution 2 Work done during internship at the AWS AI Labs desirable.,Introduction,"However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. ",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 7/40
==========================================================================================
However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. ",457847,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017;Dhingra et al., 2018;Wang et al., 2018;Alberti et al., 2019).",Introduction,"In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 8/40
==========================================================================================
In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question.",457848,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. ",Introduction,"Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 9/40
==========================================================================================
Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context.",457849,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question.",Introduction,"A training instance for supervised reading comprehension consists of three components: a question, a context, and an answer.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 10/40
==========================================================================================
A training instance for supervised reading comprehension consists of three components: a question, a context, and an answer.",457850,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context.",Introduction,"For a given dataset domain, a collection of documents can usually be easily obtained, providing context in the form of paragraphs or sets of sentences.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 11/40
==========================================================================================
For a given dataset domain, a collection of documents can usually be easily obtained, providing context in the form of paragraphs or sets of sentences.",457851,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"A training instance for supervised reading comprehension consists of three components: a question, a context, and an answer.",Introduction,Answers can be gathered from keywords and phrases from the context.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 12/40
==========================================================================================
Answers can be gathered from keywords and phrases from the context.",457852,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"For a given dataset domain, a collection of documents can usually be easily obtained, providing context in the form of paragraphs or sets of sentences.",Introduction,We focus mainly on factoid QA; the question concerns a concise fact.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 13/40
==========================================================================================
We focus mainly on factoid QA; the question concerns a concise fact.",457853,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Answers can be gathered from keywords and phrases from the context.,Introduction,"In particular, we emphasize questions whose answers are named entities, the majority type of factoid questions.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 14/40
==========================================================================================
In particular, we emphasize questions whose answers are named entities, the majority type of factoid questions.",457854,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We focus mainly on factoid QA; the question concerns a concise fact.,Introduction,Entities can be extracted from text using named entity recognition (NER) techniques as the training instance's answer.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 15/40
==========================================================================================
Entities can be extracted from text using named entity recognition (NER) techniques as the training instance's answer.",457855,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"In particular, we emphasize questions whose answers are named entities, the majority type of factoid questions.",Introduction,"Thus, the main challenge, and the focus of this paper, is creating a relevant question from a (context, answer) pair in an unsupervised manner. ",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 16/40
==========================================================================================
Thus, the main challenge, and the focus of this paper, is creating a relevant question from a (context, answer) pair in an unsupervised manner. ",457856,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Entities can be extracted from text using named entity recognition (NER) techniques as the training instance's answer.,Introduction,"Recent work of (Lewis et al., 2019) uses style transfer for generating questions for (context, answer) pairs but shows little improvement over applying a much simpler question generator which drops, permutates and masks words.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 17/40
==========================================================================================
Recent work of (Lewis et al., 2019) uses style transfer for generating questions for (context, answer) pairs but shows little improvement over applying a much simpler question generator which drops, permutates and masks words.",457857,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Thus, the main challenge, and the focus of this paper, is creating a relevant question from a (context, answer) pair in an unsupervised manner. ",Introduction,"We improve upon this paper by proposing a simple, intuitive, retrieval and template-based question generation approach, illustrated in Figure 1.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 18/40
==========================================================================================
We improve upon this paper by proposing a simple, intuitive, retrieval and template-based question generation approach, illustrated in Figure 1.",457858,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Recent work of (Lewis et al., 2019) uses style transfer for generating questions for (context, answer) pairs but shows little improvement over applying a much simpler question generator which drops, permutates and masks words.",Introduction,"The idea is to retrieve a sentence from the corpus similar to the current context, and then generate a question based on that sentence.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 19/40
==========================================================================================
The idea is to retrieve a sentence from the corpus similar to the current context, and then generate a question based on that sentence.",457859,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We improve upon this paper by proposing a simple, intuitive, retrieval and template-based question generation approach, illustrated in Figure 1.",Introduction,"Having created a question for all (context, answer) pairs, we then fine-tune a pretrained BERT model on this data and evaluate on the SQuAD v1.1 dataset (Rajpurkar et al., 2016). ",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 20/40
==========================================================================================
Having created a question for all (context, answer) pairs, we then fine-tune a pretrained BERT model on this data and evaluate on the SQuAD v1.1 dataset (Rajpurkar et al., 2016). ",457860,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"The idea is to retrieve a sentence from the corpus similar to the current context, and then generate a question based on that sentence.",Introduction,"Our contributions are as follows: we introduce a retrieval, template-based framework which achieves state-of-the-art results on SQuAD for unsupervised models, particularly when the answer is a named entity.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 21/40
==========================================================================================
Our contributions are as follows: we introduce a retrieval, template-based framework which achieves state-of-the-art results on SQuAD for unsupervised models, particularly when the answer is a named entity.",457861,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Having created a question for all (context, answer) pairs, we then fine-tune a pretrained BERT model on this data and evaluate on the SQuAD v1.1 dataset (Rajpurkar et al., 2016). ",Introduction,We perform ablation studies to determine the effect of components in template question generation.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 22/40
==========================================================================================
We perform ablation studies to determine the effect of components in template question generation.",457862,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Our contributions are as follows: we introduce a retrieval, template-based framework which achieves state-of-the-art results on SQuAD for unsupervised models, particularly when the answer is a named entity.",Introduction,We are releasing our synthetic training data and code.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 23/40
==========================================================================================
We are releasing our synthetic training data and code.",457863,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We perform ablation studies to determine the effect of components in template question generation.,Introduction,"1 2 Unsupervised QA Approach We focus on creating high-quality, non-trivial questions which will allow the model to learn to extract the proper answer from a context-question pair. ",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 24/40
==========================================================================================
1 2 Unsupervised QA Approach We focus on creating high-quality, non-trivial questions which will allow the model to learn to extract the proper answer from a context-question pair. ",457864,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We are releasing our synthetic training data and code.,Introduction,Sentence Retrieval: A standard cloze question can be obtained by taking the original sentence in which the answer appears from the context and masking the answer with a chosen token.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 25/40
==========================================================================================
Sentence Retrieval: A standard cloze question can be obtained by taking the original sentence in which the answer appears from the context and masking the answer with a chosen token.",457865,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"1 2 Unsupervised QA Approach We focus on creating high-quality, non-trivial questions which will allow the model to learn to extract the proper answer from a context-question pair. ",Introduction,"However, a model trained on this data will only learn text matching and how to fill-in-the-blank, with little generalizability.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 26/40
==========================================================================================
However, a model trained on this data will only learn text matching and how to fill-in-the-blank, with little generalizability.",457866,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Sentence Retrieval: A standard cloze question can be obtained by taking the original sentence in which the answer appears from the context and masking the answer with a chosen token.,Introduction,"For this reason, we chose to use a retrieval-based approach to obtain a sentence similar to that which contains the answer, upon which to create a given question.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 27/40
==========================================================================================
For this reason, we chose to use a retrieval-based approach to obtain a sentence similar to that which contains the answer, upon which to create a given question.",457867,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"However, a model trained on this data will only learn text matching and how to fill-in-the-blank, with little generalizability.",Introduction,"For our experiments, we focused on answers which are named entities, which has proven to be a useful prior assumption for downstream QA performance (Lewis et al., 2019) confirmed by our initial experiments.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 28/40
==========================================================================================
For our experiments, we focused on answers which are named entities, which has proven to be a useful prior assumption for downstream QA performance (Lewis et al., 2019) confirmed by our initial experiments.",457868,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"For this reason, we chose to use a retrieval-based approach to obtain a sentence similar to that which contains the answer, upon which to create a given question.",Introduction,"First, we indexed all of the sentences from a Wikipedia dump using the ElasticSearch search engine.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 29/40
==========================================================================================
First, we indexed all of the sentences from a Wikipedia dump using the ElasticSearch search engine.",457869,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"For our experiments, we focused on answers which are named entities, which has proven to be a useful prior assumption for downstream QA performance (Lewis et al., 2019) confirmed by our initial experiments.",Introduction,We also extract named entities for each sentence in both the Wikipedia corpus and the sentences used as queries.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 30/40
==========================================================================================
We also extract named entities for each sentence in both the Wikipedia corpus and the sentences used as queries.",457870,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"First, we indexed all of the sentences from a Wikipedia dump using the ElasticSearch search engine.",Introduction,"We assume access to a named-entity recognition system, and in this work make use of the spaCy 2 NER pipeline.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 31/40
==========================================================================================
We assume access to a named-entity recognition system, and in this work make use of the spaCy 2 NER pipeline.",457871,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We also extract named entities for each sentence in both the Wikipedia corpus and the sentences used as queries.,Introduction,"Then, for a given context-answer pair, we query the index, using the original context sentence as a query, to return a sentence which ( 1) contains the answer, (2) does not come from the context, and (3) has a lower than 95% F1 score with the query sentence to discard highly similar or plagiarized sentences.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 32/40
==========================================================================================
Then, for a given context-answer pair, we query the index, using the original context sentence as a query, to return a sentence which ( 1) contains the answer, (2) does not come from the context, and (3) has a lower than 95% F1 score with the query sentence to discard highly similar or plagiarized sentences.",457872,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We assume access to a named-entity recognition system, and in this work make use of the spaCy 2 NER pipeline.",Introduction,"Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both the query sentence and in the entire context, and we perform ablation studies on the effect of this matching below.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 33/40
==========================================================================================
Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both the query sentence and in the entire context, and we perform ablation studies on the effect of this matching below.",457873,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Then, for a given context-answer pair, we query the index, using the original context sentence as a query, to return a sentence which ( 1) contains the answer, (2) does not come from the context, and (3) has a lower than 95% F1 score with the query sentence to discard highly similar or plagiarized sentences.",Introduction,These retrieved sentences are then fed into our question-generation module. ,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 34/40
==========================================================================================
These retrieved sentences are then fed into our question-generation module. ",457874,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both the query sentence and in the entire context, and we perform ablation studies on the effect of this matching below.",Introduction,"Template-based Question Generation: We consider several question styles (1) generic clozestyle questions where the answer is replaced by the token ""[MASK]"", (2) templated question ""Wh+B+A+?"" as well as variations on the ordering of this template, as shown in Figure 2.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 35/40
==========================================================================================
Template-based Question Generation: We consider several question styles (1) generic clozestyle questions where the answer is replaced by the token ""[MASK]"", (2) templated question ""Wh+B+A+?"" as well as variations on the ordering of this template, as shown in Figure 2.",457875,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,These retrieved sentences are then fed into our question-generation module. ,Introduction,"Given the retrieved sentence in the form of [Fragment A] [Answer] [Fragment B], the templated question ""Wh+B+A+?"" replaces the answer with a Wh-component (e.g., what, who, where), which depends on the entity type of the answer and places the Wh-component at the beginning of the question, followed by sentence Fragment B and Fragment A. For the choice of wh-component, we sample a bi-gram based on prior probabilities of that bi-gram being associated with the named-entity type of the answer.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 36/40
==========================================================================================
Given the retrieved sentence in the form of [Fragment A] [Answer] [Fragment B], the templated question ""Wh+B+A+?"" replaces the answer with a Wh-component (e.g., what, who, where), which depends on the entity type of the answer and places the Wh-component at the beginning of the question, followed by sentence Fragment B and Fragment A. For the choice of wh-component, we sample a bi-gram based on prior probabilities of that bi-gram being associated with the named-entity type of the answer.",457876,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Template-based Question Generation: We consider several question styles (1) generic clozestyle questions where the answer is replaced by the token ""[MASK]"", (2) templated question ""Wh+B+A+?"" as well as variations on the ordering of this template, as shown in Figure 2.",Introduction,This prior probability is calculated based on named-entity and question bi-gram starters from the SQuAD dataset.,Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 37/40
==========================================================================================
This prior probability is calculated based on named-entity and question bi-gram starters from the SQuAD dataset.",457877,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Given the retrieved sentence in the form of [Fragment A] [Answer] [Fragment B], the templated question ""Wh+B+A+?"" replaces the answer with a Wh-component (e.g., what, who, where), which depends on the entity type of the answer and places the Wh-component at the beginning of the question, followed by sentence Fragment B and Fragment A. For the choice of wh-component, we sample a bi-gram based on prior probabilities of that bi-gram being associated with the named-entity type of the answer.",Introduction,"This information does not make use of the full context-question-answer and can be viewed as prior information, not disturbing the integrity of our unsupervised approach.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 38/40
==========================================================================================
This information does not make use of the full context-question-answer and can be viewed as prior information, not disturbing the integrity of our unsupervised approach.",457878,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,This prior probability is calculated based on named-entity and question bi-gram starters from the SQuAD dataset.,Introduction,"Additionally, the choice of wh component does not significantly affect results.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 39/40
==========================================================================================
Additionally, the choice of wh component does not significantly affect results.",457879,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"This information does not make use of the full context-question-answer and can be viewed as prior information, not disturbing the integrity of our unsupervised approach.",Introduction,"For template-based approaches, we also experimented with clause-based templates but did not find significant differences in performance.",Introduction,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
1. Introduction -- 40/40
==========================================================================================
For template-based approaches, we also experimented with clause-based templates but did not find significant differences in performance.",457880,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Additionally, the choice of wh component does not significantly affect results.",Introduction,"Settings: For all downstream question answering models, we fine-tune a pretrained BERT model using the Transformers repository (Wolf et al., 2019) and report ablation study numbers using the baseuncased version of BERT, consistent with (Lewis et al., 2019).",Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 1/54
==========================================================================================
Settings: For all downstream question answering models, we fine-tune a pretrained BERT model using the Transformers repository (Wolf et al., 2019) and report ablation study numbers using the baseuncased version of BERT, consistent with (Lewis et al., 2019).",457881,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"For template-based approaches, we also experimented with clause-based templates but did not find significant differences in performance.",Introduction,All models are trained and validated on generated pairs of questions and answers along with their contexts tested on the SQuAD development set.,Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 2/54
==========================================================================================
All models are trained and validated on generated pairs of questions and answers along with their contexts tested on the SQuAD development set.",457882,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Settings: For all downstream question answering models, we fine-tune a pretrained BERT model using the Transformers repository (Wolf et al., 2019) and report ablation study numbers using the baseuncased version of BERT, consistent with (Lewis et al., 2019).",Experiments,"The training set differs for each ablation study and will be described below, while the validation dataset is a random set of 1,000 template-based generated data points, which is consistent across all ablation studies.",Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 3/54
==========================================================================================
The training set differs for each ablation study and will be described below, while the validation dataset is a random set of 1,000 template-based generated data points, which is consistent across all ablation studies.",457883,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,All models are trained and validated on generated pairs of questions and answers along with their contexts tested on the SQuAD development set.,Experiments,"We train all QA models for 2 epochs, checkpointing the models every 500 steps and choosing the checkpoint with the highest F1 score on the validation set as the best model.",Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 4/54
==========================================================================================
We train all QA models for 2 epochs, checkpointing the models every 500 steps and choosing the checkpoint with the highest F1 score on the validation set as the best model.",457884,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"The training set differs for each ablation study and will be described below, while the validation dataset is a random set of 1,000 template-based generated data points, which is consistent across all ablation studies.",Experiments,All ablation studies are averaged over two training runs with different seeds.,Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 5/54
==========================================================================================
All ablation studies are averaged over two training runs with different seeds.",457885,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We train all QA models for 2 epochs, checkpointing the models every 500 steps and choosing the checkpoint with the highest F1 score on the validation set as the best model.",Experiments,"Unless otherwise stated, experiments are performed using 50,000 synthetic QA training examples, as initial models performed best with this amount.",Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 6/54
==========================================================================================
Unless otherwise stated, experiments are performed using 50,000 synthetic QA training examples, as initial models performed best with this amount.",457886,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,All ablation studies are averaged over two training runs with different seeds.,Experiments,We will make this generated training data public.,Experiments,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 7/54
==========================================================================================
We will make this generated training data public.",457887,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Unless otherwise stated, experiments are performed using 50,000 synthetic QA training examples, as initial models performed best with this amount.",Experiments,Effect of retrieved sentences: We test the effect of retrieved vs original sentences as input to question generation when using generic cloze questions.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 8/54
==========================================================================================
0. Model Analysis -- 1/40
------------------------------------------------------------------------------------------
Effect of retrieved sentences: We test the effect of retrieved vs original sentences as input to question generation when using generic cloze questions.",457888,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We will make this generated training data public.,Experiments,"As shown in Table 1, using retrieved sentences improves over using the original sentence, reinforcing our motivation that a retrieved sentence, which may not match trivially the current context, forces the QA model to learn more complex relationships than just simple entity matching.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 9/54
==========================================================================================
0. Model Analysis -- 2/40
------------------------------------------------------------------------------------------
As shown in Table 1, using retrieved sentences improves over using the original sentence, reinforcing our motivation that a retrieved sentence, which may not match trivially the current context, forces the QA model to learn more complex relationships than just simple entity matching.",457889,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Effect of retrieved sentences: We test the effect of retrieved vs original sentences as input to question generation when using generic cloze questions.,Model Analysis,The retrieval process may return sentences which do not match the original context.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 10/54
==========================================================================================
0. Model Analysis -- 3/40
------------------------------------------------------------------------------------------
The retrieval process may return sentences which do not match the original context.",457890,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"As shown in Table 1, using retrieved sentences improves over using the original sentence, reinforcing our motivation that a retrieved sentence, which may not match trivially the current context, forces the QA model to learn more complex relationships than just simple entity matching.",Model Analysis,"On a random sample, 15/18 retrieved sentences were judged as entirely relevant to the original sentence.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 11/54
==========================================================================================
0. Model Analysis -- 4/40
------------------------------------------------------------------------------------------
On a random sample, 15/18 retrieved sentences were judged as entirely relevant to the original sentence.",457891,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,The retrieval process may return sentences which do not match the original context.,Model Analysis,"This retrieval is already quite good, as we use a high quality ElasticSearch retrieval and use the original context sentence as the query, not just the answer word.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 12/54
==========================================================================================
0. Model Analysis -- 5/40
------------------------------------------------------------------------------------------
This retrieval is already quite good, as we use a high quality ElasticSearch retrieval and use the original context sentence as the query, not just the answer word.",457892,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"On a random sample, 15/18 retrieved sentences were judged as entirely relevant to the original sentence.",Model Analysis,"While we do not explicitly ensure that the retrieved sentence has the same meaning, we find that the search results with entity matching gives largely semantically matching sentences.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 13/54
==========================================================================================
0. Model Analysis -- 6/40
------------------------------------------------------------------------------------------
While we do not explicitly ensure that the retrieved sentence has the same meaning, we find that the search results with entity matching gives largely semantically matching sentences.",457893,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"This retrieval is already quite good, as we use a high quality ElasticSearch retrieval and use the original context sentence as the query, not just the answer word.",Model Analysis,"Additionally, we believe the sentences which have loosely related meaning may act as a regularization factor which prevent the downstream QA model from learning only string matching patterns.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 14/54
==========================================================================================
0. Model Analysis -- 7/40
------------------------------------------------------------------------------------------
Additionally, we believe the sentences which have loosely related meaning may act as a regularization factor which prevent the downstream QA model from learning only string matching patterns.",457894,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"While we do not explicitly ensure that the retrieved sentence has the same meaning, we find that the search results with entity matching gives largely semantically matching sentences.",Model Analysis,"Along these lines, (Lewis et al., 2019) found that a simple noise function of dropping, masking and permuting words was a strong question generation baseline.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 15/54
==========================================================================================
0. Model Analysis -- 8/40
------------------------------------------------------------------------------------------
Along these lines, (Lewis et al., 2019) found that a simple noise function of dropping, masking and permuting words was a strong question generation baseline.",457895,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Additionally, we believe the sentences which have loosely related meaning may act as a regularization factor which prevent the downstream QA model from learning only string matching patterns.",Model Analysis,"We believe that loosely related context sentences can act as a more intuitive noise function, and investigating the role of the semantic match of the retrieved sentences is an important direction for future work.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 16/54
==========================================================================================
0. Model Analysis -- 9/40
------------------------------------------------------------------------------------------
We believe that loosely related context sentences can act as a more intuitive noise function, and investigating the role of the semantic match of the retrieved sentences is an important direction for future work.",457896,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Along these lines, (Lewis et al., 2019) found that a simple noise function of dropping, masking and permuting words was a strong question generation baseline.",Model Analysis,"For the sections which follow, we only show results of retrieved sentences, as the trend of improved performance held across all experiments. ",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 17/54
==========================================================================================
0. Model Analysis -- 10/40
------------------------------------------------------------------------------------------
For the sections which follow, we only show results of retrieved sentences, as the trend of improved performance held across all experiments. ",457897,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We believe that loosely related context sentences can act as a more intuitive noise function, and investigating the role of the semantic match of the retrieved sentences is an important direction for future work.",Model Analysis,Effect of template components: We evaluate the effect of individual template components on downstream QA performance.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 18/54
==========================================================================================
0. Model Analysis -- 11/40
------------------------------------------------------------------------------------------
Effect of template components: We evaluate the effect of individual template components on downstream QA performance.",457898,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"For the sections which follow, we only show results of retrieved sentences, as the trend of improved performance held across all experiments. ",Model Analysis,Results are shown in Table 2.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 19/54
==========================================================================================
0. Model Analysis -- 12/40
------------------------------------------------------------------------------------------
Results are shown in Table 2.",457899,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Effect of template components: We evaluate the effect of individual template components on downstream QA performance.,Model Analysis,Wh template methods improve largely over the simple cloze templates.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 20/54
==========================================================================================
0. Model Analysis -- 13/40
------------------------------------------------------------------------------------------
Wh template methods improve largely over the simple cloze templates.",457900,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Results are shown in Table 2.,Model Analysis,"""Wh + B + A + ?"" performs best among the template-based methods, as having the Wh word at the beginning most resembles the target SQuAD domain and switching the order of Fragment B and Fragment A may force the model to learn more complex relationships from the question.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 21/54
==========================================================================================
0. Model Analysis -- 14/40
------------------------------------------------------------------------------------------
""Wh + B + A + ?"" performs best among the template-based methods, as having the Wh word at the beginning most resembles the target SQuAD domain and switching the order of Fragment B and Fragment A may force the model to learn more complex relationships from the question.",457901,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Wh template methods improve largely over the simple cloze templates.,Model Analysis,We additionally test the effect of the wh-component and the question mark added at the end of the sentence.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 22/54
==========================================================================================
0. Model Analysis -- 15/40
------------------------------------------------------------------------------------------
We additionally test the effect of the wh-component and the question mark added at the end of the sentence.",457902,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"""Wh + B + A + ?"" performs best among the template-based methods, as having the Wh word at the beginning most resembles the target SQuAD domain and switching the order of Fragment B and Fragment A may force the model to learn more complex relationships from the question.",Model Analysis,"Using the same data as ""Wh + B + A + ?""",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 23/54
==========================================================================================
0. Model Analysis -- 16/40
------------------------------------------------------------------------------------------
Using the same data as ""Wh + B + A + ?""",457903,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We additionally test the effect of the wh-component and the question mark added at the end of the sentence.,Model Analysis,but removing the wh-component results in a large decrease in performance.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 24/54
==========================================================================================
0. Model Analysis -- 17/40
------------------------------------------------------------------------------------------
but removing the wh-component results in a large decrease in performance.",457904,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Using the same data as ""Wh + B + A + ?""",Model Analysis,"We believe that this is because the wh-component signals the type of possible answer entities, which helps narrow down the space of possible answers.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 25/54
==========================================================================================
0. Model Analysis -- 18/40
------------------------------------------------------------------------------------------
We believe that this is because the wh-component signals the type of possible answer entities, which helps narrow down the space of possible answers.",457905,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,but removing the wh-component results in a large decrease in performance.,Model Analysis,"Removing the question mark at the end of the template also results in decreased performance, but not as large as removing the wh-component.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 26/54
==========================================================================================
0. Model Analysis -- 19/40
------------------------------------------------------------------------------------------
Removing the question mark at the end of the template also results in decreased performance, but not as large as removing the wh-component.",457906,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We believe that this is because the wh-component signals the type of possible answer entities, which helps narrow down the space of possible answers.",Model Analysis,This may be a result of BERT pretraining which expects certain punctuation based on sentence structure.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 27/54
==========================================================================================
0. Model Analysis -- 20/40
------------------------------------------------------------------------------------------
This may be a result of BERT pretraining which expects certain punctuation based on sentence structure.",457907,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Removing the question mark at the end of the template also results in decreased performance, but not as large as removing the wh-component.",Model Analysis,"We note that these questions may not be grammatical, which may have an impact on performance.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 28/54
==========================================================================================
0. Model Analysis -- 21/40
------------------------------------------------------------------------------------------
We note that these questions may not be grammatical, which may have an impact on performance.",457908,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,This may be a result of BERT pretraining which expects certain punctuation based on sentence structure.,Model Analysis,Improving the question quality makes a difference in performance as seen from the jump from cloze-style questions to template questions.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 29/54
==========================================================================================
0. Model Analysis -- 22/40
------------------------------------------------------------------------------------------
Improving the question quality makes a difference in performance as seen from the jump from cloze-style questions to template questions.",457909,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We note that these questions may not be grammatical, which may have an impact on performance.",Model Analysis,"The ablation studies suggest that a combination of question relevance, though matching entities, and question formulation, as described above, determine downstream performance.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 30/54
==========================================================================================
0. Model Analysis -- 23/40
------------------------------------------------------------------------------------------
The ablation studies suggest that a combination of question relevance, though matching entities, and question formulation, as described above, determine downstream performance.",457910,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Improving the question quality makes a difference in performance as seen from the jump from cloze-style questions to template questions.,Model Analysis,Balancing those two components is an interesting problem and we leave improving grammaticality and fluency through means such as language model generation for future experiments. ,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 31/54
==========================================================================================
0. Model Analysis -- 24/40
------------------------------------------------------------------------------------------
Balancing those two components is an interesting problem and we leave improving grammaticality and fluency through means such as language model generation for future experiments. ",457911,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"The ablation studies suggest that a combination of question relevance, though matching entities, and question formulation, as described above, determine downstream performance.",Model Analysis,"In the last two rows of Table 2, we show the effect of using the wh bi-gram prior on downstream QA training.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 32/54
==========================================================================================
0. Model Analysis -- 25/40
------------------------------------------------------------------------------------------
In the last two rows of Table 2, we show the effect of using the wh bi-gram prior on downstream QA training.",457912,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Balancing those two components is an interesting problem and we leave improving grammaticality and fluency through means such as language model generation for future experiments. ,Model Analysis,"Using the most-common wh word by grouping named entities into 5 categories according to (Lewis et al., 2019) performs very close to the best-performing wh n-gram prior method, while using a single wh-word (what) results in a significant decrease in performance.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 33/54
==========================================================================================
0. Model Analysis -- 26/40
------------------------------------------------------------------------------------------
Using the most-common wh word by grouping named entities into 5 categories according to (Lewis et al., 2019) performs very close to the best-performing wh n-gram prior method, while using a single wh-word (what) results in a significant decrease in performance.",457913,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"In the last two rows of Table 2, we show the effect of using the wh bi-gram prior on downstream QA training.",Model Analysis,These results suggest that information about named entity type signaled by the wh-word does provide important information to the model but further information beyond wh-simple does not improve results significantly. ,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 34/54
==========================================================================================
0. Model Analysis -- 27/40
------------------------------------------------------------------------------------------
These results suggest that information about named entity type signaled by the wh-word does provide important information to the model but further information beyond wh-simple does not improve results significantly. ",457914,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Using the most-common wh word by grouping named entities into 5 categories according to (Lewis et al., 2019) performs very close to the best-performing wh n-gram prior method, while using a single wh-word (what) results in a significant decrease in performance.",Model Analysis,Effect of filtering by entity matching:,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 35/54
==========================================================================================
0. Model Analysis -- 28/40
------------------------------------------------------------------------------------------
Effect of filtering by entity matching:",457915,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,These results suggest that information about named entity type signaled by the wh-word does provide important information to the model but further information beyond wh-simple does not improve results significantly. ,Model Analysis,"Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both query sentence and entire context.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 36/54
==========================================================================================
0. Model Analysis -- 29/40
------------------------------------------------------------------------------------------
Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both query sentence and entire context.",457916,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Effect of filtering by entity matching:,Model Analysis,Results are shown in Table 3.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 37/54
==========================================================================================
0. Model Analysis -- 30/40
------------------------------------------------------------------------------------------
Results are shown in Table 3.",457917,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Besides ensuring that the retrieved sentence and query sentence share the answer entity, we require that at least one additional matching entity appears in both query sentence and entire context.",Model Analysis,"Auxillary matching leads to improvements over no matching when using template-based data, with best results using matching with both query and context.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 38/54
==========================================================================================
0. Model Analysis -- 31/40
------------------------------------------------------------------------------------------
Auxillary matching leads to improvements over no matching when using template-based data, with best results using matching with both query and context.",457918,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Results are shown in Table 3.,Model Analysis,Matching may filter some sentences whose topic are too far from the original context.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 39/54
==========================================================================================
0. Model Analysis -- 32/40
------------------------------------------------------------------------------------------
Matching may filter some sentences whose topic are too far from the original context.",457919,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Auxillary matching leads to improvements over no matching when using template-based data, with best results using matching with both query and context.",Model Analysis,We leave further investigation of the effect of retrieved sentence relevance to future work. ,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 40/54
==========================================================================================
0. Model Analysis -- 33/40
------------------------------------------------------------------------------------------
We leave further investigation of the effect of retrieved sentence relevance to future work. ",457920,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Matching may filter some sentences whose topic are too far from the original context.,Model Analysis,"Effect of synthetic training dataset size: Notably, (Lewis et al., 2019)    as well.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 41/54
==========================================================================================
0. Model Analysis -- 34/40
------------------------------------------------------------------------------------------
Effect of synthetic training dataset size: Notably, (Lewis et al., 2019)    as well.",457921,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We leave further investigation of the effect of retrieved sentence relevance to future work. ,Model Analysis,Figure 3 shows the performance from training over random subsets of differing sizes and testing on the SQuAD development data.,Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 42/54
==========================================================================================
0. Model Analysis -- 35/40
------------------------------------------------------------------------------------------
Figure 3 shows the performance from training over random subsets of differing sizes and testing on the SQuAD development data.",457922,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Effect of synthetic training dataset size: Notably, (Lewis et al., 2019)    as well.",Model Analysis,"We sample a random question for each context from the data of (Lewis et al., 2019).",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 43/54
==========================================================================================
0. Model Analysis -- 36/40
------------------------------------------------------------------------------------------
We sample a random question for each context from the data of (Lewis et al., 2019).",457923,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,Figure 3 shows the performance from training over random subsets of differing sizes and testing on the SQuAD development data.,Model Analysis,"Even with as little as 10k datapoints, training from our synthetically generated template-based data with auxiliary matching outperforms the results from ablation studies in (Lewis et al., 2019).",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 44/54
==========================================================================================
0. Model Analysis -- 37/40
------------------------------------------------------------------------------------------
Even with as little as 10k datapoints, training from our synthetically generated template-based data with auxiliary matching outperforms the results from ablation studies in (Lewis et al., 2019).",457924,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We sample a random question for each context from the data of (Lewis et al., 2019).",Model Analysis,"Using data from our templatebased data consistently outperforms that of (Lewis et al., 2019).",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 45/54
==========================================================================================
0. Model Analysis -- 38/40
------------------------------------------------------------------------------------------
Using data from our templatebased data consistently outperforms that of (Lewis et al., 2019).",457925,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Even with as little as 10k datapoints, training from our synthetically generated template-based data with auxiliary matching outperforms the results from ablation studies in (Lewis et al., 2019).",Model Analysis,"Training on either dataset shows similar trends; performance decreases after increasing the number of synthetic examples past 100,000, likely due to a distributional mismatch with the SQuAD data.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 46/54
==========================================================================================
0. Model Analysis -- 39/40
------------------------------------------------------------------------------------------
Training on either dataset shows similar trends; performance decreases after increasing the number of synthetic examples past 100,000, likely due to a distributional mismatch with the SQuAD data.",457926,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Using data from our templatebased data consistently outperforms that of (Lewis et al., 2019).",Model Analysis,"We chose to use 50,000 examples for our final experiments with other ablation studies as this number gave good performance in initial experiments.",Model Analysis,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 47/54
==========================================================================================
0. Model Analysis -- 40/40
------------------------------------------------------------------------------------------
We chose to use 50,000 examples for our final experiments with other ablation studies as this number gave good performance in initial experiments.",457927,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"Training on either dataset shows similar trends; performance decreases after increasing the number of synthetic examples past 100,000, likely due to a distributional mismatch with the SQuAD data.",Model Analysis,We compare training on our best template-based data with state-of-the-art in Table 4.,Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 48/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 1/7
------------------------------------------------------------------------------------------
We compare training on our best template-based data with state-of-the-art in Table 4.",457928,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We chose to use 50,000 examples for our final experiments with other ablation studies as this number gave good performance in initial experiments.",Model Analysis,SQuAD F1 results reflect results on the hidden SQuAD test set.,Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 49/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 2/7
------------------------------------------------------------------------------------------
SQuAD F1 results reflect results on the hidden SQuAD test set.",457929,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We compare training on our best template-based data with state-of-the-art in Table 4.,Comparison of Best-Performing Models:,We report single-model numbers; Lewis et al.,Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 50/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 3/7
------------------------------------------------------------------------------------------
We report single-model numbers; Lewis et al.",457930,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,SQuAD F1 results reflect results on the hidden SQuAD test set.,Comparison of Best-Performing Models:,"(2019)  BERT-large, although using the original BERTlarge gives similar performance of 62.69 on the SQuAD dev set.",Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 51/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 4/7
------------------------------------------------------------------------------------------
(2019)  BERT-large, although using the original BERTlarge gives similar performance of 62.69 on the SQuAD dev set.",457931,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We report single-model numbers; Lewis et al.,Comparison of Best-Performing Models:,"We report numbers on the sample of SQuAD questions which are named entities, which we refer to as SQuAD-NER.",Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 52/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 5/7
------------------------------------------------------------------------------------------
We report numbers on the sample of SQuAD questions which are named entities, which we refer to as SQuAD-NER.",457932,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"(2019)  BERT-large, although using the original BERTlarge gives similar performance of 62.69 on the SQuAD dev set.",Comparison of Best-Performing Models:,"The subset corresponding to the SQuAD development dataset has 4,338 samples, and may differ slightly from (Lewis et al., 2019) due to differences in NER preprocessing.",Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 53/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 6/7
------------------------------------------------------------------------------------------
The subset corresponding to the SQuAD development dataset has 4,338 samples, and may differ slightly from (Lewis et al., 2019) due to differences in NER preprocessing.",457933,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We report numbers on the sample of SQuAD questions which are named entities, which we refer to as SQuAD-NER.",Comparison of Best-Performing Models:,"We also trained a fully-supervised model on the SQuAD training dataset with varying amounts of data and found our unsupervised performance equals the supervised performance trained on about 3,000 labeled examples.",Comparison of Best-Performing Models:,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
2. Experiments -- 54/54
==========================================================================================
1. Comparison of Best-Performing Models: -- 7/7
------------------------------------------------------------------------------------------
We also trained a fully-supervised model on the SQuAD training dataset with varying amounts of data and found our unsupervised performance equals the supervised performance trained on about 3,000 labeled examples.",457934,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"The subset corresponding to the SQuAD development dataset has 4,338 samples, and may differ slightly from (Lewis et al., 2019) due to differences in NER preprocessing.",Comparison of Best-Performing Models:,In this paper we introduce a retrieval-based approach to unsupervised extractive question answering.,Conclusion,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
3. Conclusion -- 1/4
==========================================================================================
In this paper we introduce a retrieval-based approach to unsupervised extractive question answering.",457935,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"We also trained a fully-supervised model on the SQuAD training dataset with varying amounts of data and found our unsupervised performance equals the supervised performance trained on about 3,000 labeled examples.",Comparison of Best-Performing Models:,"A simple template-based approach achieves state-of-the-art results for unsupervised methods on the SQuAD dataset of 64.04 F1, and 77.55 F1 when the answer is a named entity.",Conclusion,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
3. Conclusion -- 2/4
==========================================================================================
A simple template-based approach achieves state-of-the-art results for unsupervised methods on the SQuAD dataset of 64.04 F1, and 77.55 F1 when the answer is a named entity.",457936,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,In this paper we introduce a retrieval-based approach to unsupervised extractive question answering.,Conclusion,We analyze the effect of several components in our template-based approaches through ablation studies.,Conclusion,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
3. Conclusion -- 3/4
==========================================================================================
We analyze the effect of several components in our template-based approaches through ablation studies.",457937,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,"A simple template-based approach achieves state-of-the-art results for unsupervised methods on the SQuAD dataset of 64.04 F1, and 77.55 F1 when the answer is a named entity.",Conclusion,"We aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting.",Conclusion,
"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering
==========================================================================================
3. Conclusion -- 4/4
==========================================================================================
We aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting.",457938,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,We analyze the effect of several components in our template-based approaches through ablation studies.,Conclusion,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering>>. Please answer following questions: 

1. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its technicity / because it handles subjects you are unfamiliar with ?

Please add any label of your choice if your answer is yes.",457938,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,,,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering>>. Please answer following questions: 

2. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its writing style / structure / parsing errors ?

Please add any label of your choice if your answer is yes.",457938,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,,,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering>>. Please answer following questions: 

3. Did you know / read the article before this annotation task, or do you think you have identified its authors ?

Please add any label of your choice if your answer is yes.",457938,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"0. abstract
1. Introduction
2. Experiments
3. Conclusion
",2020,,,,,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
0. abstract -- 1/4
==========================================================================================
We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline.",1862661,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,,,"We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics.",abstract,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
0. abstract -- 2/4
==========================================================================================
We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics.",1862662,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline.",abstract,"Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems.",abstract,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
0. abstract -- 3/4
==========================================================================================
Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems.",1862663,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics.",abstract,"We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",abstract,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
0. abstract -- 4/4
==========================================================================================
We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",1862664,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems.",abstract,"Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002).",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 1/20
==========================================================================================
Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002).",1862665,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",abstract,"Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 2/20
==========================================================================================
Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task.",1862666,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002).",Introduction,"In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment, and although there are obvious parallels between evaluation of systems in the two areas, when it comes to evaluation of metrics, summarization has diverged considerably from methodologies applied to evaluation of metrics in MT. ",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 3/20
==========================================================================================
In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment, and although there are obvious parallels between evaluation of systems in the two areas, when it comes to evaluation of metrics, summarization has diverged considerably from methodologies applied to evaluation of metrics in MT. ",1862667,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task.",Introduction,"Since the inception of BLEU, evaluation of automatic metrics in MT has been by correlation with human assessment.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 4/20
==========================================================================================
Since the inception of BLEU, evaluation of automatic metrics in MT has been by correlation with human assessment.",1862668,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment, and although there are obvious parallels between evaluation of systems in the two areas, when it comes to evaluation of metrics, summarization has diverged considerably from methodologies applied to evaluation of metrics in MT. ",Introduction,"In contrast in summarization, over the years since the introduction of ROUGE, summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 5/20
==========================================================================================
In contrast in summarization, over the years since the introduction of ROUGE, summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics.",1862669,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Since the inception of BLEU, evaluation of automatic metrics in MT has been by correlation with human assessment.",Introduction,"Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (Rankel et al., 2011), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (Owczarzak et al., 2012). ",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 6/20
==========================================================================================
Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (Rankel et al., 2011), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (Owczarzak et al., 2012). ",1862670,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In contrast in summarization, over the years since the introduction of ROUGE, summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics.",Introduction,"Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 7/20
==========================================================================================
Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants.",1862671,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (Rankel et al., 2011), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (Owczarzak et al., 2012). ",Introduction,"For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 8/20
==========================================================================================
For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations.",1862672,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants.",Introduction,This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission. ,Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 9/20
==========================================================================================
This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission. ",1862673,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations.",Introduction,"Despite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014).",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 10/20
==========================================================================================
Despite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014).",1862674,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission. ,Introduction,This forces us to raise some important questions.,Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 11/20
==========================================================================================
This forces us to raise some important questions.",1862675,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Despite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014).",Introduction,"Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded?",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 12/20
==========================================================================================
Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded?",1862676,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,This forces us to raise some important questions.,Introduction,"For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings?",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 13/20
==========================================================================================
For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings?",1862677,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded?",Introduction,"Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings?",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 14/20
==========================================================================================
Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings?",1862678,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings?",Introduction,"Furthermore, although methods of statistical significance testing are commonly applied to evaluation of summarization systems, attempts to identify significant differences in performance of metrics are extremely rare, and when they have been applied unfortunately have not used an appropriate test. ",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 15/20
==========================================================================================
Furthermore, although methods of statistical significance testing are commonly applied to evaluation of summarization systems, attempts to identify significant differences in performance of metrics are extremely rare, and when they have been applied unfortunately have not used an appropriate test. ",1862679,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings?",Introduction,This motivates our review of past and current methodologies applied to the evaluation of summarization metrics.,Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 16/20
==========================================================================================
This motivates our review of past and current methodologies applied to the evaluation of summarization metrics.",1862680,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Furthermore, although methods of statistical significance testing are commonly applied to evaluation of summarization systems, attempts to identify significant differences in performance of metrics are extremely rare, and when they have been applied unfortunately have not used an appropriate test. ",Introduction,"Since MT evaluation in general has its own imperfections, we do not attempt to indiscriminately impose all MT evaluation methodologies on summarization, but specifically revisit evaluation methodologies applied to one particular area of summarization, evaluation of metrics.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 17/20
==========================================================================================
Since MT evaluation in general has its own imperfections, we do not attempt to indiscriminately impose all MT evaluation methodologies on summarization, but specifically revisit evaluation methodologies applied to one particular area of summarization, evaluation of metrics.",1862681,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,This motivates our review of past and current methodologies applied to the evaluation of summarization metrics.,Introduction,"Correlations with human assessment reveal an extremely wide range in performance among variants, highlighting the importance of an optimal choice of ROUGE variant in system evaluations.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 18/20
==========================================================================================
Correlations with human assessment reveal an extremely wide range in performance among variants, highlighting the importance of an optimal choice of ROUGE variant in system evaluations.",1862682,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Since MT evaluation in general has its own imperfections, we do not attempt to indiscriminately impose all MT evaluation methodologies on summarization, but specifically revisit evaluation methodologies applied to one particular area of summarization, evaluation of metrics.",Introduction,"Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems revealing distinct conclusions about the relative performance of systems.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 19/20
==========================================================================================
Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems revealing distinct conclusions about the relative performance of systems.",1862683,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Correlations with human assessment reveal an extremely wide range in performance among variants, highlighting the importance of an optimal choice of ROUGE variant in system evaluations.",Introduction,"In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.",Introduction,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
1. Introduction -- 20/20
==========================================================================================
In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.",1862684,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems revealing distinct conclusions about the relative performance of systems.",Introduction,"When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to that applied to metrics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment.",Related Work,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 1/24
==========================================================================================
When large-scale human evaluation of summarization systems takes place, human evaluation commonly takes the form of annotation of whether or not system-generated summary units express the meaning of model summary units, annotations subsequently used to compute human coverage scores.",1862685,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Computation of confidence intervals for individual correlation with human coverage scores, for example, unfortunately does not provide insight into whether or not a difference in correlation with human coverage scores is significant.",Related Work,"In addition, an evaluation of the linguistic quality of summaries is commonly carried out.",Summarization Metric Evaluation,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 2/24
==========================================================================================
In addition, an evaluation of the linguistic quality of summaries is commonly carried out.",1862686,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"When large-scale human evaluation of summarization systems takes place, human evaluation commonly takes the form of annotation of whether or not system-generated summary units express the meaning of model summary units, annotations subsequently used to compute human coverage scores.",Summarization Metric Evaluation,"As described in Section 2, when used for the evaluation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores.",Summarization Metric Evaluation,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 3/24
==========================================================================================
As described in Section 2, when used for the evaluation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores.",1862687,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In addition, an evaluation of the linguistic quality of summaries is commonly carried out.",Summarization Metric Evaluation,"In contrast, we include all available human assessment data for evaluating metrics.",Summarization Metric Evaluation,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 4/24
==========================================================================================
In contrast, we include all available human assessment data for evaluating metrics.",1862688,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"As described in Section 2, when used for the evaluation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores.",Summarization Metric Evaluation,"In DUC-2004(Over et al., 2007), human annotations used to compute summary coverage are carried out by identification of matching peer units (PUs), the units in a peer summary that express content of the corresponding model summary.",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 5/24
==========================================================================================
0. Combining Quality and Coverage -- 1/9
------------------------------------------------------------------------------------------
In DUC-2004(Over et al., 2007), human annotations used to compute summary coverage are carried out by identification of matching peer units (PUs), the units in a peer summary that express content of the corresponding model summary.",1862689,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In contrast, we include all available human assessment data for evaluating metrics.",Summarization Metric Evaluation,"In addition, an overall coverage estimate (E) is provided by the human annotator, the proportion of the corresponding model summary or collective model units (MUs) expressed overall by a given peer summary.",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 6/24
==========================================================================================
0. Combining Quality and Coverage -- 2/9
------------------------------------------------------------------------------------------
In addition, an overall coverage estimate (E) is provided by the human annotator, the proportion of the corresponding model summary or collective model units (MUs) expressed overall by a given peer summary.",1862690,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In DUC-2004(Over et al., 2007), human annotations used to compute summary coverage are carried out by identification of matching peer units (PUs), the units in a peer summary that express content of the corresponding model summary.",Combining Quality and Coverage,"Human coverage scores (CS) are computed by combining Matching PUs with coverage estimates as follows: CS = |M atching P U s| • E |M U s| In addition to annotations used to compute human coverage scores, human assessors were asked to rate the linguistic quality of summaries under 7 different criteria, providing ratings from A to E, with A denoting highest and E least quality rating. ",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 7/24
==========================================================================================
0. Combining Quality and Coverage -- 3/9
------------------------------------------------------------------------------------------
Human coverage scores (CS) are computed by combining Matching PUs with coverage estimates as follows: CS = |M atching P U s| • E |M U s| In addition to annotations used to compute human coverage scores, human assessors were asked to rate the linguistic quality of summaries under 7 different criteria, providing ratings from A to E, with A denoting highest and E least quality rating. ",1862691,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In addition, an overall coverage estimate (E) is provided by the human annotator, the proportion of the corresponding model summary or collective model units (MUs) expressed overall by a given peer summary.",Combining Quality and Coverage,"Figure 1 is a scatter-plot of human coverage scores and corresponding linguistic quality scores for all human-assessed summaries from DUC-2004, where, for the purpose of comparison, each of the 7 linguistic quality ratings are converted to a corresponding percentage quality (A= 100%; B= 75%; C= 50%; D= 25%; E= 0%).",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 8/24
==========================================================================================
0. Combining Quality and Coverage -- 4/9
------------------------------------------------------------------------------------------
Figure 1 is a scatter-plot of human coverage scores and corresponding linguistic quality scores for all human-assessed summaries from DUC-2004, where, for the purpose of comparison, each of the 7 linguistic quality ratings are converted to a corresponding percentage quality (A= 100%; B= 75%; C= 50%; D= 25%; E= 0%).",1862692,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Human coverage scores (CS) are computed by combining Matching PUs with coverage estimates as follows: CS = |M atching P U s| • E |M U s| In addition to annotations used to compute human coverage scores, human assessors were asked to rate the linguistic quality of summaries under 7 different criteria, providing ratings from A to E, with A denoting highest and E least quality rating. ",Combining Quality and Coverage,The location of all points almost exclusively within the upper left corner of the plot in Figure 1 indicates that the linguistic quality of almost all summaries reaches at least as high a level as its corresponding coverage score.,Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 9/24
==========================================================================================
0. Combining Quality and Coverage -- 5/9
------------------------------------------------------------------------------------------
The location of all points almost exclusively within the upper left corner of the plot in Figure 1 indicates that the linguistic quality of almost all summaries reaches at least as high a level as its corresponding coverage score.",1862693,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Figure 1 is a scatter-plot of human coverage scores and corresponding linguistic quality scores for all human-assessed summaries from DUC-2004, where, for the purpose of comparison, each of the 7 linguistic quality ratings are converted to a corresponding percentage quality (A= 100%; B= 75%; C= 50%; D= 25%; E= 0%).",Combining Quality and Coverage,"This follows the intuition that a summary is unlikely to obtain high coverage without sufficient linguistic quality, while the same cannot be said for the converse, that a high level of linguistic quality necessarily leads to high coverage.",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 10/24
==========================================================================================
0. Combining Quality and Coverage -- 6/9
------------------------------------------------------------------------------------------
This follows the intuition that a summary is unlikely to obtain high coverage without sufficient linguistic quality, while the same cannot be said for the converse, that a high level of linguistic quality necessarily leads to high coverage.",1862694,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,The location of all points almost exclusively within the upper left corner of the plot in Figure 1 indicates that the linguistic quality of almost all summaries reaches at least as high a level as its corresponding coverage score.,Combining Quality and Coverage,"More importantly, however, linguistic quality scores provide an additional dimension of human assessment, allowing greater discriminatory power between the quality of summaries than was possible with coverage scores alone. ",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 11/24
==========================================================================================
0. Combining Quality and Coverage -- 7/9
------------------------------------------------------------------------------------------
More importantly, however, linguistic quality scores provide an additional dimension of human assessment, allowing greater discriminatory power between the quality of summaries than was possible with coverage scores alone. ",1862695,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"This follows the intuition that a summary is unlikely to obtain high coverage without sufficient linguistic quality, while the same cannot be said for the converse, that a high level of linguistic quality necessarily leads to high coverage.",Combining Quality and Coverage,"Figure 2 includes linguistic quality and coverage score distributions from DUC-2004 human evaluation, where each distribution is skewed in opposing directions, in addition to the distribution of the average of the two scores for summaries. ",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 12/24
==========================================================================================
0. Combining Quality and Coverage -- 8/9
------------------------------------------------------------------------------------------
Figure 2 includes linguistic quality and coverage score distributions from DUC-2004 human evaluation, where each distribution is skewed in opposing directions, in addition to the distribution of the average of the two scores for summaries. ",1862696,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"More importantly, however, linguistic quality scores provide an additional dimension of human assessment, allowing greater discriminatory power between the quality of summaries than was possible with coverage scores alone. ",Combining Quality and Coverage,"For the purpose of metric evaluation, we combine human coverage and linguistic quality scores using the average of the two scores, and use this as a gold standard human score for evaluation of metrics:  Human Assessment Score = CS+M LQ",Combining Quality and Coverage,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 13/24
==========================================================================================
0. Combining Quality and Coverage -- 9/9
------------------------------------------------------------------------------------------
For the purpose of metric evaluation, we combine human coverage and linguistic quality scores using the average of the two scores, and use this as a gold standard human score for evaluation of metrics:  Human Assessment Score = CS+M LQ",1862697,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Figure 2 includes linguistic quality and coverage score distributions from DUC-2004 human evaluation, where each distribution is skewed in opposing directions, in addition to the distribution of the average of the two scores for summaries. ",Combining Quality and Coverage,"ROUGE includes a large number of distinct variants, including eight choices of n-gram counting method (ROUGE-1; 2; 3; 4; S4; SU4; W; L), binary settings such as word-stemming of summaries and an option to remove or retain stop-words.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 14/24
==========================================================================================
1. ROUGE -- 1/7
------------------------------------------------------------------------------------------
ROUGE includes a large number of distinct variants, including eight choices of n-gram counting method (ROUGE-1; 2; 3; 4; S4; SU4; W; L), binary settings such as word-stemming of summaries and an option to remove or retain stop-words.",1862698,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For the purpose of metric evaluation, we combine human coverage and linguistic quality scores using the average of the two scores, and use this as a gold standard human score for evaluation of metrics:  Human Assessment Score = CS+M LQ",Combining Quality and Coverage,"Additional configurations include the use of precision, recall or f-score to compute individual summary scores.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 15/24
==========================================================================================
1. ROUGE -- 2/7
------------------------------------------------------------------------------------------
Additional configurations include the use of precision, recall or f-score to compute individual summary scores.",1862699,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"ROUGE includes a large number of distinct variants, including eight choices of n-gram counting method (ROUGE-1; 2; 3; 4; S4; SU4; W; L), binary settings such as word-stemming of summaries and an option to remove or retain stop-words.",ROUGE,"Finally, options for computation of the overall score for a system is by computation of the mean or median of that system's summary score distribution.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 16/24
==========================================================================================
1. ROUGE -- 3/7
------------------------------------------------------------------------------------------
Finally, options for computation of the overall score for a system is by computation of the mean or median of that system's summary score distribution.",1862700,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Additional configurations include the use of precision, recall or f-score to compute individual summary scores.",ROUGE,"In total, therefore, when employing ROUGE for the evaluation of summarization systems, there are 192 (8 x 2 x 2 x 3 x 2) possible system-level variants to choose from.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 17/24
==========================================================================================
1. ROUGE -- 4/7
------------------------------------------------------------------------------------------
In total, therefore, when employing ROUGE for the evaluation of summarization systems, there are 192 (8 x 2 x 2 x 3 x 2) possible system-level variants to choose from.",1862701,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Finally, options for computation of the overall score for a system is by computation of the mean or median of that system's summary score distribution.",ROUGE,"The fact that final overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 18/24
==========================================================================================
1. ROUGE -- 5/7
------------------------------------------------------------------------------------------
The fact that final overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel.",1862702,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In total, therefore, when employing ROUGE for the evaluation of summarization systems, there are 192 (8 x 2 x 2 x 3 x 2) possible system-level variants to choose from.",ROUGE,"However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004;Graham et al., 2014).",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 19/24
==========================================================================================
1. ROUGE -- 6/7
------------------------------------------------------------------------------------------
However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004;Graham et al., 2014).",1862703,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"The fact that final overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel.",ROUGE,"For this purpose, differences in median ROUGE scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems.",ROUGE,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 20/24
==========================================================================================
1. ROUGE -- 7/7
------------------------------------------------------------------------------------------
For this purpose, differences in median ROUGE scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems.",1862704,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004;Graham et al., 2014).",ROUGE,"Moses (Koehn et al., 2007) multi-bleu 1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (Lin and Hovy, 2003).",Metric Evaluation by Pearson's r,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 21/24
==========================================================================================
2. Metric Evaluation by Pearson's r -- 1/4
------------------------------------------------------------------------------------------
Moses (Koehn et al., 2007) multi-bleu 1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (Lin and Hovy, 2003).",1862705,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For this purpose, differences in median ROUGE scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems.",ROUGE,"Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU's correlation with the same human assessment of summaries from DUC-2004.",Metric Evaluation by Pearson's r,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 22/24
==========================================================================================
2. Metric Evaluation by Pearson's r -- 2/4
------------------------------------------------------------------------------------------
Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU's correlation with the same human assessment of summaries from DUC-2004.",1862706,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Moses (Koehn et al., 2007) multi-bleu 1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (Lin and Hovy, 2003).",Metric Evaluation by Pearson's r,"Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.",Metric Evaluation by Pearson's r,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 23/24
==========================================================================================
2. Metric Evaluation by Pearson's r -- 3/4
------------------------------------------------------------------------------------------
Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.",1862707,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU's correlation with the same human assessment of summaries from DUC-2004.",Metric Evaluation by Pearson's r,"For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied.",Metric Evaluation by Pearson's r,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
3. Summarization Metric Evaluation -- 24/24
==========================================================================================
2. Metric Evaluation by Pearson's r -- 4/4
------------------------------------------------------------------------------------------
For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied.",1862708,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.",Metric Evaluation by Pearson's r,"In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014;Graham et al., 2015;Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 1/126
==========================================================================================
In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014;Graham et al., 2015;Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below.",1862709,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied.",Metric Evaluation by Pearson's r,"Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011;Yannakoudakis and Briscoe, 2012;Evanini et al., 2013). ",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 2/126
==========================================================================================
Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011;Yannakoudakis and Briscoe, 2012;Evanini et al., 2013). ",1862710,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014;Graham et al., 2015;Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below.",Metric Significance Testing,"Evaluation of a given summarization metric, M new , by Pearson correlation takes the form of quantifying the correlation, r(M new , H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(M base , H). ",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 3/126
==========================================================================================
Evaluation of a given summarization metric, M new , by Pearson correlation takes the form of quantifying the correlation, r(M new , H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(M base , H). ",1862711,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011;Yannakoudakis and Briscoe, 2012;Evanini et al., 2013). ",Metric Significance Testing,"One approach to testing for significance that may seem reasonable is to apply a significance test 1 https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2 http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-evaluation separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 4/126
==========================================================================================
One approach to testing for significance that may seem reasonable is to apply a significance test 1 https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2 http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-evaluation separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not.",1862712,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Evaluation of a given summarization metric, M new , by Pearson correlation takes the form of quantifying the correlation, r(M new , H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(M base , H). ",Metric Significance Testing,"The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(M new , H)) and that of another is not, does not necessarily mean that the difference between the two correlations is significant.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 5/126
==========================================================================================
The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(M new , H)) and that of another is not, does not necessarily mean that the difference between the two correlations is significant.",1862713,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"One approach to testing for significance that may seem reasonable is to apply a significance test 1 https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2 http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-evaluation separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not.",Metric Significance Testing,"Instead, a specific test should be applied to the difference in correlations.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 6/126
==========================================================================================
Instead, a specific test should be applied to the difference in correlations.",1862714,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(M new , H)) and that of another is not, does not necessarily mean that the difference between the two correlations is significant.",Metric Significance Testing,"For this same reason, confidence intervals for individual correlations with human assessment are also not useful. ",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 7/126
==========================================================================================
For this same reason, confidence intervals for individual correlations with human assessment are also not useful. ",1862715,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Instead, a specific test should be applied to the difference in correlations.",Metric Significance Testing,"If samples that data are drawn from are independent, and differences in correlations are computed on independent data sets, the Fisher r to z transformation is applied to test for significant differences in correlations.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 8/126
==========================================================================================
If samples that data are drawn from are independent, and differences in correlations are computed on independent data sets, the Fisher r to z transformation is applied to test for significant differences in correlations.",1862716,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For this same reason, confidence intervals for individual correlations with human assessment are also not useful. ",Metric Significance Testing,"Data used for the evaluation of summarization metrics are not independent, as evaluations comprise three sets of scores for precisely the same set of summaries (corresponding to variables X 1 , X 2 and X 3 below), and subsequently three correlations: r(M base , H), r(M new , H) and r(M new , M base ).",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 9/126
==========================================================================================
Data used for the evaluation of summarization metrics are not independent, as evaluations comprise three sets of scores for precisely the same set of summaries (corresponding to variables X 1 , X 2 and X 3 below), and subsequently three correlations: r(M base , H), r(M new , H) and r(M new , M base ).",1862717,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"If samples that data are drawn from are independent, and differences in correlations are computed on independent data sets, the Fisher r to z transformation is applied to test for significant differences in correlations.",Metric Significance Testing,"If r(M base , H) and r(M new , H) are both > 0, then the third correlation, between metric scores themselves, r(M base , M new ), must also be > 0.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 10/126
==========================================================================================
If r(M base , H) and r(M new , H) are both > 0, then the third correlation, between metric scores themselves, r(M base , M new ), must also be > 0.",1862718,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Data used for the evaluation of summarization metrics are not independent, as evaluations comprise three sets of scores for precisely the same set of summaries (corresponding to variables X 1 , X 2 and X 3 below), and subsequently three correlations: r(M base , H), r(M new , H) and r(M new , M base ).",Metric Significance Testing,"The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the difference in correlation between r(M base , H) and r(M new , H). ",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 11/126
==========================================================================================
The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the difference in correlation between r(M base , H) and r(M new , H). ",1862719,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"If r(M base , H) and r(M new , H) are both > 0, then the third correlation, between metric scores themselves, r(M base , M new ), must also be > 0.",Metric Significance Testing,"Williams test 3 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980).",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 12/126
==========================================================================================
Williams test 3 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980).",1862720,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the difference in correlation between r(M base , H) and r(M new , H). ",Metric Significance Testing,It is formulated as follows as a test of whether the population correlation between X 1 and X 3 equals the population correlation between X 2 and X 3 : ,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 13/126
==========================================================================================
It is formulated as follows as a test of whether the population correlation between X 1 and X 3 equals the population correlation between X 2 and X 3 : ",1862721,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Williams test 3 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980).",Metric Significance Testing,"(r 23 +r 13 ) 2 4 (1 − r 12 ) 3 , where r ij is the correlation between X i and X j , n is the size of the population, and:   -2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW), aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant whose correlation with human assessment is not significantly weaker than that of any other metric/variant (an optimal variant) according to pairwise Williams significance tests, variants employed in Hong et al.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 14/126
==========================================================================================
(r 23 +r 13 ) 2 4 (1 − r 12 ) 3 , where r ij is the correlation between X i and X j , n is the size of the population, and:   -2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW), aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant whose correlation with human assessment is not significantly weaker than that of any other metric/variant (an optimal variant) according to pairwise Williams significance tests, variants employed in Hong et al.",1862722,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,It is formulated as follows as a test of whether the population correlation between X 1 and X 3 equals the population correlation between X 2 and X 3 : ,Metric Significance Testing,(2014) are in bold. ,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 15/126
==========================================================================================
(2014) are in bold. ",1862723,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"(r 23 +r 13 ) 2 4 (1 − r 12 ) 3 , where r ij is the correlation between X i and X j , n is the size of the population, and:   -2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW), aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant whose correlation with human assessment is not significantly weaker than that of any other metric/variant (an optimal variant) according to pairwise Williams significance tests, variants employed in Hong et al.",Metric Significance Testing,A P 0.786 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 16/126
==========================================================================================
A P 0.786 • R-3",1862724,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,(2014) are in bold. ,Metric Significance Testing,N N A F 0.785 • R-2 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 17/126
==========================================================================================
N N A F 0.785 • R-2 N Y",1862725,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.786 • R-3,Metric Significance Testing,A P 0.783 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 18/126
==========================================================================================
A P 0.783 • R-3",1862726,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N A F 0.785 • R-2 N Y,Metric Significance Testing,N Y A P 0.781 • R-3 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 19/126
==========================================================================================
N Y A P 0.781 • R-3 Y N",1862727,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.783 • R-3,Metric Significance Testing,A F 0.779 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 20/126
==========================================================================================
A F 0.779 • R-3",1862728,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y A P 0.781 • R-3 Y N,Metric Significance Testing,N N A R 0.777 • R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 21/126
==========================================================================================
N N A R 0.777 • R-4",1862729,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.779 • R-3,Metric Significance Testing,A F 0.771 • R-3 N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 22/126
==========================================================================================
A F 0.771 • R-3 N N",1862730,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N A R 0.777 • R-4,Metric Significance Testing,A P 0.771 • R-3 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 23/126
==========================================================================================
A P 0.771 • R-3 Y N",1862731,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.771 • R-3 N N,Metric Significance Testing,A R 0.770 • R-2 N Y A F 0.769 • R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 24/126
==========================================================================================
A R 0.770 • R-2 N Y A F 0.769 • R-4",1862732,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.771 • R-3 Y N,Metric Significance Testing,A R 0.768 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 25/126
==========================================================================================
A R 0.768 • R-2",1862733,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.770 • R-2 N Y A F 0.769 • R-4,Metric Significance Testing,Y Y A F 0.768 • R-3 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 26/126
==========================================================================================
Y Y A F 0.768 • R-3 Y N",1862734,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.768 • R-2,Metric Significance Testing,A P 0.767 • R-3 N N M F 0.766 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 27/126
==========================================================================================
A P 0.767 • R-3 N N M F 0.766 • R-3",1862735,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A F 0.768 • R-3 Y N,Metric Significance Testing,N Y A F 0.764 • R-3 Y Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 28/126
==========================================================================================
N Y A F 0.764 • R-3 Y Y",1862736,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.767 • R-3 N N M F 0.766 • R-3,Metric Significance Testing,A P 0.764 • R-4 Y N A F 0.763 • R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 29/126
==========================================================================================
A P 0.764 • R-4 Y N A F 0.763 • R-4",1862737,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y A F 0.764 • R-3 Y Y,Metric Significance Testing,A P 0.762 • R-4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 30/126
==========================================================================================
A P 0.762 • R-4 Y N",1862738,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.764 • R-4 Y N A F 0.763 • R-4,Metric Significance Testing,A R 0.761 • R-3 N N M P 0.760 • R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 31/126
==========================================================================================
A R 0.761 • R-3 N N M P 0.760 • R-4",1862739,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.762 • R-4 Y N,Metric Significance Testing,Y Y A P 0.759 • R-2 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 32/126
==========================================================================================
Y Y A P 0.759 • R-2 Y N",1862740,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.761 • R-3 N N M P 0.760 • R-4,Metric Significance Testing,A P 0.759 • R-4 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 33/126
==========================================================================================
A P 0.759 • R-4 N Y",1862741,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A P 0.759 • R-2 Y N,Metric Significance Testing,A P 0.758 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 34/126
==========================================================================================
A P 0.758 • R-2",1862742,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.759 • R-4 N Y,Metric Significance Testing,A P 0.757 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 35/126
==========================================================================================
A P 0.757 • R-3",1862743,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.758 • R-2,Metric Significance Testing,N M R 0.753 • R-4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 36/126
==========================================================================================
N M R 0.753 • R-4 Y N",1862744,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.757 • R-3,Metric Significance Testing,A P 0.752 • R-3 Y Y A F 0.748 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 37/126
==========================================================================================
A P 0.752 • R-3 Y Y A F 0.748 • R-2",1862745,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N M R 0.753 • R-4 Y N,Metric Significance Testing,A F 0.747 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 38/126
==========================================================================================
A F 0.747 • R-2",1862746,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.752 • R-3 Y Y A F 0.748 • R-2,Metric Significance Testing,A F 0.747 • R-3 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 39/126
==========================================================================================
A F 0.747 • R-3 N Y",1862747,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.747 • R-2,Metric Significance Testing,A R 0.746 • R-3 Y N M P 0.744 • R-2 N Y M P 0.743 •,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 40/126
==========================================================================================
A R 0.746 • R-3 Y N M P 0.744 • R-2 N Y M P 0.743 •",1862748,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.747 • R-3 N Y,Metric Significance Testing,R-3 Y N M F 0.743 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 41/126
==========================================================================================
R-3 Y N M F 0.743 • R-2",1862749,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.746 • R-3 Y N M P 0.744 • R-2 N Y M P 0.743 •,Metric Significance Testing,A R 0.742 • R-2 Y Y M P 0.741 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 42/126
==========================================================================================
A R 0.742 • R-2 Y Y M P 0.741 • R-2",1862750,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,R-3 Y N M F 0.743 • R-2,Metric Significance Testing,N Y M F 0.740 • R-3 Y N M R 0.739 • R-2 Y Y A R 0.737 •,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 43/126
==========================================================================================
N Y M F 0.740 • R-3 Y N M R 0.739 • R-2 Y Y A R 0.737 •",1862751,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.742 • R-2 Y Y M P 0.741 • R-2,Metric Significance Testing,R-2 Y Y M F 0.735 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 44/126
==========================================================================================
R-2 Y Y M F 0.735 • R-2",1862752,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M F 0.740 • R-3 Y N M R 0.739 • R-2 Y Y A R 0.737 •,Metric Significance Testing,N N M P 0.734 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 45/126
==========================================================================================
N N M P 0.734 • R-3",1862753,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,R-2 Y Y M F 0.735 • R-2,Metric Significance Testing,Y Y M P 0.733 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 46/126
==========================================================================================
Y Y M P 0.733 • R-3",1862754,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M P 0.734 • R-3,Metric Significance Testing,Y Y A R 0.730 R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 47/126
==========================================================================================
Y Y A R 0.730 R-4",1862755,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y M P 0.733 • R-3,Metric Significance Testing,Y Y A F 0.729 • R-3 Y Y M F 0.726 • R-S4 Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 48/126
==========================================================================================
Y Y A F 0.729 • R-3 Y Y M F 0.726 • R-S4 Y",1862756,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A R 0.730 R-4,Metric Significance Testing,A P 0.725 • R-SU4 N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 49/126
==========================================================================================
A P 0.725 • R-SU4 N N",1862757,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A F 0.729 • R-3 Y Y M F 0.726 • R-S4 Y,Metric Significance Testing,A P 0.724 • R-2 Y N M P 0.724 R-S4 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 50/126
==========================================================================================
A P 0.724 • R-2 Y N M P 0.724 R-S4 N Y",1862758,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.725 • R-SU4 N N,Metric Significance Testing,A P 0.724 R-SU4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 51/126
==========================================================================================
A P 0.724 R-SU4 Y N",1862759,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.724 • R-2 Y N M P 0.724 R-S4 N Y,Metric Significance Testing,A P 0.723 • R-S4 N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 52/126
==========================================================================================
A P 0.723 • R-S4 N N",1862760,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.724 R-SU4 Y N,Metric Significance Testing,A P 0.723 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 53/126
==========================================================================================
A P 0.723 • R-2",1862761,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.723 • R-S4 N N,Metric Significance Testing,N Y M R 0.722 • R-4 N Y A F 0.721 • R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 54/126
==========================================================================================
N Y M R 0.722 • R-4 N Y A F 0.721 • R-1",1862762,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.723 • R-2,Metric Significance Testing,A P 0.720 • R-2 N N M F 0.719 • R-SU4 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 55/126
==========================================================================================
A P 0.720 • R-2 N N M F 0.719 • R-SU4 N Y",1862763,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M R 0.722 • R-4 N Y A F 0.721 • R-1,Metric Significance Testing,A P 0.719 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 56/126
==========================================================================================
A P 0.719 R-1",1862764,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.720 • R-2 N N M F 0.719 • R-SU4 N Y,Metric Significance Testing,A P 0.714 • R-2 Y Y M R 0.714 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 57/126
==========================================================================================
A P 0.714 • R-2 Y Y M R 0.714 • R-3",1862765,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.719 R-1,Metric Significance Testing,Y Y M R 0.713 • R-4 Y Y A R 0.712 • R-S4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 58/126
==========================================================================================
Y Y M R 0.713 • R-4 Y Y A R 0.712 • R-S4",1862766,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.714 • R-2 Y Y M R 0.714 • R-3,Metric Significance Testing,A P 0.711 R-SU4 Y Y A P 0.710 R-2 N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 59/126
==========================================================================================
A P 0.711 R-SU4 Y Y A P 0.710 R-2 N N",1862767,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y M R 0.713 • R-4 Y Y A R 0.712 • R-S4,Metric Significance Testing,A R 0.710 • R-W N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 60/126
==========================================================================================
A R 0.710 • R-W N Y",1862768,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.711 R-SU4 Y Y A P 0.710 R-2 N N,Metric Significance Testing,A P 0.709 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 61/126
==========================================================================================
A P 0.709 • R-2",1862769,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.710 • R-W N Y,Metric Significance Testing,A R 0.707 •,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 62/126
==========================================================================================
A R 0.707 •",1862770,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.709 • R-2,Metric Significance Testing,Metric Stemming RSW Ave./Med P/R/F r R-2 Y N M F 0.706 R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 63/126
==========================================================================================
Metric Stemming RSW Ave./Med P/R/F r R-2 Y N M F 0.706 R-3",1862771,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.707 •,Metric Significance Testing,N Y M P 0.704 • R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 64/126
==========================================================================================
N Y M P 0.704 • R-1",1862772,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Metric Stemming RSW Ave./Med P/R/F r R-2 Y N M F 0.706 R-3,Metric Significance Testing,A P 0.704 • R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 65/126
==========================================================================================
A P 0.704 • R-4",1862773,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M P 0.704 • R-1,Metric Significance Testing,N N M R 0.703 • R-L N Y A P 0.700 • R-W Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 66/126
==========================================================================================
N N M R 0.703 • R-L N Y A P 0.700 • R-W Y",1862774,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.704 • R-4,Metric Significance Testing,A P 0.700 • R-4 N Y A R 0.700 •,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 67/126
==========================================================================================
A P 0.700 • R-4 N Y A R 0.700 •",1862775,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M R 0.703 • R-L N Y A P 0.700 • R-W Y,Metric Significance Testing,R-1 Y N M P 0.699 • R-S4 N Y M P,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 68/126
==========================================================================================
R-1 Y N M P 0.699 • R-S4 N Y M P",1862776,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.700 • R-4 N Y A R 0.700 •,Metric Significance Testing,Y Y A P 0.698 • R-3 N Y M F 0.697 • R-W N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 69/126
==========================================================================================
Y Y A P 0.698 • R-3 N Y M F 0.697 • R-W N N",1862777,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,R-1 Y N M P 0.699 • R-S4 N Y M P,Metric Significance Testing,A P 0.696 • R-W Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 70/126
==========================================================================================
A P 0.696 • R-W Y N",1862778,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A P 0.698 • R-3 N Y M F 0.697 • R-W N N,Metric Significance Testing,A P 0.695 • R-4 N N M F 0.695 •,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 71/126
==========================================================================================
A P 0.695 • R-4 N N M F 0.695 •",1862779,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.696 • R-W Y N,Metric Significance Testing,R-S4 N Y M F 0.693 R-S4 N Y A F 0.691 R-SU4 N Y M P 0.690 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 72/126
==========================================================================================
R-S4 N Y M F 0.693 R-S4 N Y A F 0.691 R-SU4 N Y M P 0.690 R-1",1862780,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.695 • R-4 N N M F 0.695 •,Metric Significance Testing,N N M P 0.690 • R-2,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 73/126
==========================================================================================
N N M P 0.690 • R-2",1862781,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,R-S4 N Y M F 0.693 R-S4 N Y A F 0.691 R-SU4 N Y M P 0.690 R-1,Metric Significance Testing,N N M R 0.689 R-L Y Y A P 0.688 • R-3,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 74/126
==========================================================================================
N N M R 0.689 R-L Y Y A P 0.688 • R-3",1862782,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M P 0.690 • R-2,Metric Significance Testing,N Y M R 0.687 • R-S4 N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 75/126
==========================================================================================
N Y M R 0.687 • R-S4 N",1862783,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M R 0.689 R-L Y Y A P 0.688 • R-3,Metric Significance Testing,N M P 0.687 R-S4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 76/126
==========================================================================================
N M P 0.687 R-S4 Y N",1862784,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M R 0.687 • R-S4 N,Metric Significance Testing,A F 0.687 R-S4 N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 77/126
==========================================================================================
A F 0.687 R-S4 N",1862785,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N M P 0.687 R-S4 Y N,Metric Significance Testing,N A F 0.687 R-4 N N M P 0.687 • R-L N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 78/126
==========================================================================================
N A F 0.687 R-4 N N M P 0.687 • R-L N",1862786,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.687 R-S4 N,Metric Significance Testing,A P 0.686 • R-SU4 N N M P 0.686 R-L Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 79/126
==========================================================================================
A P 0.686 • R-SU4 N N M P 0.686 R-L Y N",1862787,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N A F 0.687 R-4 N N M P 0.687 • R-L N,Metric Significance Testing,A P 0.683 • R-W N N M P 0.682 • R-W Y N M P 0.680 • R-SU4 Y N M P 0.678 R-SU4 N Y A F 0.678 R-S4 Y Y A F 0.676 R-SU4 N Y M F 0.676 R-SU4 N N A F 0.673 R-1 N Y M P,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 80/126
==========================================================================================
A P 0.683 • R-W N N M P 0.682 • R-W Y N M P 0.680 • R-SU4 Y N M P 0.678 R-SU4 N Y A F 0.678 R-S4 Y Y A F 0.676 R-SU4 N Y M F 0.676 R-SU4 N N A F 0.673 R-1 N Y M P",1862788,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.686 • R-SU4 N N M P 0.686 R-L Y N,Metric Significance Testing,0.673 R-2 Y N M R 0.672 R-SU4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 81/126
==========================================================================================
0.673 R-2 Y N M R 0.672 R-SU4 Y N",1862789,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A P 0.683 • R-W N N M P 0.682 • R-W Y N M P 0.680 • R-SU4 Y N M P 0.678 R-SU4 N Y A F 0.678 R-S4 Y Y A F 0.676 R-SU4 N Y M F 0.676 R-SU4 N N A F 0.673 R-1 N Y M P,Metric Significance Testing,A F 0.671 R-S4 N Y M R 0.670 R-S4 Y N M P 0.670 R-SU4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 82/126
==========================================================================================
A F 0.671 R-S4 N Y M R 0.670 R-S4 Y N M P 0.670 R-SU4",1862790,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,0.673 R-2 Y N M R 0.672 R-SU4 Y N,Metric Significance Testing,Y Y A F 0.668 R-S4 N N M F 0.666 R-W N Y M P 0.664 R-S4 Y Y M P 0.664 R-SU4 Y Y M P 0.663 R-L N N M P 0.661 • R-SU4 N N M F 0.658 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 83/126
==========================================================================================
Y Y A F 0.668 R-S4 N N M F 0.666 R-W N Y M P 0.664 R-S4 Y Y M P 0.664 R-SU4 Y Y M P 0.663 R-L N N M P 0.661 • R-SU4 N N M F 0.658 R-1",1862791,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.671 R-S4 N Y M R 0.670 R-S4 Y N M P 0.670 R-SU4,Metric Significance Testing,N Y A F 0.656 R-W Y Y M P 0.656 R-S4 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 84/126
==========================================================================================
N Y A F 0.656 R-W Y Y M P 0.656 R-S4 N Y",1862792,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A F 0.668 R-S4 N N M F 0.666 R-W N Y M P 0.664 R-S4 Y Y M P 0.664 R-SU4 Y Y M P 0.663 R-L N N M P 0.661 • R-SU4 N N M F 0.658 R-1,Metric Significance Testing,A R 0.656 R-L Y N M P 0.656 • R-W N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 85/126
==========================================================================================
A R 0.656 R-L Y N M P 0.656 • R-W N Y",1862793,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y A F 0.656 R-W Y Y M P 0.656 R-S4 N Y,Metric Significance Testing,A F 0.655 R-1 N Y M F 0.653 R-L N Y A F 0.652 R-1 Y Y M P,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 86/126
==========================================================================================
A F 0.655 R-1 N Y M F 0.653 R-L N Y A F 0.652 R-1 Y Y M P",1862794,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.656 R-L Y N M P 0.656 • R-W N Y,Metric Significance Testing,0.651 R-S4 Y Y M F 0.649 R-1 Y Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 87/126
==========================================================================================
0.651 R-S4 Y Y M F 0.649 R-1 Y Y",1862795,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.655 R-1 N Y M F 0.653 R-L N Y A F 0.652 R-1 Y Y M P,Metric Significance Testing,A F 0.649 R-SU4 Y Y M F 0.649 R-SU4 N Y M R 0.646 R-L N Y M P 0.645 R-W N Y M F 0.642 R-W Y Y A F 0.642 R-4 Y N M R 0.641 R-S4 Y Y A R 0.641 R-4 Y N M F 0.639 Metric Stemming RSW Ave./Med P/R/F r R-L Y Y A F 0.638 R-1 N N A F 0.637 R-S4 Y N M F 0.634 R-4 Y N M P,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 88/126
==========================================================================================
A F 0.649 R-SU4 Y Y M F 0.649 R-SU4 N Y M R 0.646 R-L N Y M P 0.645 R-W N Y M F 0.642 R-W Y Y A F 0.642 R-4 Y N M R 0.641 R-S4 Y Y A R 0.641 R-4 Y N M F 0.639 Metric Stemming RSW Ave./Med P/R/F r R-L Y Y A F 0.638 R-1 N N A F 0.637 R-S4 Y N M F 0.634 R-4 Y N M P",1862796,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,0.651 R-S4 Y Y M F 0.649 R-1 Y Y,Metric Significance Testing,N N M F 0.634 R-SU4 N Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 89/126
==========================================================================================
N N M F 0.634 R-SU4 N Y",1862797,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.649 R-SU4 Y Y M F 0.649 R-SU4 N Y M R 0.646 R-L N Y M P 0.645 R-W N Y M F 0.642 R-W Y Y A F 0.642 R-4 Y N M R 0.641 R-S4 Y Y A R 0.641 R-4 Y N M F 0.639 Metric Stemming RSW Ave./Med P/R/F r R-L Y Y A F 0.638 R-1 N N A F 0.637 R-S4 Y N M F 0.634 R-4 Y N M P,Metric Significance Testing,A R 0.633 R-L Y Y M P 0.633 R-SU4 Y Y M R 0.631 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 90/126
==========================================================================================
A R 0.633 R-L Y Y M P 0.633 R-SU4 Y Y M R 0.631 R-1",1862798,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M F 0.634 R-SU4 N Y,Metric Significance Testing,A F 0.630 R-1 Y Y M F 0.629 R-S4 Y Y M R 0.626 R-S4 N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 91/126
==========================================================================================
A F 0.630 R-1 Y Y M F 0.629 R-S4 Y Y M R 0.626 R-S4 N N",1862799,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.633 R-L Y Y M P 0.633 R-SU4 Y Y M R 0.631 R-1,Metric Significance Testing,A R 0.626 R-SU4 Y N M F 0.625 R-S4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 92/126
==========================================================================================
A R 0.626 R-SU4 Y N M F 0.625 R-S4 Y N",1862800,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.630 R-1 Y Y M F 0.629 R-S4 Y Y M R 0.626 R-S4 N N,Metric Significance Testing,A R 0.624 R-L N Y M F 0.623 R-SU4 Y Y A R 0.622 R-1 Y N M F 0.617 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 93/126
==========================================================================================
A R 0.624 R-L N Y M F 0.623 R-SU4 Y Y A R 0.622 R-1 Y N M F 0.617 R-1",1862801,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.626 R-SU4 Y N M F 0.625 R-S4 Y N,Metric Significance Testing,N Y M R 0.615 R-W N Y A R 0.613 R-S4 N N M R 0.611 R-L N Y M R 0.609 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 94/126
==========================================================================================
N Y M R 0.615 R-W N Y A R 0.613 R-S4 N N M R 0.611 R-L N Y M R 0.609 R-1",1862802,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.624 R-L N Y M F 0.623 R-SU4 Y Y A R 0.622 R-1 Y N M F 0.617 R-1,Metric Significance Testing,N Y A R 0.604 R-L N Y A R 0.601 R-W N N M F 0.600 R-L N N M F 0.599 R-W Y Y A R 0.598 R-W N Y M R 0.597 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 95/126
==========================================================================================
N Y A R 0.604 R-L N Y A R 0.601 R-W N N M F 0.600 R-L N N M F 0.599 R-W Y Y A R 0.598 R-W N Y M R 0.597 R-1",1862803,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M R 0.615 R-W N Y A R 0.613 R-S4 N N M R 0.611 R-L N Y M R 0.609 R-1,Metric Significance Testing,Y Y A R 0.595 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 96/126
==========================================================================================
Y Y A R 0.595 R-1",1862804,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y A R 0.604 R-L N Y A R 0.601 R-W N N M F 0.600 R-L N N M F 0.599 R-W Y Y A R 0.598 R-W N Y M R 0.597 R-1,Metric Significance Testing,Y Y M R 0.591 R-L N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 97/126
==========================================================================================
Y Y M R 0.591 R-L N",1862805,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y A R 0.595 R-1,Metric Significance Testing,N A F 0.586 R-W Y Y M F 0.586 R-W Y N M F 0.585 R-L Y Y A R 0.583 R-L Y Y M F 0.582 R-L Y Y M R 0.579 R-L Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 98/126
==========================================================================================
N A F 0.586 R-W Y Y M F 0.586 R-W Y N M F 0.585 R-L Y Y A R 0.583 R-L Y Y M F 0.582 R-L Y Y M R 0.579 R-L Y N",1862806,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y Y M R 0.591 R-L N,Metric Significance Testing,A F 0.579 R-W N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 99/126
==========================================================================================
A F 0.579 R-W N N",1862807,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N A F 0.586 R-W Y Y M F 0.586 R-W Y N M F 0.585 R-L Y Y A R 0.583 R-L Y Y M F 0.582 R-L Y Y M R 0.579 R-L Y N,Metric Significance Testing,A F 0.579 R-SU4 N N M R 0.576 R-W Y,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 100/126
==========================================================================================
A F 0.579 R-SU4 N N M R 0.576 R-W Y",1862808,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.579 R-W N N,Metric Significance Testing,A F 0.576 R-SU4 N N A R 0.574 R-SU4 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 101/126
==========================================================================================
A F 0.576 R-SU4 N N A R 0.574 R-SU4 Y N",1862809,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.579 R-SU4 N N M R 0.576 R-W Y,Metric Significance Testing,A R 0.571 R-L Y N M F 0.569 R-W Y Y M R 0.567 R-S4 Y N M R 0.566 R-SU4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 102/126
==========================================================================================
A R 0.571 R-L Y N M F 0.569 R-W Y Y M R 0.567 R-S4 Y N M R 0.566 R-SU4",1862810,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A F 0.576 R-SU4 N N A R 0.574 R-SU4 Y N,Metric Significance Testing,Y N M R 0.525 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 103/126
==========================================================================================
Y N M R 0.525 R-1",1862811,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.571 R-L Y N M F 0.569 R-W Y Y M R 0.567 R-S4 Y N M R 0.566 R-SU4,Metric Significance Testing,N N M R 0.488 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 104/126
==========================================================================================
N N M R 0.488 R-1",1862812,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y N M R 0.525 R-1,Metric Significance Testing,Y N M R 0.477 R-W Y N M R 0.477 R-1,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 105/126
==========================================================================================
Y N M R 0.477 R-W Y N M R 0.477 R-1",1862813,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N N M R 0.488 R-1,Metric Significance Testing,A R 0.470 R-W N N M R 0.470 R-L N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 106/126
==========================================================================================
A R 0.470 R-W N N M R 0.470 R-L N",1862814,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Y N M R 0.477 R-W Y N M R 0.477 R-1,Metric Significance Testing,N M R 0.470 R-1 Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 107/126
==========================================================================================
N M R 0.470 R-1 Y N",1862815,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.470 R-W N N M R 0.470 R-L N,Metric Significance Testing,A R 0.459 R-W N N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 108/126
==========================================================================================
A R 0.459 R-W N N",1862816,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N M R 0.470 R-1 Y N,Metric Significance Testing,A R 0.456 R-W Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 109/126
==========================================================================================
A R 0.456 R-W Y N",1862817,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.459 R-W N N,Metric Significance Testing,A R 0.452 R-L Y N M R 0.423 R-L N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 110/126
==========================================================================================
A R 0.452 R-L Y N M R 0.423 R-L N",1862818,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.456 R-W Y N,Metric Significance Testing,A R 0.416 R-L Y N,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 111/126
==========================================================================================
A R 0.416 R-L Y N",1862819,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.452 R-L Y N M R 0.423 R-L N,Metric Significance Testing,A R 0.406 R-4 Y Y M P 0.307 R-4 Y Y M F 0.302 R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 112/126
==========================================================================================
A R 0.406 R-4 Y Y M P 0.307 R-4 Y Y M F 0.302 R-4",1862820,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.416 R-L Y N,Metric Significance Testing,N Y M P 0.301 R-4,Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 113/126
==========================================================================================
N Y M P 0.301 R-4",1862821,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,A R 0.406 R-4 Y Y M P 0.307 R-4 Y Y M F 0.302 R-4,Metric Significance Testing,"Y Y M R 0.297 R-4 N Y M F 0.296 R-4 N Y M R 0.293 outperform, as a metric that happens to correlate strongly with a higher number of competing metrics in a given competition would be at an unfair advantage.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 114/126
==========================================================================================
Y Y M R 0.297 R-4 N Y M F 0.296 R-4 N Y M R 0.293 outperform, as a metric that happens to correlate strongly with a higher number of competing metrics in a given competition would be at an unfair advantage.",1862822,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,N Y M P 0.301 R-4,Metric Significance Testing,"This increased power also means, somewhat counter-intuitively, it can happen for a pair of competing metrics for which the correlation between metric scores is strong, that a small difference in competing correlations with human assessment is significant, while, for a different pair of metrics with a larger difference in correlation, the difference is not significant, because r(M base , M new ) is weak.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 115/126
==========================================================================================
This increased power also means, somewhat counter-intuitively, it can happen for a pair of competing metrics for which the correlation between metric scores is strong, that a small difference in competing correlations with human assessment is significant, while, for a different pair of metrics with a larger difference in correlation, the difference is not significant, because r(M base , M new ) is weak.",1862823,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Y Y M R 0.297 R-4 N Y M F 0.296 R-4 N Y M R 0.293 outperform, as a metric that happens to correlate strongly with a higher number of competing metrics in a given competition would be at an unfair advantage.",Metric Significance Testing,"For example, in Table 1 the difference in correlation with human assessment of BLEU and that of median ROUGE-L precision with stemming and stop-words retained, 0.141 (0.797 − 0.656), is not significant, while the smaller difference in correlation with human assessment between correlations of BLEU and average ROUGE-3 recall with stemming and stopwords removed, 0.067 (0.797 − 0.73) is significant, since scores of the latter pair of metrics correlate with one another with more strength.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 116/126
==========================================================================================
For example, in Table 1 the difference in correlation with human assessment of BLEU and that of median ROUGE-L precision with stemming and stop-words retained, 0.141 (0.797 − 0.656), is not significant, while the smaller difference in correlation with human assessment between correlations of BLEU and average ROUGE-3 recall with stemming and stopwords removed, 0.067 (0.797 − 0.73) is significant, since scores of the latter pair of metrics correlate with one another with more strength.",1862824,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"This increased power also means, somewhat counter-intuitively, it can happen for a pair of competing metrics for which the correlation between metric scores is strong, that a small difference in competing correlations with human assessment is significant, while, for a different pair of metrics with a larger difference in correlation, the difference is not significant, because r(M base , M new ) is weak.",Metric Significance Testing,"As part of this research, we have made available an open-source implementation of statistical tests for evaluation of summarization metrics, at https://github.com/ygraham/nlp-williams.",Metric Significance Testing,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 117/126
==========================================================================================
As part of this research, we have made available an open-source implementation of statistical tests for evaluation of summarization metrics, at https://github.com/ygraham/nlp-williams.",1862825,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"For example, in Table 1 the difference in correlation with human assessment of BLEU and that of median ROUGE-L precision with stemming and stop-words retained, 0.141 (0.797 − 0.656), is not significant, while the smaller difference in correlation with human assessment between correlations of BLEU and average ROUGE-3 recall with stemming and stopwords removed, 0.067 (0.797 − 0.73) is significant, since scores of the latter pair of metrics correlate with one another with more strength.",Metric Significance Testing,"In Table 1, • identifies variants of ROUGE not significantly outperformed by any other variant.",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 118/126
==========================================================================================
0. Significance Test Results -- 1/9
------------------------------------------------------------------------------------------
In Table 1, • identifies variants of ROUGE not significantly outperformed by any other variant.",1862826,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"As part of this research, we have made available an open-source implementation of statistical tests for evaluation of summarization metrics, at https://github.com/ygraham/nlp-williams.",Metric Significance Testing,"Figure 3 shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants (Owczarzak et al.",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 119/126
==========================================================================================
0. Significance Test Results -- 2/9
------------------------------------------------------------------------------------------
Figure 3 shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants (Owczarzak et al.",1862827,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"In Table 1, • identifies variants of ROUGE not significantly outperformed by any other variant.",Significance Test Results,(2012)) used to compare systems in Hong et al.,Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 120/126
==========================================================================================
0. Significance Test Results -- 3/9
------------------------------------------------------------------------------------------
(2012)) used to compare systems in Hong et al.",1862828,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Figure 3 shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants (Owczarzak et al.",Significance Test Results,Current recommended best variants of ROUGE are shown to be significantly outperformed by several other ROUGE variants. ,Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 121/126
==========================================================================================
0. Significance Test Results -- 4/9
------------------------------------------------------------------------------------------
Current recommended best variants of ROUGE are shown to be significantly outperformed by several other ROUGE variants. ",1862829,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,(2012)) used to compare systems in Hong et al.,Significance Test Results,"Although BLEU achieves strongest correlation with human assessment overall, Figure 3 reveals the difference between BLEU's correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems, for this reason alone we recommend the use of the best-performing ROUGE variant over BLEU, average ROUGE-2 precision with stemming and stopwords removed.  ",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 122/126
==========================================================================================
0. Significance Test Results -- 5/9
------------------------------------------------------------------------------------------
Although BLEU achieves strongest correlation with human assessment overall, Figure 3 reveals the difference between BLEU's correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems, for this reason alone we recommend the use of the best-performing ROUGE variant over BLEU, average ROUGE-2 precision with stemming and stopwords removed.  ",1862830,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Current recommended best variants of ROUGE are shown to be significantly outperformed by several other ROUGE variants. ,Significance Test Results,"addition, since data used to evaluate systems are not independent, paired tests are also appropriate (Rankel et al., 2011).",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 123/126
==========================================================================================
0. Significance Test Results -- 6/9
------------------------------------------------------------------------------------------
addition, since data used to evaluate systems are not independent, paired tests are also appropriate (Rankel et al., 2011).",1862831,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Although BLEU achieves strongest correlation with human assessment overall, Figure 3 reveals the difference between BLEU's correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems, for this reason alone we recommend the use of the best-performing ROUGE variant over BLEU, average ROUGE-2 precision with stemming and stopwords removed.  ",Significance Test Results,"ROUGE score distributions for systems were tested for normality using the Shapiro-Wilk test (Royston, 1982) where score distributions for none of the included systems were shown to be significantly non-normal. ",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 124/126
==========================================================================================
0. Significance Test Results -- 7/9
------------------------------------------------------------------------------------------
ROUGE score distributions for systems were tested for normality using the Shapiro-Wilk test (Royston, 1982) where score distributions for none of the included systems were shown to be significantly non-normal. ",1862832,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"addition, since data used to evaluate systems are not independent, paired tests are also appropriate (Rankel et al., 2011).",Significance Test Results,"Figure 4 shows outcomes of paired t-tests for summary score distributions of each pair of systems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM.",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 125/126
==========================================================================================
0. Significance Test Results -- 8/9
------------------------------------------------------------------------------------------
Figure 4 shows outcomes of paired t-tests for summary score distributions of each pair of systems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM.",1862833,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"ROUGE score distributions for systems were tested for normality using the Shapiro-Wilk test (Royston, 1982) where score distributions for none of the included systems were shown to be significantly non-normal. ",Significance Test Results,"In addition, as expected, all state-of-the-art systems significantly outperform all baseline systems.",Significance Test Results,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
4. Metric Significance Testing -- 126/126
==========================================================================================
0. Significance Test Results -- 9/9
------------------------------------------------------------------------------------------
In addition, as expected, all state-of-the-art systems significantly outperform all baseline systems.",1862834,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Figure 4 shows outcomes of paired t-tests for summary score distributions of each pair of systems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM.",Significance Test Results,"In order to evaluate metrics by correlation with human assessment, it is necessary to obtain a single human evaluation score per system.",Human Assessment Combinations,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
6. Conclusions -- 1/5
==========================================================================================
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE.",1862835,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,with stemming and stop words removed is not significantly outperformed by BLEU or any other variant of ROUGE for any of the three combined mean human assessment scores.,Human Assessment Combinations,"Detail of the first suitable summarization metric significance test, Williams test, was provided.",Conclusions,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
6. Conclusions -- 2/5
==========================================================================================
Detail of the first suitable summarization metric significance test, Williams test, was provided.",1862836,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE.,Conclusions,Results reveal superior variants of metrics distinct from previously best recommendations.,Conclusions,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
6. Conclusions -- 3/5
==========================================================================================
Results reveal superior variants of metrics distinct from previously best recommendations.",1862837,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,"Detail of the first suitable summarization metric significance test, Williams test, was provided.",Conclusions,Replication of a recent evaluation of state-of-the-art summarization systems also revealed contrasting conclusions about the relative performance of systems.,Conclusions,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
6. Conclusions -- 4/5
==========================================================================================
Replication of a recent evaluation of state-of-the-art summarization systems also revealed contrasting conclusions about the relative performance of systems.",1862838,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Results reveal superior variants of metrics distinct from previously best recommendations.,Conclusions,"In addition, BLEU achieves strongest correlation with human assessment overall, but does not significantly outperform the best-performing ROUGE variant.",Conclusions,
"Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}
==========================================================================================
6. Conclusions -- 5/5
==========================================================================================
In addition, BLEU achieves strongest correlation with human assessment overall, but does not significantly outperform the best-performing ROUGE variant.",1862839,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,Replication of a recent evaluation of state-of-the-art summarization systems also revealed contrasting conclusions about the relative performance of systems.,Conclusions,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}>>. Please answer following questions: 

1. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its technicity / because it handles subjects you are unfamiliar with ?

Please add any label of your choice if your answer is yes.",1862839,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,,,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}>>. Please answer following questions: 

2. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its writing style / structure / parsing errors ?

Please add any label of your choice if your answer is yes.",1862839,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,,,,,
"==========================================================================================
Annotator feedback
==========================================================================================

You just finished annotating the article entitled <<Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}>>. Please answer following questions: 

3. Did you know / read the article before this annotation task, or do you think you have identified its authors ?

Please add any label of your choice if your answer is yes.",1862839,Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE},"0. abstract
1. Introduction
2. Related Work
3. Summarization Metric Evaluation
4. Metric Significance Testing
5. Human Assessment Combinations
6. Conclusions
",2015,,,,,
