id,text,doccano_art_id,sentence_id,current_sentence_section,previous_sentence_section,previous_sentence,next_sentence_section,next_sentence,label,Comments
3130,In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos.,0,0,abstract,,,abstract,"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",FACT,
3131,"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",0,1,abstract,abstract,In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos.,abstract,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",FACT,
3132,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",0,2,abstract,abstract,"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",abstract,"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.",POS,
3133,"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.",0,3,abstract,abstract,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",Introduction,"One of the problems relating to sign language recognition is the lack of appropriate datasets for algorithm training, since most datasets are recorded for academic purposes and as such, they concentrate in human learning rather than machine learning.",POS,
3134,"One of the problems relating to sign language recognition is the lack of appropriate datasets for algorithm training, since most datasets are recorded for academic purposes and as such, they concentrate in human learning rather than machine learning.",0,4,Introduction,abstract,"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.",Introduction,"Therefore, most data collections contain a very large number of different glosses with very few repetitions of each.",NC,
3135,"Therefore, most data collections contain a very large number of different glosses with very few repetitions of each.",0,5,Introduction,Introduction,"One of the problems relating to sign language recognition is the lack of appropriate datasets for algorithm training, since most datasets are recorded for academic purposes and as such, they concentrate in human learning rather than machine learning.",Introduction,This characteristic makes it very unlikely for these datasets to be used as training sets for classification algorithms in sign recognition level.,NC,
3136,This characteristic makes it very unlikely for these datasets to be used as training sets for classification algorithms in sign recognition level.,0,6,Introduction,Introduction,"Therefore, most data collections contain a very large number of different glosses with very few repetitions of each.",Introduction,"Thus, we developed a system in the direction of ""phonological"" features recognition.",NC,
3137,"Thus, we developed a system in the direction of ""phonological"" features recognition.",0,7,Introduction,Introduction,This characteristic makes it very unlikely for these datasets to be used as training sets for classification algorithms in sign recognition level.,Introduction,"This way we can extract a dataset with a lot of examples for every handshape, palm orientation and hand location out of the video collections.",FACT,
3138,"This way we can extract a dataset with a lot of examples for every handshape, palm orientation and hand location out of the video collections.",0,8,Introduction,Introduction,"Thus, we developed a system in the direction of ""phonological"" features recognition.",Datasets,For the purposes of the project two collections of single gloss videos were used as datasets.,POS,
3139,"OpenPose is a software freely distributed by Carnegie Melon University, Perceptual Computing Lab (Cao et al., 2018).",0,22,Openpose,Datasets,This feature is making the whole process easier when trying to split the dictionary into handshape classes.,Openpose,It is used as a tool of human body keypoints extraction from a single image or video frame.,NC,
3140,It is used as a tool of human body keypoints extraction from a single image or video frame.,0,23,Openpose,Openpose,"OpenPose is a software freely distributed by Carnegie Melon University, Perceptual Computing Lab (Cao et al., 2018).",Openpose,"It offers an estimation of 25 body/foot keypoints, 2x21 hand keypoints and 70 face keypoints.",NC,
3141,"It offers an estimation of 25 body/foot keypoints, 2x21 hand keypoints and 70 face keypoints.",0,24,Openpose,Openpose,It is used as a tool of human body keypoints extraction from a single image or video frame.,Openpose,"In the case of a 2D video input, for each keypoint it returns a vector containing 3 elements.",NC,
3142,"In the case of a 2D video input, for each keypoint it returns a vector containing 3 elements.",0,25,Openpose,Openpose,"It offers an estimation of 25 body/foot keypoints, 2x21 hand keypoints and 70 face keypoints.",Openpose,"The first 2 correspond to the (x,y) coordinates with reference to the upper left corner of the image.",NC,
3143,"The first 2 correspond to the (x,y) coordinates with reference to the upper left corner of the image.",0,26,Openpose,Openpose,"In the case of a 2D video input, for each keypoint it returns a vector containing 3 elements.",Openpose,"The third is a value in the range [0,1] which is quantification of the confidence given by the program that the specific keypoint is correctly located in the frame.",NC,
3144,"The third is a value in the range [0,1] which is quantification of the confidence given by the program that the specific keypoint is correctly located in the frame.",0,27,Openpose,Openpose,"The first 2 correspond to the (x,y) coordinates with reference to the upper left corner of the image.",Openpose,The novelty behind OpenPose relies on the fact that it works for more than one person per image but more importantly the keypoint analysis is not affected when part of the individual's body is out of frame.,NC,
3145,The novelty behind OpenPose relies on the fact that it works for more than one person per image but more importantly the keypoint analysis is not affected when part of the individual's body is out of frame.,0,28,Openpose,Openpose,"The third is a value in the range [0,1] which is quantification of the confidence given by the program that the specific keypoint is correctly located in the frame.",Openpose,This last feature is crucial for applications on sign language videos where the signer appears above the waist level (Figure 1).,NC,
3146,This last feature is crucial for applications on sign language videos where the signer appears above the waist level (Figure 1).,0,29,Openpose,Openpose,The novelty behind OpenPose relies on the fact that it works for more than one person per image but more importantly the keypoint analysis is not affected when part of the individual's body is out of frame.,Our Method,The first step in our method is transforming each video frame into keypoints using the OpenPose software.,NC,
3147,Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English.,1,0,abstract,,,abstract,"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.",NC,
3148,"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.",1,1,abstract,abstract,Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English.,abstract,"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.",NC,
3149,"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.",1,2,abstract,abstract,"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.",abstract,This degrades translation accuracy.,NC,
3150,This degrades translation accuracy.,1,3,abstract,abstract,"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.",abstract,"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.",NC,
3151,"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.",1,4,abstract,abstract,This degrades translation accuracy.,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.",POS,
3152,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.",1,5,abstract,abstract,"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.",abstract,1,POS,
3153,1,1,6,abstract,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.",Introduction,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008).",NC,
3154,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008).",1,7,Introduction,abstract,1,Introduction,"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English.",NC,
3155,"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English.",1,8,Introduction,Introduction,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008).",Introduction,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",NC,
3156,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",1,9,Introduction,Introduction,"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English.",Introduction,"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",NC,
3157,"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",1,10,Introduction,Introduction,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",Introduction,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015).",NC,"Ce n'est pas un claim as such, mais je me rends compte qu'en citant de cette manière, on influence la suite alors qu'on ne parle pas de la recherche menée, mais de l'état de l'art"
3158,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015).",1,11,Introduction,Introduction,"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",Introduction,"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora.",NC,
3159,"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora.",1,12,Introduction,Introduction,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015).",Introduction,"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists.",NC,
3160,"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists.",1,13,Introduction,Introduction,"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora.",Introduction,"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms.",NC,
3161,"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms.",1,14,Introduction,Introduction,"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists.",Introduction,"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context.",NC,
3162,"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context.",1,15,Introduction,Introduction,"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms.",Introduction,"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given.",NC,
3163,"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given.",1,16,Introduction,Introduction,"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context.",Introduction,This varying syntactic structure will affect translation.,NC,
3164,This varying syntactic structure will affect translation.,1,17,Introduction,Introduction,"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given.",Introduction,"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun.",NC,
3165,"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun.",1,18,Introduction,Introduction,This varying syntactic structure will affect translation.,Introduction,"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word.",NC,
3166,"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word.",1,19,Introduction,Introduction,"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun.",Introduction,"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change.",NC,
3167,"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change.",1,20,Introduction,Introduction,"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word.",Introduction,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",NC,
3168,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",1,21,Introduction,Introduction,"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change.",Introduction,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",NC,
3169,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",1,22,Introduction,Introduction,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",Introduction,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",NC,
3170,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",1,23,Introduction,Introduction,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",Introduction,"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b).",PROSP,Hypothèse =? prospective ?
3171,"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b).",1,24,Introduction,Introduction,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",Introduction,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",FACT,
3172,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",1,25,Introduction,Introduction,"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b).",Introduction,"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",POS,
3173,"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",1,26,Introduction,Introduction,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",Introduction,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",FACT,
3174,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",1,27,Introduction,Introduction,"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",NC,
3175,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",1,28,Introduction,Introduction,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",Introduction,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",NC,
3176,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",1,29,Introduction,Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",Introduction,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",POS,
3177,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",1,30,Introduction,Introduction,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",Synchronous Context-Free Grammars,"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007).",POS,
3178,"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007).",1,31,Synchronous Context-Free Grammars,Introduction,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",Synchronous Context-Free Grammars,"In SCFGs, the elementary structures used in translation are synchronous rewrite rules with aligned pairs of source and target symbols on the right-hand side: X → ⟨ s, t ⟩ (1) where X is the head symbol of the rewrite rule, and s and t are both strings of terminals and nonterminals on the source and target side respectively.",NC,
3179,"In SCFGs, the elementary structures used in translation are synchronous rewrite rules with aligned pairs of source and target symbols on the right-hand side: X → ⟨ s, t ⟩ (1) where X is the head symbol of the rewrite rule, and s and t are both strings of terminals and nonterminals on the source and target side respectively.",1,32,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007).",Synchronous Context-Free Grammars,"Each string in the right side pair has the same number of indexed non-terminals, and identically indexed non-terminals correspond to eachother.",,
3180,"Each string in the right side pair has the same number of indexed non-terminals, and identically indexed non-terminals correspond to eachother.",1,33,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"In SCFGs, the elementary structures used in translation are synchronous rewrite rules with aligned pairs of source and target symbols on the right-hand side: X → ⟨ s, t ⟩ (1) where X is the head symbol of the rewrite rule, and s and t are both strings of terminals and nonterminals on the source and target side respectively.",Synchronous Context-Free Grammars,"For example, a synchronous rule could take the form of: X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",,
3181,"For example, a synchronous rule could take the form of: X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",1,34,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"Each string in the right side pair has the same number of indexed non-terminals, and identically indexed non-terminals correspond to eachother.",Synchronous Context-Free Grammars,Synchronous rules can be extracted based on parallel sentences and automatically obtained word alignments.,,
3182,Synchronous rules can be extracted based on parallel sentences and automatically obtained word alignments.,1,35,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"For example, a synchronous rule could take the form of: X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",Synchronous Context-Free Grammars,"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1.",,
3183,"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1.",1,36,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,Synchronous rules can be extracted based on parallel sentences and automatically obtained word alignments.,Synchronous Context-Free Grammars,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",,
3184,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",1,37,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1.",Synchronous Context-Free Grammars,"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",,
3185,"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",1,38,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",Synchronous Context-Free Grammars,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007).",,
3186,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007).",1,39,Synchronous Context-Free Grammars,Synchronous Context-Free Grammars,"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",Hierarchical Rules,"In this section, we specifically cover the rules used in Hiero.",,
3187,"In this section, we specifically cover the rules used in Hiero.",1,40,Hierarchical Rules,Synchronous Context-Free Grammars,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007).",Hierarchical Rules,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X.",,
3188,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X.",1,41,Hierarchical Rules,Hierarchical Rules,"In this section, we specifically cover the rules used in Hiero.",Hierarchical Rules,"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007).",,
3189,"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007).",1,42,Hierarchical Rules,Hierarchical Rules,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X.",Hierarchical Rules,"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",,
3190,"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",1,43,Hierarchical Rules,Hierarchical Rules,"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007).",Hierarchical Rules,"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing.",,
3191,"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing.",1,44,Hierarchical Rules,Hierarchical Rules,"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",Hierarchical Rules,"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations.",,
3192,"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations.",1,45,Hierarchical Rules,Hierarchical Rules,"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing.",Explicitly Syntactic Rules,"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules).",,
3193,"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules).",1,46,Explicitly Syntactic Rules,Hierarchical Rules,"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations.",Explicitly Syntactic Rules,"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree.",,
3194,"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree.",1,47,Explicitly Syntactic Rules,Explicitly Syntactic Rules,"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules).",Explicitly Syntactic Rules,"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ .",,
3195,"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ .",1,48,Explicitly Syntactic Rules,Explicitly Syntactic Rules,"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree.",Explicitly Syntactic Rules,"Here, parse subtrees of the source language rules are given in the form of S-expressions.",,
3196,"Here, parse subtrees of the source language rules are given in the form of S-expressions.",1,49,Explicitly Syntactic Rules,Explicitly Syntactic Rules,"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ .",Explicitly Syntactic Rules,"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar.",,
3197,"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar.",1,50,Explicitly Syntactic Rules,Explicitly Syntactic Rules,"Here, parse subtrees of the source language rules are given in the form of S-expressions.",Explicitly Syntactic Rules,"This method also has the advantage the number of less-useful synchronous rules extracted by syntax-agnostic methods such as Hiero are reduced, making it possible to learn more compact rule tables and allowing for faster translation.",,
3198,"This method also has the advantage the number of less-useful synchronous rules extracted by syntax-agnostic methods such as Hiero are reduced, making it possible to learn more compact rule tables and allowing for faster translation.",1,51,Explicitly Syntactic Rules,Explicitly Syntactic Rules,"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar.",Standard Triangulation Method,"In the triangulation method by Cohn and Lapata (2007), we first train source-pivot and pivot-target rule tables as T SP and T P T respectively.",,
3199,"In the previous section, we explained about the standard triangulation method and mentioned that the pivot-side ambiguity causes incorrect estimation of translation probability and the translation accuracy might decrease.",1,59,Triangulation with Syntactic Matching,Standard Triangulation Method,"Specifically, if there are multiple interpretations of the pivot phrase as shown in the example of Figure 1, source and target phrases that do not correspond to each other semantically might be connected, and over-estimation by summing products of the translation probabilities is likely to cause failed translations.",Triangulation with Syntactic Matching,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",,
3200,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",1,60,Triangulation with Syntactic Matching,Triangulation with Syntactic Matching,"In the previous section, we explained about the standard triangulation method and mentioned that the pivot-side ambiguity causes incorrect estimation of translation probability and the translation accuracy might decrease.",Triangulation with Syntactic Matching,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching.",,
3201,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching.",1,61,Triangulation with Syntactic Matching,Triangulation with Syntactic Matching,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",Exact Matching of Parse Subtrees,"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively.",,
3202,"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively.",1,62,Exact Matching of Parse Subtrees,Triangulation with Syntactic Matching,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching.",Exact Matching of Parse Subtrees,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",,
3203,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",1,63,Exact Matching of Parse Subtrees,Exact Matching of Parse Subtrees,"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively.",Exact Matching of Parse Subtrees,"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2.",,
3204,"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2.",1,64,Exact Matching of Parse Subtrees,Exact Matching of Parse Subtrees,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",Exact Matching of Parse Subtrees,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",,
3205,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",1,65,Exact Matching of Parse Subtrees,Exact Matching of Parse Subtrees,"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2.",Exact Matching of Parse Subtrees,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",,
3206,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",1,66,Exact Matching of Parse Subtrees,Exact Matching of Parse Subtrees,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",Exact Matching of Parse Subtrees,Therefore it is important to create TMs that are both reliabile and have high coverage.,,
3207,Therefore it is important to create TMs that are both reliabile and have high coverage.,1,67,Exact Matching of Parse Subtrees,Exact Matching of Parse Subtrees,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",Partial Matching of Parse Subtrees,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees.",,
3208,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees.",1,68,Partial Matching of Parse Subtrees,Exact Matching of Parse Subtrees,Therefore it is important to create TMs that are both reliabile and have high coverage.,Partial Matching of Parse Subtrees,"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively.",,
3209,"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively.",1,69,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees.",Partial Matching of Parse Subtrees,"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections.",,
3210,"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections.",1,70,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively.",Partial Matching of Parse Subtrees,"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14).",,
3211,"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14).",1,71,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections.",Partial Matching of Parse Subtrees,"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available.",,
3212,"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available.",1,72,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14).",Partial Matching of Parse Subtrees,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998).",,
3213,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998).",1,73,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available.",Partial Matching of Parse Subtrees,"According to equations ( 21)-( 24), we can assure that incomplete match of pivot subtrees leads d(•) ≥ 1 and penalizes such that ψ(•) ≤ 1/e d ≤ 1/e, while exact match of subtrees leads to a value of ψ(•) at least e ≈ 2.718 times larger than when using partially matched subtrees.",,
3214,"According to equations ( 21)-( 24), we can assure that incomplete match of pivot subtrees leads d(•) ≥ 1 and penalizes such that ψ(•) ≤ 1/e d ≤ 1/e, while exact match of subtrees leads to a value of ψ(•) at least e ≈ 2.718 times larger than when using partially matched subtrees.",1,74,Partial Matching of Parse Subtrees,Partial Matching of Parse Subtrees,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998).",Experimental Set-Up,"To investigate the effect of our proposed approach, we evaluate the translation accuracy through pivot translation experiments on the United Nations Parallel Corpus (UN6Way) (Ziemski et al., 2016).",,
3215,Translating with a Hiero TM directly trained on the source-target parallel corpus without using pivot language (as an oracle).,1,92,Direct:,Experimental Set-Up,We evaluate 6 translation methods:,Tri. Hiero:,"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3).",,
3216,"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3).",1,93,Tri. Hiero:,Direct:,Translating with a Hiero TM directly trained on the source-target parallel corpus without using pivot language (as an oracle).,Tri. TreeExact,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed exact matching of pivot subtrees (proposed 1, §4.1).",,
3217,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed exact matching of pivot subtrees (proposed 1, §4.1).",1,94,Tri. TreeExact,Tri. Hiero:,"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3).",Tri. TreePartial,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed partial matching of pivot subtrees (proposed 2, §4.2).",,
3218,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed partial matching of pivot subtrees (proposed 2, §4.2).",1,95,Tri. TreePartial,Tri. TreeExact,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed exact matching of pivot subtrees (proposed 1, §4.1).",Experimental Results,The result of experiments using all combinations of pivot translation tasks for 5 languages via English is shown in Table 1.,,
3219,The result of experiments using all combinations of pivot translation tasks for 5 languages via English is shown in Table 1.,1,96,Experimental Results,Tri. TreePartial,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed partial matching of pivot subtrees (proposed 2, §4.2).",Experimental Results,"From the results, we can see that the proposed partial matching method of pivot subtrees in triangulation outperforms the standard triangulation method for all language pairs and achieves higher or almost equal scores than proposed exact matching method.",,
3220,"From the results, we can see that the proposed partial matching method of pivot subtrees in triangulation outperforms the standard triangulation method for all language pairs and achieves higher or almost equal scores than proposed exact matching method.",1,97,Experimental Results,Experimental Results,The result of experiments using all combinations of pivot translation tasks for 5 languages via English is shown in Table 1.,Experimental Results,"The exact matching method also outperforms the standard triangulation method in the majority of the language pairs, but has a lesser improvement than partial matching method.",,
3221,"The exact matching method also outperforms the standard triangulation method in the majority of the language pairs, but has a lesser improvement than partial matching method.",1,98,Experimental Results,Experimental Results,"From the results, we can see that the proposed partial matching method of pivot subtrees in triangulation outperforms the standard triangulation method for all language pairs and achieves higher or almost equal scores than proposed exact matching method.",Experimental Results,In Table 2 we show the comparison of coverage of each proposed triangulated method.,,
3222,In Table 2 we show the comparison of coverage of each proposed triangulated method.,1,99,Experimental Results,Experimental Results,"The exact matching method also outperforms the standard triangulation method in the majority of the language pairs, but has a lesser improvement than partial matching method.",Experimental Results,"From this table, we can see that the exact matching method reduces several percent in number of unique phrases while the partial matching method keeps the same coverage with surfaceform matching.",,
3223,"From this table, we can see that the exact matching method reduces several percent in number of unique phrases while the partial matching method keeps the same coverage with surfaceform matching.",1,100,Experimental Results,Experimental Results,In Table 2 we show the comparison of coverage of each proposed triangulated method.,Experimental Results,We can consider that it is one of the reasons of the difference in improvement stability between the partial and exact matching methods.,,
3224,We can consider that it is one of the reasons of the difference in improvement stability between the partial and exact matching methods.,1,101,Experimental Results,Experimental Results,"From this table, we can see that the exact matching method reduces several percent in number of unique phrases while the partial matching method keeps the same coverage with surfaceform matching.",Experimental Results,We show an example of a translated sentences for which pivot-side ambiguity is resolved in the the syntactic matching methods:,,
3225,We show an example of a translated sentences for which pivot-side ambiguity is resolved in the the syntactic matching methods:,1,102,Experimental Results,Experimental Results,We can consider that it is one of the reasons of the difference in improvement stability between the partial and exact matching methods.,Reference in Spanish:,Suiza alienta a todos los Estados partes : a ::: que ::::::: apoyen ::: la :::::: actual ::::: labor :::::::::: conceptual ::: de :: la ::::::::: Secretaría .,,
3226,"Recent results (Firat et al., 2016;Johnson et al., 2016) have found that neural machine translation systems can gain the ability to perform translation with zero parallel resources by training on multiple sets of bilingual data.",1,107,Comparison with Neural MT:,Reference in Spanish:,We can confirm that the derivation improves word-selection and word-reordering by using this rule.,Comparison with Neural MT:,"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero.",,
3227,"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero.",1,108,Comparison with Neural MT:,Comparison with Neural MT:,"Recent results (Firat et al., 2016;Johnson et al., 2016) have found that neural machine translation systems can gain the ability to perform translation with zero parallel resources by training on multiple sets of bilingual data.",Comparison with Neural MT:,"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al.",,
3228,"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al.",1,109,Comparison with Neural MT:,Comparison with Neural MT:,"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero.",Comparison with Neural MT:,"To train and evaluate NMT models, we adopt NMTKit.",,
3229,"To train and evaluate NMT models, we adopt NMTKit.",1,110,Comparison with Neural MT:,Comparison with Neural MT:,"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al.",Comparison with Neural MT:,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training.",,
3230,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training.",1,111,Comparison with Neural MT:,Comparison with Neural MT:,"To train and evaluate NMT models, we adopt NMTKit.",Comparison with Neural MT:,"Cascade is one immediate method connecting two TMs, and NMT cascade translation shows the medium performance in this experiment.",,
3231,"Cascade is one immediate method connecting two TMs, and NMT cascade translation shows the medium performance in this experiment.",1,112,Comparison with Neural MT:,Comparison with Neural MT:,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training.",Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker.",,
3232,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker.",1,113,Comparison with Neural MT:,Comparison with Neural MT:,"Cascade is one immediate method connecting two TMs, and NMT cascade translation shows the medium performance in this experiment.",Comparison with Neural MT:,"This may be because we used only 1 LSTM layer for encoder/decoder, or because the amount of parallel corpora or language pairs were not sufficient.",,
3233,"This may be because we used only 1 LSTM layer for encoder/decoder, or because the amount of parallel corpora or language pairs were not sufficient.",1,114,Comparison with Neural MT:,Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker.",Comparison with Neural MT:,"Thus, we can posit that while zero-shot translation has demonstrated reasonable results in some settings, successful zero-shot translation systems are far from trivial to build, and pivot-based symbolic MT systems such as PBMT or Hiero may still be a competitive alternative.",,
3234,"Thus, we can posit that while zero-shot translation has demonstrated reasonable results in some settings, successful zero-shot translation systems are far from trivial to build, and pivot-based symbolic MT systems such as PBMT or Hiero may still be a competitive alternative.",1,115,Comparison with Neural MT:,Comparison with Neural MT:,"This may be because we used only 1 LSTM layer for encoder/decoder, or because the amount of parallel corpora or language pairs were not sufficient.",Conclusion,"In this paper, we have proposed a method of pivot translation using triangulation with exact or partial matching method of pivot-side parse subtrees.",,
3235,"In this paper, we have proposed a method of pivot translation using triangulation with exact or partial matching method of pivot-side parse subtrees.",1,116,Conclusion,Comparison with Neural MT:,"Thus, we can posit that while zero-shot translation has demonstrated reasonable results in some settings, successful zero-shot translation systems are far from trivial to build, and pivot-based symbolic MT systems such as PBMT or Hiero may still be a competitive alternative.",Conclusion,"In experiments, we found that these triangulated models are effective in particular when allowing partial matching.",,
3236,"In experiments, we found that these triangulated models are effective in particular when allowing partial matching.",1,117,Conclusion,Conclusion,"In this paper, we have proposed a method of pivot translation using triangulation with exact or partial matching method of pivot-side parse subtrees.",Conclusion,"To estimate translation probabilities, we introduced heuristic that has no guarantee to be optimal.",,
3237,"To estimate translation probabilities, we introduced heuristic that has no guarantee to be optimal.",1,118,Conclusion,Conclusion,"In experiments, we found that these triangulated models are effective in particular when allowing partial matching.",Conclusion,"Therefore in the future, we plan to explore more refined estimation methods that utilize machine learning.",,
3238,"Therefore in the future, we plan to explore more refined estimation methods that utilize machine learning.",1,119,Conclusion,Conclusion,"To estimate translation probabilities, we introduced heuristic that has no guarantee to be optimal.",,,,
3239,"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",3,0,abstract,,,abstract,"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation.",,
3240,"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation.",3,1,abstract,abstract,"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",abstract,"In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time.",,
3241,"In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time.",3,2,abstract,abstract,"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation.",abstract,"Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process.",,
3242,"Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process.",3,3,abstract,abstract,"In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time.",abstract,"We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.",,
3243,"We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.",3,4,abstract,abstract,"Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process.",INTRODUCTION,"Online automatic speech recognition (ASR), which immediately recognizes incomplete speeches as humans do, is emerging as a core element of diverse ASR-based services such as teleconferences, AI secretaries, or AI booking services.",,
3244,"Online automatic speech recognition (ASR), which immediately recognizes incomplete speeches as humans do, is emerging as a core element of diverse ASR-based services such as teleconferences, AI secretaries, or AI booking services.",3,5,INTRODUCTION,abstract,"We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.",INTRODUCTION,"In particular, in these days, where the untact service market is rapidly growing due to the recent global outbreak of COVID-19, the importance of providing more realistic services by reducing latency is also growing.",,
3245,"In particular, in these days, where the untact service market is rapidly growing due to the recent global outbreak of COVID-19, the importance of providing more realistic services by reducing latency is also growing.",3,6,INTRODUCTION,INTRODUCTION,"Online automatic speech recognition (ASR), which immediately recognizes incomplete speeches as humans do, is emerging as a core element of diverse ASR-based services such as teleconferences, AI secretaries, or AI booking services.",INTRODUCTION,"However, of course, online ASR models [1,2] targeting real-time inference have concerns about performance degradation compared to traditional Copyright 2021 IEEE.",,
3246,"However, of course, online ASR models [1,2] targeting real-time inference have concerns about performance degradation compared to traditional Copyright 2021 IEEE.",3,7,INTRODUCTION,INTRODUCTION,"In particular, in these days, where the untact service market is rapidly growing due to the recent global outbreak of COVID-19, the importance of providing more realistic services by reducing latency is also growing.",INTRODUCTION,"Published in ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 6-11 June 2021 in Toronto, Ontario, Canada.",,
3247,"Published in ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 6-11 June 2021 in Toronto, Ontario, Canada.",3,8,INTRODUCTION,INTRODUCTION,"However, of course, online ASR models [1,2] targeting real-time inference have concerns about performance degradation compared to traditional Copyright 2021 IEEE.",INTRODUCTION,Personal use of this material is permitted.,,
3248,Personal use of this material is permitted.,3,9,INTRODUCTION,INTRODUCTION,"Published in ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 6-11 June 2021 in Toronto, Ontario, Canada.",INTRODUCTION,"However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE.",,
3249,"However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE.",3,10,INTRODUCTION,INTRODUCTION,Personal use of this material is permitted.,INTRODUCTION,"Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O.",,
3250,"Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O.",3,11,INTRODUCTION,INTRODUCTION,"However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE.",INTRODUCTION,"Box 1331 / Piscataway, NJ 08855-1331, USA.",,
3251,"Box 1331 / Piscataway, NJ 08855-1331, USA.",3,12,INTRODUCTION,INTRODUCTION,"Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O.",INTRODUCTION,Telephone: + Intl.,,
3252,Telephone: + Intl.,3,13,INTRODUCTION,INTRODUCTION,"Box 1331 / Piscataway, NJ 08855-1331, USA.",INTRODUCTION,908-562-3966.,,
3253,908-562-3966.,3,14,INTRODUCTION,INTRODUCTION,Telephone: + Intl.,INTRODUCTION,"DNN-HMM hybrid models with pre-segmented alignments or offline models based on Transformer [3,4], which is the state-of-the-art in many sequence-sequence tasks nowadays.",,
3254,"DNN-HMM hybrid models with pre-segmented alignments or offline models based on Transformer [3,4], which is the state-of-the-art in many sequence-sequence tasks nowadays.",3,15,INTRODUCTION,INTRODUCTION,908-562-3966.,INTRODUCTION,"In order to overcome this performance-delay trade-off, several attempts have been made to learn or find monotonic alignments between source and target via attention mechanism [5,6,7,8,9].",,
3255,"In order to overcome this performance-delay trade-off, several attempts have been made to learn or find monotonic alignments between source and target via attention mechanism [5,6,7,8,9].",3,16,INTRODUCTION,INTRODUCTION,"DNN-HMM hybrid models with pre-segmented alignments or offline models based on Transformer [3,4], which is the state-of-the-art in many sequence-sequence tasks nowadays.",INTRODUCTION,"Especially, Monotonic Attention (MA), and Monotonic Chunkwise Attention (MoChA) [8,9] learn alignments in an end-to-end manner by calculating differentiable expected alignments in training phase and shows comparable performance to models using an offline attention.",,
3256,"Especially, Monotonic Attention (MA), and Monotonic Chunkwise Attention (MoChA) [8,9] learn alignments in an end-to-end manner by calculating differentiable expected alignments in training phase and shows comparable performance to models using an offline attention.",3,17,INTRODUCTION,INTRODUCTION,"In order to overcome this performance-delay trade-off, several attempts have been made to learn or find monotonic alignments between source and target via attention mechanism [5,6,7,8,9].",INTRODUCTION,"Very recently, motivated by the success of Transformer architecture even in ASR [4], direct attempts to make it online by applying these learning alignment strategies to Transformer, not on the traditional RNN based models, are emerging [10,11,12,13].",,
3257,"Very recently, motivated by the success of Transformer architecture even in ASR [4], direct attempts to make it online by applying these learning alignment strategies to Transformer, not on the traditional RNN based models, are emerging [10,11,12,13].",3,18,INTRODUCTION,INTRODUCTION,"Especially, Monotonic Attention (MA), and Monotonic Chunkwise Attention (MoChA) [8,9] learn alignments in an end-to-end manner by calculating differentiable expected alignments in training phase and shows comparable performance to models using an offline attention.",INTRODUCTION,"Among others, Monotonic Multihead Attention (MMA) [14] converts each of multi-heads in Transformer to MA and exploits the diversity of alignments from multiple heads.",,
3258,"Among others, Monotonic Multihead Attention (MMA) [14] converts each of multi-heads in Transformer to MA and exploits the diversity of alignments from multiple heads.",3,19,INTRODUCTION,INTRODUCTION,"Very recently, motivated by the success of Transformer architecture even in ASR [4], direct attempts to make it online by applying these learning alignment strategies to Transformer, not on the traditional RNN based models, are emerging [10,11,12,13].",INTRODUCTION,"In order to resolve the issue of MMA that has to wait for all multi-heads to decode, HeadDrop [15] drops heads stochastically in the training stage.",,
3259,"In order to resolve the issue of MMA that has to wait for all multi-heads to decode, HeadDrop [15] drops heads stochastically in the training stage.",3,20,INTRODUCTION,INTRODUCTION,"Among others, Monotonic Multihead Attention (MMA) [14] converts each of multi-heads in Transformer to MA and exploits the diversity of alignments from multiple heads.",INTRODUCTION,"[15] also proposed to use head-synchronous beam search decoding (HSD) which limits the difference in selection time between the heads in the same layer only in the inference phase, but resulting in the discrepancy between training and inference.",,
3260,"[15] also proposed to use head-synchronous beam search decoding (HSD) which limits the difference in selection time between the heads in the same layer only in the inference phase, but resulting in the discrepancy between training and inference.",3,21,INTRODUCTION,INTRODUCTION,"In order to resolve the issue of MMA that has to wait for all multi-heads to decode, HeadDrop [15] drops heads stochastically in the training stage.",INTRODUCTION,"In this paper, we propose an algorithm, called ""Mutually-Constrained Monotonic Multihead Attention"" (MCMMA), that enables the model to learns alignments along with other heads by modifying expected alignments to consistently bring constrained alignments of the test time to the training time.",,
3261,"In this paper, we propose an algorithm, called ""Mutually-Constrained Monotonic Multihead Attention"" (MCMMA), that enables the model to learns alignments along with other heads by modifying expected alignments to consistently bring constrained alignments of the test time to the training time.",3,22,INTRODUCTION,INTRODUCTION,"[15] also proposed to use head-synchronous beam search decoding (HSD) which limits the difference in selection time between the heads in the same layer only in the inference phase, but resulting in the discrepancy between training and inference.",INTRODUCTION,"By bridging the gap between the training and the test stages, MCMMA effectively improves performance.",,
3262,"By bridging the gap between the training and the test stages, MCMMA effectively improves performance.",3,23,INTRODUCTION,INTRODUCTION,"In this paper, we propose an algorithm, called ""Mutually-Constrained Monotonic Multihead Attention"" (MCMMA), that enables the model to learns alignments along with other heads by modifying expected alignments to consistently bring constrained alignments of the test time to the training time.",PRELIMINARY,"We first review the main components which our model is based on, including monotonic attention, monotonic multihead attention, and HeadDrop with head-synchronous beam search decoding in Subsection 2.1, 2.2, and 2.3, respectively.",,
3263,"We first review the main components which our model is based on, including monotonic attention, monotonic multihead attention, and HeadDrop with head-synchronous beam search decoding in Subsection 2.1, 2.2, and 2.3, respectively.",3,24,PRELIMINARY,INTRODUCTION,"By bridging the gap between the training and the test stages, MCMMA effectively improves performance.",Monotonic Attention,MA [8] is the attention-based encoder-decoder RNN model which is able to learn monotonic alignments in an end-toend manner.,,
3264,MA [8] is the attention-based encoder-decoder RNN model which is able to learn monotonic alignments in an end-toend manner.,3,25,Monotonic Attention,PRELIMINARY,"We first review the main components which our model is based on, including monotonic attention, monotonic multihead attention, and HeadDrop with head-synchronous beam search decoding in Subsection 2.1, 2.2, and 2.3, respectively.",Monotonic Attention,"The encoder processes input sequence x = (x 1 , .",,
3265,"The encoder processes input sequence x = (x 1 , .",3,26,Monotonic Attention,Monotonic Attention,MA [8] is the attention-based encoder-decoder RNN model which is able to learn monotonic alignments in an end-toend manner.,Monotonic Attention,", x T ) to encoder states h = (h 1 , .",,
3266,", x T ) to encoder states h = (h 1 , .",3,27,Monotonic Attention,Monotonic Attention,"The encoder processes input sequence x = (x 1 , .",Monotonic Attention,"At i-th arXiv:2103.14302v1 [cs.CL] 26 Mar 2021 output step, the decoder sequentially inspects encoder states from the last selected one in the previous step and decide whether to take it or not to produce current output.",,
3267,"At i-th arXiv:2103.14302v1 [cs.CL] 26 Mar 2021 output step, the decoder sequentially inspects encoder states from the last selected one in the previous step and decide whether to take it or not to produce current output.",3,28,Monotonic Attention,Monotonic Attention,", x T ) to encoder states h = (h 1 , .",Monotonic Attention,"The probability p i,j to select h j for i-th output is computed as e i,j = MonotonicEnergy (s i-1 , h j ) and p i,j = σ (e i,j ) where s i-1 is a decoder state of (i -1)-th output step.",,
3268,"The probability p i,j to select h j for i-th output is computed as e i,j = MonotonicEnergy (s i-1 , h j ) and p i,j = σ (e i,j ) where s i-1 is a decoder state of (i -1)-th output step.",3,29,Monotonic Attention,Monotonic Attention,"At i-th arXiv:2103.14302v1 [cs.CL] 26 Mar 2021 output step, the decoder sequentially inspects encoder states from the last selected one in the previous step and decide whether to take it or not to produce current output.",Monotonic Attention,"If h j is selected, the RNN decoder takes it as context c i = h j with the previous decoder state s i-1 and output y i-1 to compute current state.",,
3269,"If h j is selected, the RNN decoder takes it as context c i = h j with the previous decoder state s i-1 and output y i-1 to compute current state.",3,30,Monotonic Attention,Monotonic Attention,"The probability p i,j to select h j for i-th output is computed as e i,j = MonotonicEnergy (s i-1 , h j ) and p i,j = σ (e i,j ) where s i-1 is a decoder state of (i -1)-th output step.",Monotonic Attention,"To make alignment learnable in the training phase, a hard selected context above is replaced by a expected context c i = L j=1 α i,j h j , the weighted sum of h with the expected alignment α computed as α i,j = p i,j j k=1 α i-1,k j-1 l=k (1 -p i,l ) .",,
3270,"To make alignment learnable in the training phase, a hard selected context above is replaced by a expected context c i = L j=1 α i,j h j , the weighted sum of h with the expected alignment α computed as α i,j = p i,j j k=1 α i-1,k j-1 l=k (1 -p i,l ) .",3,31,Monotonic Attention,Monotonic Attention,"If h j is selected, the RNN decoder takes it as context c i = h j with the previous decoder state s i-1 and output y i-1 to compute current state.",Monotonic Attention,(1) MoChA [9] extends MA by performing soft attention over fixed-length chunks of encoder states preceding the position chosen by a MA mechanism.,,
3271,(1) MoChA [9] extends MA by performing soft attention over fixed-length chunks of encoder states preceding the position chosen by a MA mechanism.,3,32,Monotonic Attention,Monotonic Attention,"To make alignment learnable in the training phase, a hard selected context above is replaced by a expected context c i = L j=1 α i,j h j , the weighted sum of h with the expected alignment α computed as α i,j = p i,j j k=1 α i-1,k j-1 l=k (1 -p i,l ) .",Monotonic Multihead Attention,MMA [14] applies MA mechanism to Transformer [3] by making each of the multiple heads of decoder-encoder attention learn monotonic alignments as MA.,,
3272,MMA [14] applies MA mechanism to Transformer [3] by making each of the multiple heads of decoder-encoder attention learn monotonic alignments as MA.,3,33,Monotonic Multihead Attention,Monotonic Attention,(1) MoChA [9] extends MA by performing soft attention over fixed-length chunks of encoder states preceding the position chosen by a MA mechanism.,Monotonic Multihead Attention,MMA borrows scaled dot-product operation of Transformer.,,
3273,MMA borrows scaled dot-product operation of Transformer.,3,34,Monotonic Multihead Attention,Monotonic Multihead Attention,MMA [14] applies MA mechanism to Transformer [3] by making each of the multiple heads of decoder-encoder attention learn monotonic alignments as MA.,Monotonic Multihead Attention,"Although MMA leads to considerable improvement in online machine translation, the latency is still high since the model should wait until all heads to select their contexts for every decoding step.",,
3274,"Although MMA leads to considerable improvement in online machine translation, the latency is still high since the model should wait until all heads to select their contexts for every decoding step.",3,35,Monotonic Multihead Attention,Monotonic Multihead Attention,MMA borrows scaled dot-product operation of Transformer.,Monotonic Multihead Attention,"Thus, the authors of [14] proposed to use additional regularization to minimize the variance of expected alignments of all heads to reduce the latency.",,
3275,"Thus, the authors of [14] proposed to use additional regularization to minimize the variance of expected alignments of all heads to reduce the latency.",3,36,Monotonic Multihead Attention,Monotonic Multihead Attention,"Although MMA leads to considerable improvement in online machine translation, the latency is still high since the model should wait until all heads to select their contexts for every decoding step.",Monotonic Multihead Attention,"Nevertheless, this approach does not model the dependency between heads explicitly.",,
3276,"Nevertheless, this approach does not model the dependency between heads explicitly.",3,37,Monotonic Multihead Attention,Monotonic Multihead Attention,"Thus, the authors of [14] proposed to use additional regularization to minimize the variance of expected alignments of all heads to reduce the latency.",HeadDrop and Head-Synchronous Decoding,HeadDrop [15] is the method that drops each head stochastically for each individual to learn alignments correctly.,,
3277,HeadDrop [15] is the method that drops each head stochastically for each individual to learn alignments correctly.,3,38,HeadDrop and Head-Synchronous Decoding,Monotonic Multihead Attention,"Nevertheless, this approach does not model the dependency between heads explicitly.",HeadDrop and Head-Synchronous Decoding,This approach improves boundary coverage and streamability of MMA [15].,,
3278,This approach improves boundary coverage and streamability of MMA [15].,3,39,HeadDrop and Head-Synchronous Decoding,HeadDrop and Head-Synchronous Decoding,HeadDrop [15] is the method that drops each head stochastically for each individual to learn alignments correctly.,HeadDrop and Head-Synchronous Decoding,"Head-synchronous decoding (HSD) [15] is the inference algorithm, where the leftmost head forces slow heads, which fail to choose any frames within waiting time threshold , to choose the rightmost of selected frames.",,
3279,"Head-synchronous decoding (HSD) [15] is the inference algorithm, where the leftmost head forces slow heads, which fail to choose any frames within waiting time threshold , to choose the rightmost of selected frames.",3,40,HeadDrop and Head-Synchronous Decoding,HeadDrop and Head-Synchronous Decoding,This approach improves boundary coverage and streamability of MMA [15].,HeadDrop and Head-Synchronous Decoding,"However, HSD considers alignments of other heads and only at the test phase.",,
3280,"However, HSD considers alignments of other heads and only at the test phase.",3,41,HeadDrop and Head-Synchronous Decoding,HeadDrop and Head-Synchronous Decoding,"Head-synchronous decoding (HSD) [15] is the inference algorithm, where the leftmost head forces slow heads, which fail to choose any frames within waiting time threshold , to choose the rightmost of selected frames.",METHOD,"In this section, we propose the algorithm to learn alignments of MA heads under a constraint of the difference of heads' alignments within each decoder layer.",,
3281,Boundary coverage and streamability [15] is the metric to evaluate whether the model is streamable.,3,82,Relative Latency,Experiment setup,"We utilize 2 CNN blocks for encoder and apply max-pooling with 2-stride after the second CNN block, the fourth, and the eighth layer in AISHELL-1.",Relative Latency,"However, it does not well suit with the MMA mechanism since predicting each output is done when the last head completes the selection.",,
3282,"However, it does not well suit with the MMA mechanism since predicting each output is done when the last head completes the selection.",3,83,Relative Latency,Relative Latency,Boundary coverage and streamability [15] is the metric to evaluate whether the model is streamable.,Relative Latency,"Instead of the above, we utilize the relative latency (ReL) by  We note that ReL is the natural extension of the existing latency metric.",,
3283,"Instead of the above, we utilize the relative latency (ReL) by  We note that ReL is the natural extension of the existing latency metric.",3,84,Relative Latency,Relative Latency,"However, it does not well suit with the MMA mechanism since predicting each output is done when the last head completes the selection.",Relative Latency,[23] provides the utterance-level latency which is the same with ReL when replacing the boundaries produced by the reference model with the gold boundaries in the definition of relative latency.,,
3284,[23] provides the utterance-level latency which is the same with ReL when replacing the boundaries produced by the reference model with the gold boundaries in the definition of relative latency.,3,85,Relative Latency,Relative Latency,"Instead of the above, we utilize the relative latency (ReL) by  We note that ReL is the natural extension of the existing latency metric.",Relative Latency,"However, acquiring the gold boundaries is complicated, so we utilize the boundaries of MMA without HSD as the reference boundaries.",,
3285,"However, acquiring the gold boundaries is complicated, so we utilize the boundaries of MMA without HSD as the reference boundaries.",3,86,Relative Latency,Relative Latency,[23] provides the utterance-level latency which is the same with ReL when replacing the boundaries produced by the reference model with the gold boundaries in the definition of relative latency.,Online ASR Results,We present the results of our approach with baselines in table 1.,,
3286,We present the results of our approach with baselines in table 1.,3,87,Online ASR Results,Relative Latency,"However, acquiring the gold boundaries is complicated, so we utilize the boundaries of MMA without HSD as the reference boundaries.",Online ASR Results,"We train our model with = 10 and = 12 on Librispeech and AISHELL-1, respectively and evaluate it with = 8 to make the setting same with [15].",,
3287,"We train our model with = 10 and = 12 on Librispeech and AISHELL-1, respectively and evaluate it with = 8 to make the setting same with [15].",3,88,Online ASR Results,Online ASR Results,We present the results of our approach with baselines in table 1.,Online ASR Results,Our model shows better performance than the baselines including HeadDrop [15].,,
3288,Our model shows better performance than the baselines including HeadDrop [15].,3,89,Online ASR Results,Online ASR Results,"We train our model with = 10 and = 12 on Librispeech and AISHELL-1, respectively and evaluate it with = 8 to make the setting same with [15].",Online ASR Results,"Especially, we reduce 2.2% of WER than HeadDrop [15] on testother in Librispeech.",,
3289,"Especially, we reduce 2.2% of WER than HeadDrop [15] on testother in Librispeech.",3,90,Online ASR Results,Online ASR Results,Our model shows better performance than the baselines including HeadDrop [15].,Online ASR Results,These results show that training alignments together with other heads' selection time improves the performance.,,
3290,These results show that training alignments together with other heads' selection time improves the performance.,3,91,Online ASR Results,Online ASR Results,"Especially, we reduce 2.2% of WER than HeadDrop [15] on testother in Librispeech.",Online ASR Results,One very interesting and unexpected point we observed in table 1 is that the WER of Transformer is higher than online models (except for MMA) in test-clean experiments.,,
3291,One very interesting and unexpected point we observed in table 1 is that the WER of Transformer is higher than online models (except for MMA) in test-clean experiments.,3,92,Online ASR Results,Online ASR Results,These results show that training alignments together with other heads' selection time improves the performance.,Online ASR Results,We con- jecture that online attention mechanisms are beneficial to exploit locality since they strongly force models to attend small chunks of an input sequence from the training phase.,,
3292,We con- jecture that online attention mechanisms are beneficial to exploit locality since they strongly force models to attend small chunks of an input sequence from the training phase.,3,93,Online ASR Results,Online ASR Results,One very interesting and unexpected point we observed in table 1 is that the WER of Transformer is higher than online models (except for MMA) in test-clean experiments.,Trade-off between Performance and Latency,"We provide trade-off graphs between quality and relative latency in fig 2 through adjusting ∈ {6, 8, 10, 12}, and ∈ {4, 8, 12} in inference time for Librispeech, and AISHELL-1, respectively.",,
3293,"We provide trade-off graphs between quality and relative latency in fig 2 through adjusting ∈ {6, 8, 10, 12}, and ∈ {4, 8, 12} in inference time for Librispeech, and AISHELL-1, respectively.",3,94,Trade-off between Performance and Latency,Online ASR Results,We con- jecture that online attention mechanisms are beneficial to exploit locality since they strongly force models to attend small chunks of an input sequence from the training phase.,Trade-off between Performance and Latency,"To calculate relative latency with time units, we multiply frame-level relative latency by 80ms since the reducing factor of frames is 8 and the shifting size is 10ms.",,
3294,"To calculate relative latency with time units, we multiply frame-level relative latency by 80ms since the reducing factor of frames is 8 and the shifting size is 10ms.",3,95,Trade-off between Performance and Latency,Trade-off between Performance and Latency,"We provide trade-off graphs between quality and relative latency in fig 2 through adjusting ∈ {6, 8, 10, 12}, and ∈ {4, 8, 12} in inference time for Librispeech, and AISHELL-1, respectively.",Trade-off between Performance and Latency,Our model outperforms baselines and is still faster than MMA without HSD even though there are small increases in relative latency compared to HeadDrop except for the case with extremely small text .,,
3295,Our model outperforms baselines and is still faster than MMA without HSD even though there are small increases in relative latency compared to HeadDrop except for the case with extremely small text .,3,96,Trade-off between Performance and Latency,Trade-off between Performance and Latency,"To calculate relative latency with time units, we multiply frame-level relative latency by 80ms since the reducing factor of frames is 8 and the shifting size is 10ms.",Trade-off between Performance and Latency,The performance degradation with small occurs since accessible input information is very limited and training models with small restricts head diversity severely.,,
3296,The performance degradation with small occurs since accessible input information is very limited and training models with small restricts head diversity severely.,3,97,Trade-off between Performance and Latency,Trade-off between Performance and Latency,Our model outperforms baselines and is still faster than MMA without HSD even though there are small increases in relative latency compared to HeadDrop except for the case with extremely small text .,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small .",,
3297,"Thus, this result suggests that the practitioners should avoid choosing small .",3,98,Trade-off between Performance and Latency,Trade-off between Performance and Latency,The performance degradation with small occurs since accessible input information is very limited and training models with small restricts head diversity severely.,CONCLUSION,We suggest the method to learn alignments with considering other heads' alignments by modifying expected alignments for all the heads of each layer to select an input frame within a fixed size window.,,
3298,We suggest the method to learn alignments with considering other heads' alignments by modifying expected alignments for all the heads of each layer to select an input frame within a fixed size window.,3,99,CONCLUSION,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small .",CONCLUSION,Our approach improves performance with only a small increase in latency by regularizing the intra-layer difference of boundaries effectively from the training phase.,,
3299,Our approach improves performance with only a small increase in latency by regularizing the intra-layer difference of boundaries effectively from the training phase.,3,100,CONCLUSION,CONCLUSION,We suggest the method to learn alignments with considering other heads' alignments by modifying expected alignments for all the heads of each layer to select an input frame within a fixed size window.,,,,
3300,  Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing.,6,0,abstract,,,abstract,Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance.,,
3301,Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance.,6,1,abstract,abstract,  Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing.,abstract,Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations.,,
3302,Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations.,6,2,abstract,abstract,Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance.,abstract,"We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling.",,
3303,"We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling.",6,3,abstract,abstract,Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations.,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.,,
3304,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.,6,4,abstract,abstract,"We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling.",abstract,These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.,,
3305,These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.,6,5,abstract,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.,Introduction,The need for effective regularization methods for RNNs has seen extensive focus in recent years.,,
3306,The need for effective regularization methods for RNNs has seen extensive focus in recent years.,6,6,Introduction,abstract,These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.,Introduction,"While application of dropout (Srivastava et al., 2014) to the input and output of an RNN has been shown to be effective (Zaremba et al., 2014), dropout is destructive when naively applied to the recurrent connections of an RNN.",,
3307,"While application of dropout (Srivastava et al., 2014) to the input and output of an RNN has been shown to be effective (Zaremba et al., 2014), dropout is destructive when naively applied to the recurrent connections of an RNN.",6,7,Introduction,Introduction,The need for effective regularization methods for RNNs has seen extensive focus in recent years.,Introduction,"When naive dropout is applied to the recurrent connections, it is almost impossible to retain information over long periods of time.",,
3308,"When naive dropout is applied to the recurrent connections, it is almost impossible to retain information over long periods of time.",6,8,Introduction,Introduction,"While application of dropout (Srivastava et al., 2014) to the input and output of an RNN has been shown to be effective (Zaremba et al., 2014), dropout is destructive when naively applied to the recurrent connections of an RNN.",Introduction,"Given this fundamental issue, substantial work has gone into understanding and improving dropout when applied to recurrent connections.",,
3309,"Given this fundamental issue, substantial work has gone into understanding and improving dropout when applied to recurrent connections.",6,9,Introduction,Introduction,"When naive dropout is applied to the recurrent connections, it is almost impossible to retain information over long periods of time.",Introduction,"Of these techniques, which we shall broadly refer to as recurrent dropout, some specific variations have gained popular usage.",,
3310,"Of these techniques, which we shall broadly refer to as recurrent dropout, some specific variations have gained popular usage.",6,10,Introduction,Introduction,"Given this fundamental issue, substantial work has gone into understanding and improving dropout when applied to recurrent connections.",Introduction,"Part of 34 th International Conference on Machine Learning's Workshop on Learning to Generate Natural Language, Sydney, Australia, 2017.",,
3311,"Part of 34 th International Conference on Machine Learning's Workshop on Learning to Generate Natural Language, Sydney, Australia, 2017.",6,11,Introduction,Introduction,"Of these techniques, which we shall broadly refer to as recurrent dropout, some specific variations have gained popular usage.",Introduction,Copyright 2017 by the author(s).,,
3312,Copyright 2017 by the author(s).,6,12,Introduction,Introduction,"Part of 34 th International Conference on Machine Learning's Workshop on Learning to Generate Natural Language, Sydney, Australia, 2017.",Introduction,"Variational RNNs (Gal & Ghahramani, 2016) drop the same network units at each timestep, as opposed to dropping different network units at each timestep.",,
3313,"Variational RNNs (Gal & Ghahramani, 2016) drop the same network units at each timestep, as opposed to dropping different network units at each timestep.",6,13,Introduction,Introduction,Copyright 2017 by the author(s).,Introduction,"By performing dropout on the same units at each timestep, destructive loss of the RNN hidden state is avoided and the same information is masked at each timestep.",,
3314,"By performing dropout on the same units at each timestep, destructive loss of the RNN hidden state is avoided and the same information is masked at each timestep.",6,14,Introduction,Introduction,"Variational RNNs (Gal & Ghahramani, 2016) drop the same network units at each timestep, as opposed to dropping different network units at each timestep.",Introduction,"Rather than dropping units, another tactic is to drop updates to given network units.",,
3315,"Rather than dropping units, another tactic is to drop updates to given network units.",6,15,Introduction,Introduction,"By performing dropout on the same units at each timestep, destructive loss of the RNN hidden state is avoided and the same information is masked at each timestep.",Introduction,Semeniuta et al.,,
3316,Semeniuta et al.,6,16,Introduction,Introduction,"Rather than dropping units, another tactic is to drop updates to given network units.",Introduction,"(2016) perform dropout on the input gate of the LSTM (Hochreiter & Schmidhuber, 1997) but allow the forget gate to discard portions of the existing hidden state.",,
3317,"(2016) perform dropout on the input gate of the LSTM (Hochreiter & Schmidhuber, 1997) but allow the forget gate to discard portions of the existing hidden state.",6,17,Introduction,Introduction,Semeniuta et al.,Introduction,"Zoneout (Krueger et al., 2016) prevents hidden state updates from occurring by setting a randomly selected subset of network unit activations in h t+1 to be equal to the previous activations from h t .",,
3318,"Zoneout (Krueger et al., 2016) prevents hidden state updates from occurring by setting a randomly selected subset of network unit activations in h t+1 to be equal to the previous activations from h t .",6,18,Introduction,Introduction,"(2016) perform dropout on the input gate of the LSTM (Hochreiter & Schmidhuber, 1997) but allow the forget gate to discard portions of the existing hidden state.",Introduction,Both of these act to prevent updates to the hidden state while preserving existing content.,,
3319,Both of these act to prevent updates to the hidden state while preserving existing content.,6,19,Introduction,Introduction,"Zoneout (Krueger et al., 2016) prevents hidden state updates from occurring by setting a randomly selected subset of network unit activations in h t+1 to be equal to the previous activations from h t .",Introduction,"On an extreme end, work has also been done to restrict the recurrent matrices in an RNN in order to limit their computational capacity.",,
3320,"On an extreme end, work has also been done to restrict the recurrent matrices in an RNN in order to limit their computational capacity.",6,20,Introduction,Introduction,Both of these act to prevent updates to the hidden state while preserving existing content.,Introduction,"Some RNN architectures only allow element-wise interactions (Balduzzi & Ghifary, 2016;Bradbury et al., 2016;Seo et al., 2016), removing the recurrent matrix entirely, while others act to restrict the capacity by parameterizing the recurrent matrix (Arjovsky et al., 2016;Wisdom et al., 2016;Jing et al., 2016).",,
3321,"Some RNN architectures only allow element-wise interactions (Balduzzi & Ghifary, 2016;Bradbury et al., 2016;Seo et al., 2016), removing the recurrent matrix entirely, while others act to restrict the capacity by parameterizing the recurrent matrix (Arjovsky et al., 2016;Wisdom et al., 2016;Jing et al., 2016).",6,21,Introduction,Introduction,"On an extreme end, work has also been done to restrict the recurrent matrices in an RNN in order to limit their computational capacity.",Introduction,"Other forms of regularization explicitly act upon activations such as such as batch normalization (Ioffe & Szegedy, 2015), recurrent batch normalization (Cooijmans et al., 2016), and layer normalization (Ba et al., 2016).",,
3322,"Other forms of regularization explicitly act upon activations such as such as batch normalization (Ioffe & Szegedy, 2015), recurrent batch normalization (Cooijmans et al., 2016), and layer normalization (Ba et al., 2016).",6,22,Introduction,Introduction,"Some RNN architectures only allow element-wise interactions (Balduzzi & Ghifary, 2016;Bradbury et al., 2016;Seo et al., 2016), removing the recurrent matrix entirely, while others act to restrict the capacity by parameterizing the recurrent matrix (Arjovsky et al., 2016;Wisdom et al., 2016;Jing et al., 2016).",Introduction,These all introduce additional training parameters and can complicate the training process while increasing the sensitivity of the model.,,
3323,These all introduce additional training parameters and can complicate the training process while increasing the sensitivity of the model.,6,23,Introduction,Introduction,"Other forms of regularization explicitly act upon activations such as such as batch normalization (Ioffe & Szegedy, 2015), recurrent batch normalization (Cooijmans et al., 2016), and layer normalization (Ba et al., 2016).",Introduction,"Norm stabilization (Krueger & Memisevic, 2015) penalizes the model when the norm of an RNN's hidden state changes substantially between timesteps, achieving strong results in character language modeling on and phoneme recognition.",,
3324,"Norm stabilization (Krueger & Memisevic, 2015) penalizes the model when the norm of an RNN's hidden state changes substantially between timesteps, achieving strong results in character language modeling on and phoneme recognition.",6,24,Introduction,Introduction,These all introduce additional training parameters and can complicate the training process while increasing the sensitivity of the model.,Introduction,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",,
3325,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",6,25,Introduction,Introduction,"Norm stabilization (Krueger & Memisevic, 2015) penalizes the model when the norm of an RNN's hidden state changes substantially between timesteps, achieving strong results in character language modeling on and phoneme recognition.",Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",,
3326,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",6,26,Introduction,Introduction,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",,
3327,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",6,27,Compared to other invasive regularization techniques,Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",Compared to other invasive regularization techniques,This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.,,
3328,This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.,6,28,Compared to other invasive regularization techniques,Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",Activation Regularization,"L 2 activation regularization (AR) While L 2 regularization is traditionally used on the weights of machine learning models (L 2 weight decay), it could also be used on the activations.",,
3329,"L 2 activation regularization (AR) While L 2 regularization is traditionally used on the weights of machine learning models (L 2 weight decay), it could also be used on the activations.",6,29,Activation Regularization,Compared to other invasive regularization techniques,This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.,Activation Regularization,"We define AR as α L 2 (m ⊙ h t ) where m is the dropout mask used by later parts of the model, L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and α is a scaling coefficient.",,
3330,"We define AR as α L 2 (m ⊙ h t ) where m is the dropout mask used by later parts of the model, L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and α is a scaling coefficient.",6,30,Activation Regularization,Activation Regularization,"L 2 activation regularization (AR) While L 2 regularization is traditionally used on the weights of machine learning models (L 2 weight decay), it could also be used on the activations.",Activation Regularization,"When applied to the output of a dense layer, AR penalizes activations that are substantially away from 0, encouraging the activations to remain small.",,
3331,"When applied to the output of a dense layer, AR penalizes activations that are substantially away from 0, encouraging the activations to remain small.",6,31,Activation Regularization,Activation Regularization,"We define AR as α L 2 (m ⊙ h t ) where m is the dropout mask used by later parts of the model, L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and α is a scaling coefficient.",Activation Regularization,"While acting implicitly rather than explicitly, this has similarities to the various batch or layer normalization techniques.",,
3332,"While acting implicitly rather than explicitly, this has similarities to the various batch or layer normalization techniques.",6,32,Activation Regularization,Activation Regularization,"When applied to the output of a dense layer, AR penalizes activations that are substantially away from 0, encouraging the activations to remain small.",Activation Regularization,The L 2 penalty on the RNN activations can be applied to h t or to m ⊙ h t (the dropped output used in the rest of the model).,,
3333,The L 2 penalty on the RNN activations can be applied to h t or to m ⊙ h t (the dropped output used in the rest of the model).,6,33,Activation Regularization,Activation Regularization,"While acting implicitly rather than explicitly, this has similarities to the various batch or layer normalization techniques.",Activation Regularization,"In our experiments, we found that applying AR to m ⊙ h t was more effective than applying it to neurons not updated during the current optimization step.",,
3334,"In our experiments, we found that applying AR to m ⊙ h t was more effective than applying it to neurons not updated during the current optimization step.",6,34,Activation Regularization,Activation Regularization,The L 2 penalty on the RNN activations can be applied to h t or to m ⊙ h t (the dropped output used in the rest of the model).,Temporal activation regularization (TAR),Adding a prior that minimizes differences between states has been explored in the past.,,
3335,Adding a prior that minimizes differences between states has been explored in the past.,6,35,Temporal activation regularization (TAR),Activation Regularization,"In our experiments, we found that applying AR to m ⊙ h t was more effective than applying it to neurons not updated during the current optimization step.",Temporal activation regularization (TAR),"This broad concept falls under the broad concept of slowness regularization (Hinton, 1989;Földiák, 1991;Luciw & Schmidhuber, 2012;Jonschkowski & Brock, 2015;Wen et al., 2015) which attempts to minimize L(f (x t ), f (x t+1 )) where L is a loss function describing the distance between f (x t ) and f (x t+1 ) and f is an arbitrary mapping function.",,
3336,"This broad concept falls under the broad concept of slowness regularization (Hinton, 1989;Földiák, 1991;Luciw & Schmidhuber, 2012;Jonschkowski & Brock, 2015;Wen et al., 2015) which attempts to minimize L(f (x t ), f (x t+1 )) where L is a loss function describing the distance between f (x t ) and f (x t+1 ) and f is an arbitrary mapping function.",6,36,Temporal activation regularization (TAR),Temporal activation regularization (TAR),Adding a prior that minimizes differences between states has been explored in the past.,Temporal activation regularization (TAR),"Temporal activation regularization (TAR) is a direct descendant of this slowness regularization, minimizing β L 2 (h t -h t+1 ) where L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and β is a scaling coefficient.",,
3337,"Temporal activation regularization (TAR) is a direct descendant of this slowness regularization, minimizing β L 2 (h t -h t+1 ) where L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and β is a scaling coefficient.",6,37,Temporal activation regularization (TAR),Temporal activation regularization (TAR),"This broad concept falls under the broad concept of slowness regularization (Hinton, 1989;Földiák, 1991;Luciw & Schmidhuber, 2012;Jonschkowski & Brock, 2015;Wen et al., 2015) which attempts to minimize L(f (x t ), f (x t+1 )) where L is a loss function describing the distance between f (x t ) and f (x t+1 ) and f is an arbitrary mapping function.",Temporal activation regularization (TAR),"TAR penalizes any large changes in hidden state between timesteps, encouraging the model to keep the output as consistent as possible.",,
3338,"TAR penalizes any large changes in hidden state between timesteps, encouraging the model to keep the output as consistent as possible.",6,38,Temporal activation regularization (TAR),Temporal activation regularization (TAR),"Temporal activation regularization (TAR) is a direct descendant of this slowness regularization, minimizing β L 2 (h t -h t+1 ) where L 2 (•) = • 2 (L 2 norm), h t is the output of the RNN at timestep t, and β is a scaling coefficient.",Temporal activation regularization (TAR),"For the LSTM, the hidden state which is regularized is only h t , not the long term memory c t , though this could optionally be regularized in a similar manner.",,
3339,"For the LSTM, the hidden state which is regularized is only h t , not the long term memory c t , though this could optionally be regularized in a similar manner.",6,39,Temporal activation regularization (TAR),Temporal activation regularization (TAR),"TAR penalizes any large changes in hidden state between timesteps, encouraging the model to keep the output as consistent as possible.",Model Parameters Validation,"Results over the Penn Treebank for testing α coefficients for AR with base model h = 650, β = 0, dp = 0.5, dp h = 0.5.",,
3340,"To understand the potential of AR and TAR, we investigate their impact on language model perplexity when used independently in Table 1 (AR) and Table 2 (TAR).",6,56,Evaluating AR and TAR independently on PTB:,Model,"All models use weight tying between the embedding and softmax layer (Inan et al., 2016;Press & Wolf, 2016).",Evaluating AR and TAR independently on PTB:,"While both result in a substantial reduction in perplexity, AR results in the strongest improvement of 5.3, while TAR only achieves 4.3.",,
3341,"While both result in a substantial reduction in perplexity, AR results in the strongest improvement of 5.3, while TAR only achieves 4.3.",6,57,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"To understand the potential of AR and TAR, we investigate their impact on language model perplexity when used independently in Table 1 (AR) and Table 2 (TAR).",Evaluating AR and TAR independently on PTB:,The drops achieved by this are equivalent to using an LSTM model with twice as many parameters -a substantial improvement given the simplicity of AR and TAR.,,
3342,The drops achieved by this are equivalent to using an LSTM model with twice as many parameters -a substantial improvement given the simplicity of AR and TAR.,6,58,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"While both result in a substantial reduction in perplexity, AR results in the strongest improvement of 5.3, while TAR only achieves 4.3.",Evaluating AR and TAR independently on PTB:,"Evaluating AR and TAR jointly on PTB: When both AR and TAR are used together, we found the best result was achieved by decreasing α and β, likely as the model was over-regularized otherwise.",,
3343,"Evaluating AR and TAR jointly on PTB: When both AR and TAR are used together, we found the best result was achieved by decreasing α and β, likely as the model was over-regularized otherwise.",6,59,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,The drops achieved by this are equivalent to using an LSTM model with twice as many parameters -a substantial improvement given the simplicity of AR and TAR.,Evaluating AR and TAR independently on PTB:,In Table 3 we present PTB results for three different model sizes comparing models without AR/TAR to those which use both.,,
3344,In Table 3 we present PTB results for three different model sizes comparing models without AR/TAR to those which use both.,6,60,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"Evaluating AR and TAR jointly on PTB: When both AR and TAR are used together, we found the best result was achieved by decreasing α and β, likely as the model was over-regularized otherwise.",Evaluating AR and TAR independently on PTB:,"The model sizes h ∈ [650, 950, 1500] were chosen to be comparable in size to other published results.",,
3345,"The model sizes h ∈ [650, 950, 1500] were chosen to be comparable in size to other published results.",6,61,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,In Table 3 we present PTB results for three different model sizes comparing models without AR/TAR to those which use both.,Evaluating AR and TAR independently on PTB:,"With both AR and TAR, the smallest model has an improvement of 6.2 over the baseline model.",,
3346,"With both AR and TAR, the smallest model has an improvement of 6.2 over the baseline model.",6,62,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"The model sizes h ∈ [650, 950, 1500] were chosen to be comparable in size to other published results.",Evaluating AR and TAR independently on PTB:,"The improvements continue for the two larger size models, h = 950 and h = 1500, though the gains fall off as the model size is increased.",,
3347,"The improvements continue for the two larger size models, h = 950 and h = 1500, though the gains fall off as the model size is increased.",6,63,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"With both AR and TAR, the smallest model has an improvement of 6.2 over the baseline model.",Evaluating AR and TAR independently on PTB:,Comparing to state-of-the-art PTB: In Table 5 we summarize the current state of the art models in language modeling over the Penn Treebank.,,
3348,Comparing to state-of-the-art PTB: In Table 5 we summarize the current state of the art models in language modeling over the Penn Treebank.,6,64,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"The improvements continue for the two larger size models, h = 950 and h = 1500, though the gains fall off as the model size is increased.",Evaluating AR and TAR independently on PTB:,"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",,
3349,"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",6,65,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,Comparing to state-of-the-art PTB: In Table 5 we summarize the current state of the art models in language modeling over the Penn Treebank.,Evaluating AR and TAR independently on PTB:,"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation.",,
3350,"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation.",6,66,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",Evaluating AR and TAR independently on PTB:,"This would likely result in the RHN being slower than the larger LSTM model during both training and prediction, especially when factoring in optimized LSTM implementations such as NVIDIA's cuDNN LSTM.",,
3351,"This would likely result in the RHN being slower than the larger LSTM model during both training and prediction, especially when factoring in optimized LSTM implementations such as NVIDIA's cuDNN LSTM.",6,67,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation.",Evaluating AR and TAR independently on PTB:,"We also compare to the Neural Architecture Search (NAS) cell (Zoph & Le, 2016).",,
3352,"We also compare to the Neural Architecture Search (NAS) cell (Zoph & Le, 2016).",6,68,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"This would likely result in the RHN being slower than the larger LSTM model during both training and prediction, especially when factoring in optimized LSTM implementations such as NVIDIA's cuDNN LSTM.",Evaluating AR and TAR independently on PTB:,"While Zoph & Le (2016) do not report any of the hyperparameters or what type of dropout they used for their Penn Treebank result, they do note that they performed an extensive hyperparameter search over learning rate, weight initialization, dropout rates, and decay epoch in order to produce their best performing model.",,
3353,"While Zoph & Le (2016) do not report any of the hyperparameters or what type of dropout they used for their Penn Treebank result, they do note that they performed an extensive hyperparameter search over learning rate, weight initialization, dropout rates, and decay epoch in order to produce their best performing model.",6,69,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"We also compare to the Neural Architecture Search (NAS) cell (Zoph & Le, 2016).",Evaluating AR and TAR independently on PTB:,It is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not compare their NAS cell results to a standard or variational LSTM cell that was subjected to the same extensive hyperparameter search.,,
3354,It is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not compare their NAS cell results to a standard or variational LSTM cell that was subjected to the same extensive hyperparameter search.,6,70,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,"While Zoph & Le (2016) do not report any of the hyperparameters or what type of dropout they used for their Penn Treebank result, they do note that they performed an extensive hyperparameter search over learning rate, weight initialization, dropout rates, and decay epoch in order to produce their best performing model.",Evaluating AR and TAR independently on PTB:,"Our largest LSTM results are 3 perplexity higher in comparison but have not undergone extensive hyperparameter search, do not use additional regularization techniques such as recurrent or embedding dropout, and do not use a custom RNN cell.",,
3355,"Our largest LSTM results are 3 perplexity higher in comparison but have not undergone extensive hyperparameter search, do not use additional regularization techniques such as recurrent or embedding dropout, and do not use a custom RNN cell.",6,71,Evaluating AR and TAR independently on PTB:,Evaluating AR and TAR independently on PTB:,It is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not compare their NAS cell results to a standard or variational LSTM cell that was subjected to the same extensive hyperparameter search.,Model,WikiText-2 Results: We compare our WikiText-2 results to Inan et al.,,
3356,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",6,88,Conclusion,Model,"This would be important given the weights in this model were randomly initialized and suggests TAR acts as an implicit identity initialization constraint (Le et al., 2015).",Conclusion,"While simple to implement, activity regularization and temporal activity regularization are com-",,
3357,"While simple to implement, activity regularization and temporal activity regularization are com-",6,89,Conclusion,Conclusion,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Sample generated text,"For generating text samples, words were sampled using the standard generation script contained in the PyTorch word level language modeling example.",,
3358,"For generating text samples, words were sampled using the standard generation script contained in the PyTorch word level language modeling example.",6,90,Sample generated text,Conclusion,"While simple to implement, activity regularization and temporal activity regularization are com-",Sample generated text,WikiText-2 was used given the larger vocabulary and more realistic looking text.,,
3359,WikiText-2 was used given the larger vocabulary and more realistic looking text.,6,91,Sample generated text,Sample generated text,"For generating text samples, words were sampled using the standard generation script contained in the PyTorch word level language modeling example.",Sample generated text,Neither the eos token nor the unk were allowed to be selected.,,
3360,Neither the eos token nor the unk were allowed to be selected.,6,92,Sample generated text,Sample generated text,WikiText-2 was used given the larger vocabulary and more realistic looking text.,Sample generated text,"Each paragraph is a separate sample of text with the tokens following Moses (Koehn et al., 2007), joining words with @-@ and dot-decimal split to a @.",,
3361,"Each paragraph is a separate sample of text with the tokens following Moses (Koehn et al., 2007), joining words with @-@ and dot-decimal split to a @.",6,93,Sample generated text,Sample generated text,Neither the eos token nor the unk were allowed to be selected.,Sample generated text,"Something Borrowed "" is the second episode of the fourth season of the American comedy television series The X @-@ Files .",,
3362,"Something Borrowed "" is the second episode of the fourth season of the American comedy television series The X @-@ Files .",6,94,Sample generated text,Sample generated text,"Each paragraph is a separate sample of text with the tokens following Moses (Koehn et al., 2007), joining words with @-@ and dot-decimal split to a @.",Sample generated text,The episode was written by David McCarthy and directed by Mark Sacks .,,
3363,The episode was written by David McCarthy and directed by Mark Sacks .,6,95,Sample generated text,Sample generated text,"Something Borrowed "" is the second episode of the fourth season of the American comedy television series The X @-@ Files .",Sample generated text,"It aired in the United States on November 30 , 2011 , as a two @-@ episode episode, watched by 4 @.",,
3364,"It aired in the United States on November 30 , 2011 , as a two @-@ episode episode, watched by 4 @.",6,96,Sample generated text,Sample generated text,The episode was written by David McCarthy and directed by Mark Sacks .,Sample generated text,@ 9 million viewers and was the highest rated show on the Fox network .,,
3365,@ 9 million viewers and was the highest rated show on the Fox network .,6,97,Sample generated text,Sample generated text,"It aired in the United States on November 30 , 2011 , as a two @-@ episode episode, watched by 4 @.",Sample generated text,"The work of Olivier 's , a large 1950s table with the center of a vinyl beam , was used for bony motifs from the upper @-@ production model via the Club van X .",,
3366,"The work of Olivier 's , a large 1950s table with the center of a vinyl beam , was used for bony motifs from the upper @-@ production model via the Club van X .",6,98,Sample generated text,Sample generated text,@ 9 million viewers and was the highest rated show on the Fox network .,Sample generated text,"The modified works were released in the museum , which gave its namesake to the visual designers in Hong Kong .",,
3367,"The modified works were released in the museum , which gave its namesake to the visual designers in Hong Kong .",6,99,Sample generated text,Sample generated text,"The work of Olivier 's , a large 1950s table with the center of a vinyl beam , was used for bony motifs from the upper @-@ production model via the Club van X .",Sample generated text,"The first prototype was released for the PlayStation 4 , containing the 2 @.",,
3368,"The first prototype was released for the PlayStation 4 , containing the 2 @.",6,100,Sample generated text,Sample generated text,"The modified works were released in the museum , which gave its namesake to the visual designers in Hong Kong .",Sample generated text,"@ 5 part series , with 3 @.",,
3369,"@ 5 part series , with 3 @.",6,101,Sample generated text,Sample generated text,"The first prototype was released for the PlayStation 4 , containing the 2 @.",Sample generated text,@ 5 million copies sold .,,
3370,@ 5 million copies sold .,6,102,Sample generated text,Sample generated text,"@ 5 part series , with 3 @.",Sample generated text,"In October 2010 , Activision announced that both the game and the main gameplay was "" downloadable "" .",,
3371,"In October 2010 , Activision announced that both the game and the main gameplay was "" downloadable "" .",6,103,Sample generated text,Sample generated text,@ 5 million copies sold .,Sample generated text,"The first game , titled Snow : The Game of the Battlefield 2 : The Ultimate Warrior , was the third anime game , and was released in August 2016 .",,
3372,"The first game , titled Snow : The Game of the Battlefield 2 : The Ultimate Warrior , was the third anime game , and was released in August 2016 .",6,104,Sample generated text,Sample generated text,"In October 2010 , Activision announced that both the game and the main gameplay was "" downloadable "" .",Sample generated text,"The German Land Forces had been reversed in the early 1990s , although the Soviet Union continued to deter NDH forces in the nation .",,
3373,"The German Land Forces had been reversed in the early 1990s , although the Soviet Union continued to deter NDH forces in the nation .",6,105,Sample generated text,Sample generated text,"The first game , titled Snow : The Game of the Battlefield 2 : The Ultimate Warrior , was the third anime game , and was released in August 2016 .",Sample generated text,"The area was moved to Sarajevo , and the troops were despatched to the National Register of Historic Places in the summer of 1918 for the establishment of full political and social parties .",,
3374,"The area was moved to Sarajevo , and the troops were despatched to the National Register of Historic Places in the summer of 1918 for the establishment of full political and social parties .",6,106,Sample generated text,Sample generated text,"The German Land Forces had been reversed in the early 1990s , although the Soviet Union continued to deter NDH forces in the nation .",Sample generated text,"The Polish language was protected by the Soviet Union , which was the first Polish continental conflict of the newly formed Union in North America , and the Polish Front with the last of the Polish Communist Party .",,
3375,"The Polish language was protected by the Soviet Union , which was the first Polish continental conflict of the newly formed Union in North America , and the Polish Front with the last of the Polish Communist Party .",6,107,Sample generated text,Sample generated text,"The area was moved to Sarajevo , and the troops were despatched to the National Register of Historic Places in the summer of 1918 for the establishment of full political and social parties .",,,,
