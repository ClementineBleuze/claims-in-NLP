id,text,idx,sec,prev_sec,prev_sent,next_sec,next_sent,label,Comments
4795,We propose a software architecture designed to ease the implementation of dialogue systems.,3075788,abstract,,,abstract,"The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work.",POS,
4796,"The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work.",3075789,abstract,abstract,We propose a software architecture designed to ease the implementation of dialogue systems.,abstract,"The architecture separates the domain of the conversation from the agent's dialogue strategy, and as such can be easily extended to multiple domains.",POS,
4797,"The architecture separates the domain of the conversation from the agent's dialogue strategy, and as such can be easily extended to multiple domains.",3075790,abstract,abstract,"The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work.",abstract,MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data.,POS,
4798,MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data.,3075791,abstract,abstract,"The architecture separates the domain of the conversation from the agent's dialogue strategy, and as such can be easily extended to multiple domains.",abstract,The current version of the framework already incorporates several domains and existing dialogue strategies from the recent literature.,POS,
4799,The current version of the framework already incorporates several domains and existing dialogue strategies from the recent literature.,3075792,abstract,abstract,MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data.,Introduction,Recent research in building sophisticated AIbased dialogue management systems has led to many new models supporting goal oriented or chit-chat style dialogue agents.,POS,
4800,Recent research in building sophisticated AIbased dialogue management systems has led to many new models supporting goal oriented or chit-chat style dialogue agents.,3075793,Introduction,abstract,The current version of the framework already incorporates several domains and existing dialogue strategies from the recent literature.,Introduction,"These models have been applied to a variety of consumer domains, such as restaurant booking (Kim and Banchs, 2014), flight booking (Young, 2006), etc.",RW,
4801,"These models have been applied to a variety of consumer domains, such as restaurant booking (Kim and Banchs, 2014), flight booking (Young, 2006), etc.",3075794,Introduction,Introduction,Recent research in building sophisticated AIbased dialogue management systems has led to many new models supporting goal oriented or chit-chat style dialogue agents.,Introduction,"However, the lack of tools for easy prototyping of newer models remains an impediment to developing new models and properly benchmarking against previous models.",RW,
4802,"However, the lack of tools for easy prototyping of newer models remains an impediment to developing new models and properly benchmarking against previous models.",3075795,Introduction,Introduction,"These models have been applied to a variety of consumer domains, such as restaurant booking (Kim and Banchs, 2014), flight booking (Young, 2006), etc.",Introduction,"Furthermore, the different types of conversational agents-e.g., generative (Hochreiter and Schmidhuber, 1997;Serban et al., 2015Serban et al., , 2016)), retrieval-based (Schatzmann et al., 2005a;Lowe et al., 2015a), slot-based (Young, 2006) or POMDP agents (Png and Pineau, 2011)have different working mechanisms, which pose challenges to the development of a unified platform for conversational agents with multi-domain support. ",POS,
4803,"Furthermore, the different types of conversational agents-e.g., generative (Hochreiter and Schmidhuber, 1997;Serban et al., 2015Serban et al., , 2016)), retrieval-based (Schatzmann et al., 2005a;Lowe et al., 2015a), slot-based (Young, 2006) or POMDP agents (Png and Pineau, 2011)have different working mechanisms, which pose challenges to the development of a unified platform for conversational agents with multi-domain support. ",3075796,Introduction,Introduction,"However, the lack of tools for easy prototyping of newer models remains an impediment to developing new models and properly benchmarking against previous models.",Introduction,"To address this gap, we propose a new, readyto-use, cross-platform framework for text-based conversational agents -MACA 1 (Modularized Architecture for Conversational Agents)-that supports plug-n-play use of several existing dialogue agents, as well as facilitates easy prototyping of new dialogue agents.",RW,
4804,"To address this gap, we propose a new, readyto-use, cross-platform framework for text-based conversational agents -MACA 1 (Modularized Architecture for Conversational Agents)-that supports plug-n-play use of several existing dialogue agents, as well as facilitates easy prototyping of new dialogue agents.",3075797,Introduction,Introduction,"Furthermore, the different types of conversational agents-e.g., generative (Hochreiter and Schmidhuber, 1997;Serban et al., 2015Serban et al., , 2016)), retrieval-based (Schatzmann et al., 2005a;Lowe et al., 2015a), slot-based (Young, 2006) or POMDP agents (Png and Pineau, 2011)have different working mechanisms, which pose challenges to the development of a unified platform for conversational agents with multi-domain support. ",Introduction,The architecture simplifies the specification of different types of dialogue agents and plugs in an already-built dialogue agent.,POS,
4805,The architecture simplifies the specification of different types of dialogue agents and plugs in an already-built dialogue agent.,3075798,Introduction,Introduction,"To address this gap, we propose a new, readyto-use, cross-platform framework for text-based conversational agents -MACA 1 (Modularized Architecture for Conversational Agents)-that supports plug-n-play use of several existing dialogue agents, as well as facilitates easy prototyping of new dialogue agents.",Introduction,"The framework also maintains a clear separation between domain knowledge and the dialogue agent, which improves agent and domain knowledge reusability.",POS,
4806,"The framework also maintains a clear separation between domain knowledge and the dialogue agent, which improves agent and domain knowledge reusability.",3075799,Introduction,Introduction,The architecture simplifies the specification of different types of dialogue agents and plugs in an already-built dialogue agent.,Introduction,MACA separates task definition from task selection and thereby supports multi-task agents that can extend to multiple turns. ,POS,
4807,MACA separates task definition from task selection and thereby supports multi-task agents that can extend to multiple turns. ,3075800,Introduction,Introduction,"The framework also maintains a clear separation between domain knowledge and the dialogue agent, which improves agent and domain knowledge reusability.",Introduction,"The key characteristics of the MACA framework include: • strong separation between domain knowledge and a dialogue agent • a unified architecture to support goaloriented, POMDP, generative, and retrievalbased dialogue agents • easy plug-n-play of custom-built agents • multi-task support for domain specification • reusability of slots across different tasks • tool to collect data from mTurk with ease • template to construct dialogue agents within the framework • independence from dialogue agents' implementation libraries • open source code ready for public sharing",POS,
4808,"The key characteristics of the MACA framework include: • strong separation between domain knowledge and a dialogue agent • a unified architecture to support goaloriented, POMDP, generative, and retrievalbased dialogue agents • easy plug-n-play of custom-built agents • multi-task support for domain specification • reusability of slots across different tasks • tool to collect data from mTurk with ease • template to construct dialogue agents within the framework • independence from dialogue agents' implementation libraries • open source code ready for public sharing",3075801,Introduction,Introduction,MACA separates task definition from task selection and thereby supports multi-task agents that can extend to multiple turns. ,Related Work,There are a few proposed frameworks in recent years that provide easy prototyping of dialogue agents. ,POS,
4809,Domain knowledge contains static background information about the conversation topic.,3075802,Domain Knowledge,Architecture Description,"This helps in block-wise designing of newer systems by preserving the original functionality, yet also providing a free hand in customizing of each component.",Domain Knowledge,"This can take the form of training data (e.g. transcribed conversations), constants, dictionaries, or restrictions on produced responses (e.g. sentence length, banned phrases).",NC,
4810,"This can take the form of training data (e.g. transcribed conversations), constants, dictionaries, or restrictions on produced responses (e.g. sentence length, banned phrases).",3075803,Domain Knowledge,Domain Knowledge,Domain knowledge contains static background information about the conversation topic.,Domain Knowledge,"Data stored in domain knowl-edge must be independent of the model implementation, and can be shared between different models and components.",NC,
4811,"Data stored in domain knowl-edge must be independent of the model implementation, and can be shared between different models and components.",3075804,Domain Knowledge,Domain Knowledge,"This can take the form of training data (e.g. transcribed conversations), constants, dictionaries, or restrictions on produced responses (e.g. sentence length, banned phrases).",Input,"The Input module provides or generates input utterances (i.e. statements, sentences) to the conversation pipeline.",NC,
4812,"The Input module provides or generates input utterances (i.e. statements, sentences) to the conversation pipeline.",3075805,Input,Domain Knowledge,"Data stored in domain knowl-edge must be independent of the model implementation, and can be shared between different models and components.",Input,This component represents an abstract input device whose source of context varies depending on the use case.,NC,
4813,This component represents an abstract input device whose source of context varies depending on the use case.,3075806,Input,Input,"The Input module provides or generates input utterances (i.e. statements, sentences) to the conversation pipeline.",Input,"This could include a database of previous collected conversations, a terminal interface (i.e. stdin) to acquire data in real-time, or a web interface to a data source (e.g. mTurk).",NC,
4814,"This could include a database of previous collected conversations, a terminal interface (i.e. stdin) to acquire data in real-time, or a web interface to a data source (e.g. mTurk).",3075807,Input,Input,This component represents an abstract input device whose source of context varies depending on the use case.,Preprocessing,The Pre-processing module serves as a bridge between raw data acquired via the Input component and the input format required of components of the Dialogue model module.,NC,
4815,"Through the output component, the architecture provides a generic way to output the response to appropriate audience(s) depending on the use case.",3075808,Output,Postprocessing,"In addition, these post-processing operations within the Post-processing component can also query the Domain Knowledge component for relevant data required for the generation of text response.",Output,"Currently, implemented options are command line, file based, web based, and database.",NC,
4816,"Currently, implemented options are command line, file based, web based, and database.",3075809,Output,Output,"Through the output component, the architecture provides a generic way to output the response to appropriate audience(s) depending on the use case.",Output,"Similar to the Input component, the output component provides flexibility for the architect to change the destination of produced outputs and to separate the output programming logic from that of other components.",NC,
4817,"Similar to the Input component, the output component provides flexibility for the architect to change the destination of produced outputs and to separate the output programming logic from that of other components.",3075810,Output,Output,"Currently, implemented options are command line, file based, web based, and database.",Pubsub system/Listeners,"In addition to the main pipeline presented above, the proposed system also includes a passive pubsub layer to facilitate monitoring, conversation recording, and independent evaluation of the model.",NC,
4818,"In addition to the main pipeline presented above, the proposed system also includes a passive pubsub layer to facilitate monitoring, conversation recording, and independent evaluation of the model.",3075811,Pubsub system/Listeners,Output,"Similar to the Input component, the output component provides flexibility for the architect to change the destination of produced outputs and to separate the output programming logic from that of other components.",Pubsub system/Listeners,This pubsub system allows the architect to choose or plug in a wide range of peripheral components (called Listeners) to passively monitor the main system for execution behaviors and performance.,NC,
4819,This pubsub system allows the architect to choose or plug in a wide range of peripheral components (called Listeners) to passively monitor the main system for execution behaviors and performance.,3075812,Pubsub system/Listeners,Pubsub system/Listeners,"In addition to the main pipeline presented above, the proposed system also includes a passive pubsub layer to facilitate monitoring, conversation recording, and independent evaluation of the model.",Pubsub system/Listeners,"On top of several default channels (see Operation modes section below) that the system writes to and reads from, users can freely add their own channels to communicate between the main system and the pubsub layer hosting the peripherals. ",NC,
4820,"On top of several default channels (see Operation modes section below) that the system writes to and reads from, users can freely add their own channels to communicate between the main system and the pubsub layer hosting the peripherals. ",3075813,Pubsub system/Listeners,Pubsub system/Listeners,This pubsub system allows the architect to choose or plug in a wide range of peripheral components (called Listeners) to passively monitor the main system for execution behaviors and performance.,Pubsub system/Listeners,"Listeners, as previously mentioned, are optional modules that can be plugged in to passively monitor the system over different channels.",NC,
4821,"Listeners, as previously mentioned, are optional modules that can be plugged in to passively monitor the system over different channels.",3075814,Pubsub system/Listeners,Pubsub system/Listeners,"On top of several default channels (see Operation modes section below) that the system writes to and reads from, users can freely add their own channels to communicate between the main system and the pubsub layer hosting the peripherals. ",Pubsub system/Listeners,"These modules are useful when the architect is interested in observing the system inputs and/or outputs, or visualizing internal parameters or states of the dialogue model at execution time.",NC,
4822,"These modules are useful when the architect is interested in observing the system inputs and/or outputs, or visualizing internal parameters or states of the dialogue model at execution time.",3075815,Pubsub system/Listeners,Pubsub system/Listeners,"Listeners, as previously mentioned, are optional modules that can be plugged in to passively monitor the system over different channels.",Pubsub system/Listeners,Passive monitoring logic can be independently introduced into the system without modifying the other components' implementations.,POS,
4823,Passive monitoring logic can be independently introduced into the system without modifying the other components' implementations.,3075816,Pubsub system/Listeners,Pubsub system/Listeners,"These modules are useful when the architect is interested in observing the system inputs and/or outputs, or visualizing internal parameters or states of the dialogue model at execution time.",Operation modes,"MACA can be operated in three different modes: Data Collection, Training and Execution.",POS,
4824,MACA offers a unified architecture for dialogue agents that supports the plug-n-play of different types of dialogue agents and different domains.,3075817,Discussion,HRED in execution mode,"Further, an overview of MACA with its instantiated components and their roles is provided in Table 4; specification of these attributes within MACA is achieved through the configuration file.  ",Discussion,"We hope that this will facilitate the fast development of new models, but also foster reproducibility in dialogue system research. ",POS,
4825,"We hope that this will facilitate the fast development of new models, but also foster reproducibility in dialogue system research. ",3075818,Discussion,Discussion,MACA offers a unified architecture for dialogue agents that supports the plug-n-play of different types of dialogue agents and different domains.,Discussion,"A few possible limitations in the current implementation of MACA include simplicity of the pubsub system, lack of support for distributed hosting of different components of the architecture, and lack of support for parallel conversations.",PROSP,
4826,"A few possible limitations in the current implementation of MACA include simplicity of the pubsub system, lack of support for distributed hosting of different components of the architecture, and lack of support for parallel conversations.",3075819,Discussion,Discussion,"We hope that this will facilitate the fast development of new models, but also foster reproducibility in dialogue system research. ",Discussion,"As future work, the pubsub system could be improved by capturing a wider range of system information with more monitoring pubsub channels.",NEG,
4827,"As future work, the pubsub system could be improved by capturing a wider range of system information with more monitoring pubsub channels.",3075820,Discussion,Discussion,"A few possible limitations in the current implementation of MACA include simplicity of the pubsub system, lack of support for distributed hosting of different components of the architecture, and lack of support for parallel conversations.",Discussion,"In addition, we plan to incorporate new domains and agents as they become available, along with comprehensive ML based slot-disambiguation modules.",PROSP,
4828,"In addition, we plan to incorporate new domains and agents as they become available, along with comprehensive ML based slot-disambiguation modules.",3075821,Discussion,Discussion,"As future work, the pubsub system could be improved by capturing a wider range of system information with more monitoring pubsub channels.",,,POS,
4829,  The multi-format information extraction task in the 2021 Language and Intelligence Challenge is designed to comprehensively evaluate information extraction from different dimensions.,8054469,abstract,,,abstract,It consists of an multiple slots relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,NC,
4830,It consists of an multiple slots relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,8054470,abstract,abstract,  The multi-format information extraction task in the 2021 Language and Intelligence Challenge is designed to comprehensively evaluate information extraction from different dimensions.,abstract,Here we describe our system for this multi-format information extraction competition task.,NC,
4831,Here we describe our system for this multi-format information extraction competition task.,8054471,abstract,abstract,It consists of an multiple slots relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,abstract,"Specifically, for the relation extraction subtask, we convert it to a traditional triple extraction task and design a voting based method that makes full use of existing models. ",FACT,
4832,"Specifically, for the relation extraction subtask, we convert it to a traditional triple extraction task and design a voting based method that makes full use of existing models. ",8054472,abstract,abstract,Here we describe our system for this multi-format information extraction competition task.,abstract,"For the sentence-level event extraction subtask, we convert it to a NER task and use a pointer labeling based method for extraction.",NC,
4833,"For the sentence-level event extraction subtask, we convert it to a NER task and use a pointer labeling based method for extraction.",8054473,abstract,abstract,"Specifically, for the relation extraction subtask, we convert it to a traditional triple extraction task and design a voting based method that makes full use of existing models. ",abstract,"Furthermore, considering the annotated trigger information may be helpful for event extraction, we design an auxiliary trigger recognition model and use the multi-task learning mechanism to integrate the trigger features into the event extraction model.",NC,
4834,"Furthermore, considering the annotated trigger information may be helpful for event extraction, we design an auxiliary trigger recognition model and use the multi-task learning mechanism to integrate the trigger features into the event extraction model.",8054474,abstract,abstract,"For the sentence-level event extraction subtask, we convert it to a NER task and use a pointer labeling based method for extraction.",abstract,"For the document-level event extraction subtask, we design an Encoder-Decoder based method and propose a Transformer-alike decoder. ",NC,
4835,"For the document-level event extraction subtask, we design an Encoder-Decoder based method and propose a Transformer-alike decoder. ",8054475,abstract,abstract,"Furthermore, considering the annotated trigger information may be helpful for event extraction, we design an auxiliary trigger recognition model and use the multi-task learning mechanism to integrate the trigger features into the event extraction model.",abstract,"Finally,our system ranks No.4 on the test set leader-board of this multi-format information extraction task, and its F1 scores for the subtasks of relation extraction, event extractions of sentence-level and document-level are 79.887%, 85.179%, and 70.828% respectively.",NC,
4836,"Finally,our system ranks No.4 on the test set leader-board of this multi-format information extraction task, and its F1 scores for the subtasks of relation extraction, event extractions of sentence-level and document-level are 79.887%, 85.179%, and 70.828% respectively.",8054476,abstract,abstract,"For the document-level event extraction subtask, we design an Encoder-Decoder based method and propose a Transformer-alike decoder. ",abstract,The codes of our model are available at {https://github.com/neukg/MultiIE}. ,POS,
4837,The codes of our model are available at {https://github.com/neukg/MultiIE}. ,8054477,abstract,abstract,"Finally,our system ranks No.4 on the test set leader-board of this multi-format information extraction task, and its F1 scores for the subtasks of relation extraction, event extractions of sentence-level and document-level are 79.887%, 85.179%, and 70.828% respectively.",Introduction,Information extraction (IE) aims to extract structured knowledge from unstructured texts.,NC,
4838,Information extraction (IE) aims to extract structured knowledge from unstructured texts.,8054478,Introduction,abstract,The codes of our model are available at {https://github.com/neukg/MultiIE}. ,Introduction,"Named entity recognition (NER), relation extraction (RE) and event extraction (EE) are some fundamental information extraction tasks that focus on extracting entities, relations and events respectively.",NC,
4839,"Named entity recognition (NER), relation extraction (RE) and event extraction (EE) are some fundamental information extraction tasks that focus on extracting entities, relations and events respectively.",8054479,Introduction,Introduction,Information extraction (IE) aims to extract structured knowledge from unstructured texts.,Introduction,"However, most researches only focus on extracting information in a single format, while lacking a unified †",NC,
4840,"However, most researches only focus on extracting information in a single format, while lacking a unified †",8054480,Introduction,Introduction,"Named entity recognition (NER), relation extraction (RE) and event extraction (EE) are some fundamental information extraction tasks that focus on extracting entities, relations and events respectively.",Introduction,These authors contribute equally to this research and are listed randomly.,RW,
4841,These authors contribute equally to this research and are listed randomly.,8054481,Introduction,Introduction,"However, most researches only focus on extracting information in a single format, while lacking a unified †",Introduction,Corresponding author. evaluation platform for IE in different formats.,NC,
4842,Corresponding author. evaluation platform for IE in different formats.,8054482,Introduction,Introduction,These authors contribute equally to this research and are listed randomly.,Introduction,Thus the 2021 Language and Intelligence Challenge (LIC-2021) sets up a multi-format IE competition task which is designed to comprehensively evaluate IE from different dimensions.,NC,
4843,Thus the 2021 Language and Intelligence Challenge (LIC-2021) sets up a multi-format IE competition task which is designed to comprehensively evaluate IE from different dimensions.,8054483,Introduction,Introduction,Corresponding author. evaluation platform for IE in different formats.,Introduction,The task consists of a relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,NC,
4844,The task consists of a relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,8054484,Introduction,Introduction,Thus the 2021 Language and Intelligence Challenge (LIC-2021) sets up a multi-format IE competition task which is designed to comprehensively evaluate IE from different dimensions.,Introduction,The definitions of these subtasks are as follows 1 . Relation Extraction is a task that aims to extract all SPO triples from a given sentence according to a predefined schema set.,NC,
4845,The definitions of these subtasks are as follows 1 . Relation Extraction is a task that aims to extract all SPO triples from a given sentence according to a predefined schema set.,8054485,Introduction,Introduction,The task consists of a relation extraction subtask and two event extraction subtasks that extract events from both sentence-level and document-level.,Introduction,The schema set defines relation P and the types of its corresponding subject S and object O.,NC,
4846,The schema set defines relation P and the types of its corresponding subject S and object O.,8054486,Introduction,Introduction,The definitions of these subtasks are as follows 1 . Relation Extraction is a task that aims to extract all SPO triples from a given sentence according to a predefined schema set.,Introduction,"According to the complexity of object O, there are two types of schemas.",NC,
4847,"According to the complexity of object O, there are two types of schemas.",8054487,Introduction,Introduction,The schema set defines relation P and the types of its corresponding subject S and object O.,Introduction,The first one is the single-O-value schema in which the object O gets only a single slot and value.,NC,
4848,The first one is the single-O-value schema in which the object O gets only a single slot and value.,8054488,Introduction,Introduction,"According to the complexity of object O, there are two types of schemas.",Introduction,The second one is the schema in which the object O is a structure composed of multiple slots and their corresponding values. ,NC,
4849,The second one is the schema in which the object O is a structure composed of multiple slots and their corresponding values. ,8054489,Introduction,Introduction,The first one is the single-O-value schema in which the object O gets only a single slot and value.,Introduction,Event Extraction consists of two subtasks.,NC,
4850,Event Extraction consists of two subtasks.,8054490,Introduction,Introduction,The second one is the schema in which the object O is a structure composed of multiple slots and their corresponding values. ,Introduction,"The first one is the sentencelevel event (SEE) extraction whose aim is that given a sentence and predefined event types with corresponding argument roles, to identify all events of target types mentioned in the sentence, and extract corresponding event arguments playing target roles.",NC,
4851,"The first one is the sentencelevel event (SEE) extraction whose aim is that given a sentence and predefined event types with corresponding argument roles, to identify all events of target types mentioned in the sentence, and extract corresponding event arguments playing target roles.",8054491,Introduction,Introduction,Event Extraction consists of two subtasks.,Introduction,The predefined event types and argument roles restrict the scope of extraction.,NC,
4852,The predefined event types and argument roles restrict the scope of extraction.,8054492,Introduction,Introduction,"The first one is the sentencelevel event (SEE) extraction whose aim is that given a sentence and predefined event types with corresponding argument roles, to identify all events of target types mentioned in the sentence, and extract corresponding event arguments playing target roles.",Introduction,"The second one is the document-level event (DEE) extraction task, which shares almost the same definition with the previous one, but considers input text fragments in document-level rather than sentence-level, and restricts the event types to be extracted in the financial field. ",NC,
4853,"The second one is the document-level event (DEE) extraction task, which shares almost the same definition with the previous one, but considers input text fragments in document-level rather than sentence-level, and restricts the event types to be extracted in the financial field. ",8054493,Introduction,Introduction,The predefined event types and argument roles restrict the scope of extraction.,Introduction,Table 1 demonstrates some schema examples for these subtasks.,NC,
4854,Table 1 demonstrates some schema examples for these subtasks.,8054494,Introduction,Introduction,"The second one is the document-level event (DEE) extraction task, which shares almost the same definition with the previous one, but considers input text fragments in document-level rather than sentence-level, and restricts the event types to be extracted in the financial field. ",Introduction,"For example, in the given multiple-O-values schema RE example, the object consists of two items: inWork and @value, but traditional RE may only get @value item in this example.",NC,
4855,"For example, in the given multiple-O-values schema RE example, the object consists of two items: inWork and @value, but traditional RE may only get @value item in this example.",8054495,Introduction,Introduction,Table 1 demonstrates some schema examples for these subtasks.,Introduction,"Compared with traditional single-format IE task, there are following challenges in the multi-format IE task designed in LIC-2021. ",NC,
4856,"Compared with traditional single-format IE task, there are following challenges in the multi-format IE task designed in LIC-2021. ",8054496,Introduction,Introduction,"For example, in the given multiple-O-values schema RE example, the object consists of two items: inWork and @value, but traditional RE may only get @value item in this example.",Introduction,"First, despite the success of existing joint entity and relation extraction methods, they cannot be directly applied to the multiple slots RE task in LIC-2021 because of the multiple-O-values schema relations. ",RW,
4857,"First, despite the success of existing joint entity and relation extraction methods, they cannot be directly applied to the multiple slots RE task in LIC-2021 because of the multiple-O-values schema relations. ",8054497,Introduction,Introduction,"Compared with traditional single-format IE task, there are following challenges in the multi-format IE task designed in LIC-2021. ",Introduction,"Second, the event extraction is more challenging due to the overlapping event role issue: an argument may have multiple roles.",RW,
4858,"Second, the event extraction is more challenging due to the overlapping event role issue: an argument may have multiple roles.",8054498,Introduction,Introduction,"First, despite the success of existing joint entity and relation extraction methods, they cannot be directly applied to the multiple slots RE task in LIC-2021 because of the multiple-O-values schema relations. ",Introduction,"Besides, there are annotated triggers provided in the given datasets, and how to effectively use this kind of trigger information is still an open issue. ",RW,
4859,"Besides, there are annotated triggers provided in the given datasets, and how to effectively use this kind of trigger information is still an open issue. ",8054499,Introduction,Introduction,"Second, the event extraction is more challenging due to the overlapping event role issue: an argument may have multiple roles.",Introduction,"Third, it is difficult to extract different events that have the same event type in the DEE subtask.",RW,
4860,"Third, it is difficult to extract different events that have the same event type in the DEE subtask.",8054500,Introduction,Introduction,"Besides, there are annotated triggers provided in the given datasets, and how to effectively use this kind of trigger information is still an open issue. ",Introduction,"For example, for the ""be interviewed "" event type shown in Table 1, different people maybe interviewed at different time.",RW,
4861,"For example, for the ""be interviewed "" event type shown in Table 1, different people maybe interviewed at different time.",8054501,Introduction,Introduction,"Third, it is difficult to extract different events that have the same event type in the DEE subtask.",Introduction,"For this case, in the SEE subtask, two different time arguments and two interviewed people are regarded as arguments for the same event.",NC,
4862,"For this case, in the SEE subtask, two different time arguments and two interviewed people are regarded as arguments for the same event.",8054502,Introduction,Introduction,"For example, for the ""be interviewed "" event type shown in Table 1, different people maybe interviewed at different time.",Introduction,"However, these two arguments need to be classified into arguments of two different events in the DEE subtask. ",NC,
4863,"However, these two arguments need to be classified into arguments of two different events in the DEE subtask. ",8054503,Introduction,Introduction,"For this case, in the SEE subtask, two different time arguments and two interviewed people are regarded as arguments for the same event.",Introduction,"In our system, we use some effective methods to overcome these challenges.",NC,
4864,"In our system, we use some effective methods to overcome these challenges.",8054504,Introduction,Introduction,"However, these two arguments need to be classified into arguments of two different events in the DEE subtask. ",Introduction,"Specifically, for the first one, we design a schema disintegration module to convert each multiple-O-values relation into several single-O-value relations, then use a voting based module to obtain the final relations.",POS,
4865,"Specifically, for the first one, we design a schema disintegration module to convert each multiple-O-values relation into several single-O-value relations, then use a voting based module to obtain the final relations.",8054505,Introduction,Introduction,"In our system, we use some effective methods to overcome these challenges.",Introduction,"For the second one, we convert  [1] and [13]) focus on SEE.",NC,
4866,"For the second one, we convert  [1] and [13]) focus on SEE.",8054506,Introduction,Introduction,"Specifically, for the first one, we design a schema disintegration module to convert each multiple-O-values relation into several single-O-value relations, then use a voting based module to obtain the final relations.",Introduction,"Usually, they use a pipeline based framework that predicts triggers firstly, and then predict arguments.",NC,
4867,"Usually, they use a pipeline based framework that predicts triggers firstly, and then predict arguments.",8054507,Introduction,Introduction,"For the second one, we convert  [1] and [13]) focus on SEE.",Introduction,"Recently, DEE is attracting more and more attentions.",RW,
4868,"Recently, DEE is attracting more and more attentions.",8054508,Introduction,Introduction,"Usually, they use a pipeline based framework that predicts triggers firstly, and then predict arguments.",Introduction,"Usually there are two main challenges in DEE, which are: (i) arguments of one event may scatter across some long-distance sentences of a document or even different documents, and (ii) each document is likely to contain multiple events.",RW,
4869,"Usually there are two main challenges in DEE, which are: (i) arguments of one event may scatter across some long-distance sentences of a document or even different documents, and (ii) each document is likely to contain multiple events.",8054509,Introduction,Introduction,"Recently, DEE is attracting more and more attentions.",Introduction,"A representative Chinese DEE work is [17], whose DEE model contains two main components: a SEE model that extracts event arguments and event triggers from a sentence; and a DEE model that extracts event arguments from the whole document based on a key event detection model and an arguments-completion strategy.",RW,
4870,"A representative Chinese DEE work is [17], whose DEE model contains two main components: a SEE model that extracts event arguments and event triggers from a sentence; and a DEE model that extracts event arguments from the whole document based on a key event detection model and an arguments-completion strategy.",8054510,Introduction,Introduction,"Usually there are two main challenges in DEE, which are: (i) arguments of one event may scatter across some long-distance sentences of a document or even different documents, and (ii) each document is likely to contain multiple events.",Introduction,"Another representative Chinese DEE work is [20], which uses an entity-based directed acyclic graph to fulfill the DEE task effectively.",RW,
4871,"Another representative Chinese DEE work is [20], which uses an entity-based directed acyclic graph to fulfill the DEE task effectively.",8054511,Introduction,Introduction,"A representative Chinese DEE work is [17], whose DEE model contains two main components: a SEE model that extracts event arguments and event triggers from a sentence; and a DEE model that extracts event arguments from the whole document based on a key event detection model and an arguments-completion strategy.",Introduction,"Moreover, they redefine a DEE task with the no-trigger-words design to ease document-level event labeling.",RW,
4872,"Moreover, they redefine a DEE task with the no-trigger-words design to ease document-level event labeling.",8054512,Introduction,Introduction,"Another representative Chinese DEE work is [20], which uses an entity-based directed acyclic graph to fulfill the DEE task effectively.",Introduction,Both of these two DEE work are evaluated on Chinese financial field.,RW,
4873,Both of these two DEE work are evaluated on Chinese financial field.,8054513,Introduction,Introduction,"Moreover, they redefine a DEE task with the no-trigger-words design to ease document-level event labeling.",Relation Extraction,The architecture of our RE model is shown in Fig 1 .,RW,
4874,The architecture of our RE model is shown in Fig 1 .,8054514,Relation Extraction,Introduction,Both of these two DEE work are evaluated on Chinese financial field.,Relation Extraction,We can see that it consists of two main modules: a schema disintegration module and a voting module. ,NC,
4875,We can see that it consists of two main modules: a schema disintegration module and a voting module. ,8054515,Relation Extraction,Relation Extraction,The architecture of our RE model is shown in Fig 1 .,Relation Extraction,"Given a multiple-O-values triple 2 {s, p, [(o k1 , o v1 ), ..., (o km , o vm )]}, the schema disintegration module would transform it into 2m-1 single-O-value triples.",NC,
4876,"Given a multiple-O-values triple 2 {s, p, [(o k1 , o v1 ), ..., (o km , o vm )]}, the schema disintegration module would transform it into 2m-1 single-O-value triples.",8054516,Relation Extraction,Relation Extraction,We can see that it consists of two main modules: a schema disintegration module and a voting module. ,Relation Extraction,The concrete transformation process is as follows.,NC,
4877,The concrete transformation process is as follows.,8054517,Relation Extraction,Relation Extraction,"Given a multiple-O-values triple 2 {s, p, [(o k1 , o v1 ), ..., (o km , o vm )]}, the schema disintegration module would transform it into 2m-1 single-O-value triples.",Relation Extraction,"First, o v1 will be taken as an object to form a triple {s, p, o v1 }.",NC,
4878,"First, o v1 will be taken as an object to form a triple {s, p, o v1 }.",8054518,Relation Extraction,Relation Extraction,The concrete transformation process is as follows.,Relation Extraction,"Then from i = 2 on, each (o ki , o vi ) will form following two triples: {s, p-o ki , o vi } and {o v1 , o ki , o vi }.",NC,
4879,"Then from i = 2 on, each (o ki , o vi ) will form following two triples: {s, p-o ki , o vi } and {o v1 , o ki , o vi }.",8054519,Relation Extraction,Relation Extraction,"First, o v1 will be taken as an object to form a triple {s, p, o v1 }.",Relation Extraction,"Here both s and o v1 will be repeatedly taken as subjects in the formed triples, and p-o ki is a new generated predicate.",NC,
4880,"Here both s and o v1 will be repeatedly taken as subjects in the formed triples, and p-o ki is a new generated predicate.",8054520,Relation Extraction,Relation Extraction,"Then from i = 2 on, each (o ki , o vi ) will form following two triples: {s, p-o ki , o vi } and {o v1 , o ki , o vi }.",Relation Extraction,"Accordingly, the given example will generate following triples, which are {s, p, o v1 }, {s, p-o k2 , o v2 }, {o v1 , o k2 , o v2 }, ..., {s, p-o km , o vm }, {o v1 , o km , o vm }. ",NC,
4881,"Accordingly, the given example will generate following triples, which are {s, p, o v1 }, {s, p-o k2 , o v2 }, {o v1 , o k2 , o v2 }, ..., {s, p-o km , o vm }, {o v1 , o km , o vm }. ",8054521,Relation Extraction,Relation Extraction,"Here both s and o v1 will be repeatedly taken as subjects in the formed triples, and p-o ki is a new generated predicate.",Relation Extraction,"If there are some single-O-value triples, they can be formed into a multi-O-value triple with a reverse process. ",NC,
4882,"If there are some single-O-value triples, they can be formed into a multi-O-value triple with a reverse process. ",8054522,Relation Extraction,Relation Extraction,"Accordingly, the given example will generate following triples, which are {s, p, o v1 }, {s, p-o k2 , o v2 }, {o v1 , o k2 , o v2 }, ..., {s, p-o km , o vm }, {o v1 , o km , o vm }. ",Relation Extraction,"In the voting module, three existing state-of-the-art triple extraction models are used to extract single-O-value triples separately.",NC,
4883,"In the voting module, three existing state-of-the-art triple extraction models are used to extract single-O-value triples separately.",8054523,Relation Extraction,Relation Extraction,"If there are some single-O-value triples, they can be formed into a multi-O-value triple with a reverse process. ",Relation Extraction,Then their results would be voted to output the triples that receive more votes.,NC,
4884,Then their results would be voted to output the triples that receive more votes.,8054524,Relation Extraction,Relation Extraction,"In the voting module, three existing state-of-the-art triple extraction models are used to extract single-O-value triples separately.",Relation Extraction,"Next, these obtained triples are converted into some multiple-O-values triples 3 as final results of this RE task.",NC,
4885,"Next, these obtained triples are converted into some multiple-O-values triples 3 as final results of this RE task.",8054525,Relation Extraction,Relation Extraction,Then their results would be voted to output the triples that receive more votes.,Relation Extraction,"In our system, following three existing state-of-the-art models are used for voting: TPLinker [14], SPN [10] and CasRel [16].",NC,
4886,"In our system, following three existing state-of-the-art models are used for voting: TPLinker [14], SPN [10] and CasRel [16].",8054526,Relation Extraction,Relation Extraction,"Next, these obtained triples are converted into some multiple-O-values triples 3 as final results of this RE task.",Sentence-level Event Extraction,"In our system, we treat each event argument as an entity, and concatenate this argument's corresponding event type and role as the entity type.",NC,
4887,"In our system, we treat each event argument as an entity, and concatenate this argument's corresponding event type and role as the entity type.",8054527,Sentence-level Event Extraction,Relation Extraction,"In our system, following three existing state-of-the-art models are used for voting: TPLinker [14], SPN [10] and CasRel [16].",Sentence-level Event Extraction,Then the SEE task is converted into a NER task.,NC,
4888,Then the SEE task is converted into a NER task.,8054528,Sentence-level Event Extraction,Sentence-level Event Extraction,"In our system, we treat each event argument as an entity, and concatenate this argument's corresponding event type and role as the entity type.",Sentence-level Event Extraction,But there is a multiple label issue in the SEE task: an argument may belong to multiple event types.,NC,
4889,But there is a multiple label issue in the SEE task: an argument may belong to multiple event types.,8054529,Sentence-level Event Extraction,Sentence-level Event Extraction,Then the SEE task is converted into a NER task.,Sentence-level Event Extraction,"To address this issue, here we use a pointer labeling based NER method that tags the start token and end token of an entity span for each candidate entity type.",NC,
4890,"To address this issue, here we use a pointer labeling based NER method that tags the start token and end token of an entity span for each candidate entity type.",8054530,Sentence-level Event Extraction,Sentence-level Event Extraction,But there is a multiple label issue in the SEE task: an argument may belong to multiple event types.,Sentence-level Event Extraction,"Specifically, as shown in Eq.",NC,
4891,"Specifically, as shown in Eq.",8054531,Sentence-level Event Extraction,Sentence-level Event Extraction,"To address this issue, here we use a pointer labeling based NER method that tags the start token and end token of an entity span for each candidate entity type.",Sentence-level Event Extraction,"( 1), for each entity type, the model compute two probabilities for each word to indicate the possibilities of this word being the start and end tokens of an entity that possesses this entity type. ",NC,
4892,"( 1), for each entity type, the model compute two probabilities for each word to indicate the possibilities of this word being the start and end tokens of an entity that possesses this entity type. ",8054532,Sentence-level Event Extraction,Sentence-level Event Extraction,"Specifically, as shown in Eq.",Sentence-level Event Extraction,p sij = sigmoid(W S,NC,
4893,p sij = sigmoid(W S,8054533,Sentence-level Event Extraction,Sentence-level Event Extraction,"( 1), for each entity type, the model compute two probabilities for each word to indicate the possibilities of this word being the start and end tokens of an entity that possesses this entity type. ",Sentence-level Event Extraction,"+ b S i ), p eij = sigmoid(W E",NC,
4894,"+ b S i ), p eij = sigmoid(W E",8054534,Sentence-level Event Extraction,Sentence-level Event Extraction,p sij = sigmoid(W S,Sentence-level Event Extraction,"i ∈ R d } r i=1 , {b i ∈ R} r i=1 are learnable parameters for the i-th entity type, h j ∈ R d is the token representation for the j-th word and is obtained by a pretrained language model, r is the number of entity types, p sij and p eij is the probabilities of the j-th word being the start token and the end token of an entiy that should be labeled with the i-th entity type. ",NC,
4895,"i ∈ R d } r i=1 , {b i ∈ R} r i=1 are learnable parameters for the i-th entity type, h j ∈ R d is the token representation for the j-th word and is obtained by a pretrained language model, r is the number of entity types, p sij and p eij is the probabilities of the j-th word being the start token and the end token of an entiy that should be labeled with the i-th entity type. ",8054535,Sentence-level Event Extraction,Sentence-level Event Extraction,"+ b S i ), p eij = sigmoid(W E",Sentence-level Event Extraction,"Furthermore, to make full use of the features from the annotated triggers, we design an auxiliary trigger recognition module that also recognizes triggers in the same NER manner as used above.",NC,
4896,"Furthermore, to make full use of the features from the annotated triggers, we design an auxiliary trigger recognition module that also recognizes triggers in the same NER manner as used above.",8054536,Sentence-level Event Extraction,Sentence-level Event Extraction,"i ∈ R d } r i=1 , {b i ∈ R} r i=1 are learnable parameters for the i-th entity type, h j ∈ R d is the token representation for the j-th word and is obtained by a pretrained language model, r is the number of entity types, p sij and p eij is the probabilities of the j-th word being the start token and the end token of an entiy that should be labeled with the i-th entity type. ",Sentence-level Event Extraction,This auxiliary module are trained jointly with the above argument recognition module in a multi-task learning manner. ,NC,
4897,This auxiliary module are trained jointly with the above argument recognition module in a multi-task learning manner. ,8054537,Sentence-level Event Extraction,Sentence-level Event Extraction,"Furthermore, to make full use of the features from the annotated triggers, we design an auxiliary trigger recognition module that also recognizes triggers in the same NER manner as used above.",Sentence-level Event Extraction,"During training, the trigger recognition module will generate a representation for each trigger.",NC,
4898,"During training, the trigger recognition module will generate a representation for each trigger.",8054538,Sentence-level Event Extraction,Sentence-level Event Extraction,This auxiliary module are trained jointly with the above argument recognition module in a multi-task learning manner. ,Sentence-level Event Extraction,Then these trigger representations will be merged into the token representations used by the argument recognition module with an attention mechanism for the argument recognition of next iteration.,NC,
4899,Then these trigger representations will be merged into the token representations used by the argument recognition module with an attention mechanism for the argument recognition of next iteration.,8054539,Sentence-level Event Extraction,Sentence-level Event Extraction,"During training, the trigger recognition module will generate a representation for each trigger.",Sentence-level Event Extraction,"But during inferencing, our model will first recognize all triggers (the results are denoted as T = {t 1 , t 2 , ...}).",NC,
4900,"But during inferencing, our model will first recognize all triggers (the results are denoted as T = {t 1 , t 2 , ...}).",8054540,Sentence-level Event Extraction,Sentence-level Event Extraction,Then these trigger representations will be merged into the token representations used by the argument recognition module with an attention mechanism for the argument recognition of next iteration.,Sentence-level Event Extraction,Then these triggers' representations will be fused into a unified representation (denoted as t) by a max-pooling operation.,NC,
4901,Then these triggers' representations will be fused into a unified representation (denoted as t) by a max-pooling operation.,8054541,Sentence-level Event Extraction,Sentence-level Event Extraction,"But during inferencing, our model will first recognize all triggers (the results are denoted as T = {t 1 , t 2 , ...}).",Sentence-level Event Extraction,"Next, an additiveattention based method is used to obtained a new token representation sequence (denoted as Ĥ), which will then be used as input for the argument recognition module.",NC,
4902,"Next, an additiveattention based method is used to obtained a new token representation sequence (denoted as Ĥ), which will then be used as input for the argument recognition module.",8054542,Sentence-level Event Extraction,Sentence-level Event Extraction,Then these triggers' representations will be fused into a unified representation (denoted as t) by a max-pooling operation.,Sentence-level Event Extraction,"Specifically, Ĥ is computed with following Eq.",NC,
4903,"Specifically, Ĥ is computed with following Eq.",8054543,Sentence-level Event Extraction,Sentence-level Event Extraction,"Next, an additiveattention based method is used to obtained a new token representation sequence (denoted as Ĥ), which will then be used as input for the argument recognition module.",Sentence-level Event Extraction,α = sof tmax(V T tanh(W,NC,
4904,α = sof tmax(V T tanh(W,8054544,Sentence-level Event Extraction,Sentence-level Event Extraction,"Specifically, Ĥ is computed with following Eq.",Sentence-level Event Extraction,"1 H + W 2 t)), Ĥ = α • H ( ) where H is the original token representations, V , W 1 , and W 2 are learnable parameters, and the superscript T denotes a matrix transpose operation.",NC,
4905,"1 H + W 2 t)), Ĥ = α • H ( ) where H is the original token representations, V , W 1 , and W 2 are learnable parameters, and the superscript T denotes a matrix transpose operation.",8054545,Sentence-level Event Extraction,Sentence-level Event Extraction,α = sof tmax(V T tanh(W,Document-level Event Extraction,The architecture of our DEE model is shown in Fig.,NC,
4906,The architecture of our DEE model is shown in Fig.,8054546,Document-level Event Extraction,Sentence-level Event Extraction,"1 H + W 2 t)), Ĥ = α • H ( ) where H is the original token representations, V , W 1 , and W 2 are learnable parameters, and the superscript T denotes a matrix transpose operation.",Document-level Event Extraction,"We can see it contains three main modules, which are Encoder, Decoder, and Output.",NC,
4907,"We can see it contains three main modules, which are Encoder, Decoder, and Output.",8054547,Document-level Event Extraction,Document-level Event Extraction,The architecture of our DEE model is shown in Fig.,Document-level Event Extraction,BERT [2] and some of its variants are used as Encoder to get a contextaware representation for each token in a document.,NC,
4908,BERT [2] and some of its variants are used as Encoder to get a contextaware representation for each token in a document.,8054548,Document-level Event Extraction,Document-level Event Extraction,"We can see it contains three main modules, which are Encoder, Decoder, and Output.",Document-level Event Extraction,"Note that both BERT and its variants have a length restriction for the input text (usually 512 TOKENs), so we use the sliding window based method to split a long document into several segments.",NC,
4909,"Note that both BERT and its variants have a length restriction for the input text (usually 512 TOKENs), so we use the sliding window based method to split a long document into several segments.",8054549,Document-level Event Extraction,Document-level Event Extraction,BERT [2] and some of its variants are used as Encoder to get a contextaware representation for each token in a document.,Document-level Event Extraction,Then each segment is taken as an input to be processed by our DEE model and the results of these segments are combined as final results.,NC,
4910,Then each segment is taken as an input to be processed by our DEE model and the results of these segments are combined as final results.,8054550,Document-level Event Extraction,Document-level Event Extraction,"Note that both BERT and its variants have a length restriction for the input text (usually 512 TOKENs), so we use the sliding window based method to split a long document into several segments.",Document-level Event Extraction,"Given a segment S = {e 0 , e 1 , ..., e l }, the output of Encoder is a token embedding sequence, we denote it as H ∈ R l×d , where l is the number of tokens in the given segment, and d is the dimension of the token embedding. ",NC,
4911,"Given a segment S = {e 0 , e 1 , ..., e l }, the output of Encoder is a token embedding sequence, we denote it as H ∈ R l×d , where l is the number of tokens in the given segment, and d is the dimension of the token embedding. ",8054551,Document-level Event Extraction,Document-level Event Extraction,Then each segment is taken as an input to be processed by our DEE model and the results of these segments are combined as final results.,Document-level Event Extraction,"In the Decoder module, some learnable query embeddings (as shown in Fig. 2, denoted as (q 0 , q 1 , ..., q m ))",NC,
4912,"In the Decoder module, some learnable query embeddings (as shown in Fig. 2, denoted as (q 0 , q 1 , ..., q m ))",8054552,Document-level Event Extraction,Document-level Event Extraction,"Given a segment S = {e 0 , e 1 , ..., e l }, the output of Encoder is a token embedding sequence, we denote it as H ∈ R l×d , where l is the number of tokens in the given segment, and d is the dimension of the token embedding. ",Document-level Event Extraction,along with the token embedding sequence outputted by the Encoder module are taken as input to predict events.,NC,
4913,along with the token embedding sequence outputted by the Encoder module are taken as input to predict events.,8054553,Document-level Event Extraction,Document-level Event Extraction,"In the Decoder module, some learnable query embeddings (as shown in Fig. 2, denoted as (q 0 , q 1 , ..., q m ))",Document-level Event Extraction,Each query embedding corresponds to a specific event.,NC,
4914,Each query embedding corresponds to a specific event.,8054554,Document-level Event Extraction,Document-level Event Extraction,along with the token embedding sequence outputted by the Encoder module are taken as input to predict events.,Document-level Event Extraction,"Although different documents may have different numbers of events, we set a fixed number (denoted as m, and is set to 16 here) for the used queries, which is obtained according to the maximal event number in a document of the training dataset. ",NC,
4915,"Although different documents may have different numbers of events, we set a fixed number (denoted as m, and is set to 16 here) for the used queries, which is obtained according to the maximal event number in a document of the training dataset. ",8054555,Document-level Event Extraction,Document-level Event Extraction,Each query embedding corresponds to a specific event.,Document-level Event Extraction,"Here our designed Decoder is similar to the one used in Transformer [11], but we delete the components of masking and positional encoding because there is no correlations and position relations between event's query embeddings in the DEE task.",,
4916,"Here our designed Decoder is similar to the one used in Transformer [11], but we delete the components of masking and positional encoding because there is no correlations and position relations between event's query embeddings in the DEE task.",8054556,Document-level Event Extraction,Document-level Event Extraction,"Although different documents may have different numbers of events, we set a fixed number (denoted as m, and is set to 16 here) for the used queries, which is obtained according to the maximal event number in a document of the training dataset. ",Document-level Event Extraction,"Our Decoder module consists of N stacked layers, and each layer consists of a multi-head self-attention module, a multi-head inter-attention module, and a feed forward network.",NC,
4917,"Our Decoder module consists of N stacked layers, and each layer consists of a multi-head self-attention module, a multi-head inter-attention module, and a feed forward network.",8054557,Document-level Event Extraction,Document-level Event Extraction,"Here our designed Decoder is similar to the one used in Transformer [11], but we delete the components of masking and positional encoding because there is no correlations and position relations between event's query embeddings in the DEE task.",Document-level Event Extraction,It will generate a refined embedding representation sequence (denoted as T ∈ R m×l×d ) for the input queries.,NC,
4918,It will generate a refined embedding representation sequence (denoted as T ∈ R m×l×d ) for the input queries.,8054558,Document-level Event Extraction,Document-level Event Extraction,"Our Decoder module consists of N stacked layers, and each layer consists of a multi-head self-attention module, a multi-head inter-attention module, and a feed forward network.",Document-level Event Extraction,"Specifically, the multihead self-attention module is first performed to highlight some queries.",NC,
4919,"Specifically, the multihead self-attention module is first performed to highlight some queries.",8054559,Document-level Event Extraction,Document-level Event Extraction,It will generate a refined embedding representation sequence (denoted as T ∈ R m×l×d ) for the input queries.,Document-level Event Extraction,"Then, the multi-head inter-attention operation is performed to highlight the correlations between this input queries and the input sentence.",NC,
4920,"Then, the multi-head inter-attention operation is performed to highlight the correlations between this input queries and the input sentence.",8054560,Document-level Event Extraction,Document-level Event Extraction,"Specifically, the multihead self-attention module is first performed to highlight some queries.",Document-level Event Extraction,"Next, a feed forward network is used to generate a new embedding representation sequence for the input query.",NC,
4921,"Next, a feed forward network is used to generate a new embedding representation sequence for the input query.",8054561,Document-level Event Extraction,Document-level Event Extraction,"Then, the multi-head inter-attention operation is performed to highlight the correlations between this input queries and the input sentence.",Document-level Event Extraction,The above three steps will be performed N times to obtained T . ,NC,
4922,The above three steps will be performed N times to obtained T . ,8054562,Document-level Event Extraction,Document-level Event Extraction,"Next, a feed forward network is used to generate a new embedding representation sequence for the input query.",Document-level Event Extraction,"The Output module is just the same as the extraction model for DEE, which uses a pointer labeling based method and an auxiliary trigger recognition module together to predict the event arguments.",NC,
4923,"The Output module is just the same as the extraction model for DEE, which uses a pointer labeling based method and an auxiliary trigger recognition module together to predict the event arguments.",8054563,Document-level Event Extraction,Document-level Event Extraction,The above three steps will be performed N times to obtained T . ,Document-level Event Extraction,"Specifically, it takes each t",NC,
4924,"Specifically, it takes each t",8054564,Document-level Event Extraction,Document-level Event Extraction,"The Output module is just the same as the extraction model for DEE, which uses a pointer labeling based method and an auxiliary trigger recognition module together to predict the event arguments.",Document-level Event Extraction,"i ∈ R l×d in T as input, and computes two kinds of probabilities, S i ∈ R l×r and E i ∈ R l×r , to denote the probabilities of each word being the start and end tokens of an entity that should be labeled with the i-th event argument.",NC,
4925,"i ∈ R l×d in T as input, and computes two kinds of probabilities, S i ∈ R l×r and E i ∈ R l×r , to denote the probabilities of each word being the start and end tokens of an entity that should be labeled with the i-th event argument.",8054565,Document-level Event Extraction,Document-level Event Extraction,"Specifically, it takes each t",Loss function,"The cross entropy loss function is used for both the tasks of RE and SEE, but we use a bipartite matching loss for DEE.",NC,
4926,"The cross entropy loss function is used for both the tasks of RE and SEE, but we use a bipartite matching loss for DEE.",8054566,Loss function,Document-level Event Extraction,"i ∈ R l×d in T as input, and computes two kinds of probabilities, S i ∈ R l×r and E i ∈ R l×r , to denote the probabilities of each word being the start and end tokens of an entity that should be labeled with the i-th event argument.",Loss function,The main difficulty in DEE is that the predicted arguments are not always in the same order as those in the ground truths.,NC,
4927,The main difficulty in DEE is that the predicted arguments are not always in the same order as those in the ground truths.,8054567,Loss function,Loss function,"The cross entropy loss function is used for both the tasks of RE and SEE, but we use a bipartite matching loss for DEE.",Loss function,"For example, an event is predicted by the i-th query embedding, but it may be in the j-th position of the ground truths.",NC,
4928,"For example, an event is predicted by the i-th query embedding, but it may be in the j-th position of the ground truths.",8054568,Loss function,Loss function,The main difficulty in DEE is that the predicted arguments are not always in the same order as those in the ground truths.,Loss function,"Thus, we do not apply the cross-entropy loss function directly because this loss is sensitive to the permutation of the predictions.",NC,
4929,"Thus, we do not apply the cross-entropy loss function directly because this loss is sensitive to the permutation of the predictions.",8054569,Loss function,Loss function,"For example, an event is predicted by the i-th query embedding, but it may be in the j-th position of the ground truths.",Loss function,"Instead, we use the bipartite matching loss proposed in SPN to produce an optimal matching between m predicted events and m ground truth events.",NC,
4930,"Instead, we use the bipartite matching loss proposed in SPN to produce an optimal matching between m predicted events and m ground truth events.",8054570,Loss function,Loss function,"Thus, we do not apply the cross-entropy loss function directly because this loss is sensitive to the permutation of the predictions.",Loss function,"Specifically, we concatenate S and E (see above, generated in DEE), into Ŷ ∈ R m×l×2×r , that can represent the predicted events.",NC,
4931,"Specifically, we concatenate S and E (see above, generated in DEE), into Ŷ ∈ R m×l×2×r , that can represent the predicted events.",8054571,Loss function,Loss function,"Instead, we use the bipartite matching loss proposed in SPN to produce an optimal matching between m predicted events and m ground truth events.",Loss function,The ground truths is denoted as Y ∈ R m×l×2×r .,NC,
4932,The ground truths is denoted as Y ∈ R m×l×2×r .,8054572,Loss function,Loss function,"Specifically, we concatenate S and E (see above, generated in DEE), into Ŷ ∈ R m×l×2×r , that can represent the predicted events.",Loss function,The process of computing bipartite matching loss is divided into two steps: finding an optimal matching and computing the loss function.,NC,
4933,The process of computing bipartite matching loss is divided into two steps: finding an optimal matching and computing the loss function.,8054573,Loss function,Loss function,The ground truths is denoted as Y ∈ R m×l×2×r .,Loss function,"To find optimal matching between Y and Ŷ , we search for a permutation of elements π * with the lowest cost l match , as shown in Eq.( 3). ",NC,
4934,"To find optimal matching between Y and Ŷ , we search for a permutation of elements π * with the lowest cost l match , as shown in Eq.( 3). ",8054574,Loss function,Loss function,The process of computing bipartite matching loss is divided into two steps: finding an optimal matching and computing the loss function.,Loss function,where S(m) is the space of all m-length permutations.,NC,
4935,where S(m) is the space of all m-length permutations.,8054575,Loss function,Loss function,"To find optimal matching between Y and Ŷ , we search for a permutation of elements π * with the lowest cost l match , as shown in Eq.( 3). ",Loss function,∈ R l×2×r are outputs of the i-th predicated event and the i-th ground truth.,NC,
4936,∈ R l×2×r are outputs of the i-th predicated event and the i-th ground truth.,8054576,Loss function,Loss function,where S(m) is the space of all m-length permutations.,Loss function,"l match (Y i , Ŷπ(i) ) is the pair-wise matching cost between the ground truth Y i and the prediction event with index π(i), and it is be computed as follow: l match = Ŷπ(i) • Y i ( ) where • is Hadamard product, and the optimal π * can be be computed in polynomial time (O(m 3 )) via the Hungarian algorithm 4 .",NC,
4937,"l match (Y i , Ŷπ(i) ) is the pair-wise matching cost between the ground truth Y i and the prediction event with index π(i), and it is be computed as follow: l match = Ŷπ(i) • Y i ( ) where • is Hadamard product, and the optimal π * can be be computed in polynomial time (O(m 3 )) via the Hungarian algorithm 4 .",8054577,Loss function,Loss function,∈ R l×2×r are outputs of the i-th predicated event and the i-th ground truth.,Loss function,Readers can find more detailed introduction about this loss in SPN. ,NC,
4938,Readers can find more detailed introduction about this loss in SPN. ,8054578,Loss function,Loss function,"l match (Y i , Ŷπ(i) ) is the pair-wise matching cost between the ground truth Y i and the prediction event with index π(i), and it is be computed as follow: l match = Ŷπ(i) • Y i ( ) where • is Hadamard product, and the optimal π * can be be computed in polynomial time (O(m 3 )) via the Hungarian algorithm 4 .",Loss function,"After π * is obtained, we change the event order of the predicated Ŷπ * to be in line with π * , and the re-ordered result is denoted as Y * .",NC,
4939,"After π * is obtained, we change the event order of the predicated Ŷπ * to be in line with π * , and the re-ordered result is denoted as Y * .",8054579,Loss function,Loss function,Readers can find more detailed introduction about this loss in SPN. ,Loss function,Then we use the cross entropy loss function to compute the loss between Y * and Y .,NC,
4940,Then we use the cross entropy loss function to compute the loss between Y * and Y .,8054580,Loss function,Loss function,"After π * is obtained, we change the event order of the predicated Ŷπ * to be in line with π * , and the re-ordered result is denoted as Y * .",Model Enhancement Techniques,"In our system, some model enhancement techiques are used to further improve the performance of each subtask.",NC,
4941,"The main experimental results are shown in Table 3, in which Backbone, TR, BML, Ens, AT, and PLM to denote the backbone models, the trigger recognition module, the bipartite matching loss, the ensemble model, the adversarial training, and the pretrained language models respectively.",8054581,Main Results,Model Enhancement Techniques,"And the epochs for these three subtasks are 10, 40, and 30 respectively.",Main Results,"In all of our experiments, Backbone refers to the model in which the transformer decoder and the bipartite matching loss are removed. ",NC,
4942,"In all of our experiments, Backbone refers to the model in which the transformer decoder and the bipartite matching loss are removed. ",8054582,Main Results,Main Results,"The main experimental results are shown in Table 3, in which Backbone, TR, BML, Ens, AT, and PLM to denote the backbone models, the trigger recognition module, the bipartite matching loss, the ensemble model, the adversarial training, and the pretrained language models respectively.",Main Results,"From Table 3 we can see that our RE model achieves far better results than all the compared state-of-the-art triple extraction models like CasRel, TPLinker, and SPN.",NC,
4943,"From Table 3 we can see that our RE model achieves far better results than all the compared state-of-the-art triple extraction models like CasRel, TPLinker, and SPN.",8054583,Main Results,Main Results,"In all of our experiments, Backbone refers to the model in which the transformer decoder and the bipartite matching loss are removed. ",Main Results,Here we donot compare our two EE models with other state-of-the-art models because most of existing EE models cannot be used here directly.,POS,
4944,Here we donot compare our two EE models with other state-of-the-art models because most of existing EE models cannot be used here directly.,8054584,Main Results,Main Results,"From Table 3 we can see that our RE model achieves far better results than all the compared state-of-the-art triple extraction models like CasRel, TPLinker, and SPN.",Main Results,Ablation Results From the ablation results in Table 3 we can see that the pretrained language models are much helpful for all subtasks and they always bring a significant performance gain for a subtask.,NC,
4945,Ablation Results From the ablation results in Table 3 we can see that the pretrained language models are much helpful for all subtasks and they always bring a significant performance gain for a subtask.,8054585,Main Results,Main Results,Here we donot compare our two EE models with other state-of-the-art models because most of existing EE models cannot be used here directly.,Main Results,"Besides, the adversarial training is also helpful and it consistently improves the performance of a subtask. ",POS,
4946,"Besides, the adversarial training is also helpful and it consistently improves the performance of a subtask. ",8054586,Main Results,Main Results,Ablation Results From the ablation results in Table 3 we can see that the pretrained language models are much helpful for all subtasks and they always bring a significant performance gain for a subtask.,Main Results,"From the results of both SEE and DEE we can see that the designed trigger recognition module plays a very important role to the performance, and it improves nearly 1.5% point for the F1 score of SEE, and more than 2.1% point for the F1 score of DEE.",POS,
4947,"From the results of both SEE and DEE we can see that the designed trigger recognition module plays a very important role to the performance, and it improves nearly 1.5% point for the F1 score of SEE, and more than 2.1% point for the F1 score of DEE.",8054587,Main Results,Main Results,"Besides, the adversarial training is also helpful and it consistently improves the performance of a subtask. ",Main Results,"In fact, this module plays even more roles than the pretrained language models.",POS,
4948,"In fact, this module plays even more roles than the pretrained language models.",8054588,Main Results,Main Results,"From the results of both SEE and DEE we can see that the designed trigger recognition module plays a very important role to the performance, and it improves nearly 1.5% point for the F1 score of SEE, and more than 2.1% point for the F1 score of DEE.",Main Results,"Based on these results we can conclude that the triggers do contain some important cues for both kinds of EE subtasks, and making full of these cues are much helpful for improving the performance. ",POS,
4949,"Based on these results we can conclude that the triggers do contain some important cues for both kinds of EE subtasks, and making full of these cues are much helpful for improving the performance. ",8054589,Main Results,Main Results,"In fact, this module plays even more roles than the pretrained language models.",Main Results,From the results of DEE we can see that the bipartite matching loss also plays a helpful role to improve the performance.,POS,
4950,From the results of DEE we can see that the bipartite matching loss also plays a helpful role to improve the performance.,8054590,Main Results,Main Results,"Based on these results we can conclude that the triggers do contain some important cues for both kinds of EE subtasks, and making full of these cues are much helpful for improving the performance. ",Main Results,And it brings more performance gain than the adversarial training. ,POS,
4951,And it brings more performance gain than the adversarial training. ,8054591,Main Results,Main Results,From the results of DEE we can see that the bipartite matching loss also plays a helpful role to improve the performance.,Main Results,"Besides, we also conduct experiments on the DEE subtask to evaluate: (i) the impact of the layer number of the transformer-based decoder (the number N in Fig. 2) , and (ii) the impact of different sizes of sliding windows.",POS,
4952,"Besides, we also conduct experiments on the DEE subtask to evaluate: (i) the impact of the layer number of the transformer-based decoder (the number N in Fig. 2) , and (ii) the impact of different sizes of sliding windows.",8054592,Main Results,Main Results,And it brings more performance gain than the adversarial training. ,Main Results,"For the first one, we test different numbers in the range of 0 to 5.",NC,
4953,"For the first one, we test different numbers in the range of 0 to 5.",8054593,Main Results,Main Results,"Besides, we also conduct experiments on the DEE subtask to evaluate: (i) the impact of the layer number of the transformer-based decoder (the number N in Fig. 2) , and (ii) the impact of different sizes of sliding windows.",Main Results,"Our experiments show that when the number of decoder layer is set to 3, the model achieves the best results.",NC,
4954,"Our experiments show that when the number of decoder layer is set to 3, the model achieves the best results.",8054594,Main Results,Main Results,"For the first one, we test different numbers in the range of 0 to 5.",Main Results,"We think this is mainly because that a moderate layer number will lead to more complete integration of input information into event queries, while a larger or smaller number will make the model be overfit or underfit.",POS,
4955,"We think this is mainly because that a moderate layer number will lead to more complete integration of input information into event queries, while a larger or smaller number will make the model be overfit or underfit.",8054595,Main Results,Main Results,"Our experiments show that when the number of decoder layer is set to 3, the model achieves the best results.",Main Results,"For the second one, we set the size of sliding windows to 128, 256, and 512.",POS,
4956,"For the second one, we set the size of sliding windows to 128, 256, and 512.",8054596,Main Results,Main Results,"We think this is mainly because that a moderate layer number will lead to more complete integration of input information into event queries, while a larger or smaller number will make the model be overfit or underfit.",Main Results,"And their F1 scores are 64.8%, 66.9%, and 69.2% respectively when the number of the decoder's layer is set to 3 and the bipartite matching loss is used.",NC,
4957,"And their F1 scores are 64.8%, 66.9%, and 69.2% respectively when the number of the decoder's layer is set to 3 and the bipartite matching loss is used.",8054597,Main Results,Main Results,"For the second one, we set the size of sliding windows to 128, 256, and 512.",Main Results,"These results show that usually, the performance would increase as the size of the sliding window increases.",POS,
4958,"These results show that usually, the performance would increase as the size of the sliding window increases.",8054598,Main Results,Main Results,"And their F1 scores are 64.8%, 66.9%, and 69.2% respectively when the number of the decoder's layer is set to 3 and the bipartite matching loss is used.",Main Results,This is because that more context features can be taken into consideration with a larger sliding windows size.,POS,
4959,This is because that more context features can be taken into consideration with a larger sliding windows size.,8054599,Main Results,Main Results,"These results show that usually, the performance would increase as the size of the sliding window increases.",Conclusions,This paper describes our system to the LIC-2021 multi-format IE task.,POS,
4960,This paper describes our system to the LIC-2021 multi-format IE task.,8054600,Conclusions,Main Results,This is because that more context features can be taken into consideration with a larger sliding windows size.,Conclusions,"We use different methods to overcome different challenges in this competition task, including the schema disintegration method for the multiple-O-values schema issue in the RE subtask, the multi-task learning method for the SEE subtask, and the Transform-alike decoder for the DEE subtask.",FACT,
4961,"We use different methods to overcome different challenges in this competition task, including the schema disintegration method for the multiple-O-values schema issue in the RE subtask, the multi-task learning method for the SEE subtask, and the Transform-alike decoder for the DEE subtask.",8054601,Conclusions,Conclusions,This paper describes our system to the LIC-2021 multi-format IE task.,Conclusions,Experimental results show that our system is effective and it ranks No.4 on the final test set leader-board of this competition.,NC,
4962,Experimental results show that our system is effective and it ranks No.4 on the final test set leader-board of this competition.,8054602,Conclusions,Conclusions,"We use different methods to overcome different challenges in this competition task, including the schema disintegration method for the multiple-O-values schema issue in the RE subtask, the multi-task learning method for the SEE subtask, and the Transform-alike decoder for the DEE subtask.",Conclusions,"Its F1 scores are 79.887% on DuIE2.0, 85.179% on DuEE1.0, and 70.828% on DuEE-fin respectively. ",POS,
4963,"Its F1 scores are 79.887% on DuIE2.0, 85.179% on DuEE1.0, and 70.828% on DuEE-fin respectively. ",8054603,Conclusions,Conclusions,Experimental results show that our system is effective and it ranks No.4 on the final test set leader-board of this competition.,Conclusions,"However, there is still plenty of room for improvement, and lots of work should be further explored.",POS,
4964,"However, there is still plenty of room for improvement, and lots of work should be further explored.",8054604,Conclusions,Conclusions,"Its F1 scores are 79.887% on DuIE2.0, 85.179% on DuEE1.0, and 70.828% on DuEE-fin respectively. ",Conclusions,"First, in the RE subtask, many triples are not annotated.",NEG,
4965,"First, in the RE subtask, many triples are not annotated.",8054605,Conclusions,Conclusions,"However, there is still plenty of room for improvement, and lots of work should be further explored.",Conclusions,"These triples gives model wrong supervisions, which is very harmful for the performance.",NEG,
4966,"These triples gives model wrong supervisions, which is very harmful for the performance.",8054606,Conclusions,Conclusions,"First, in the RE subtask, many triples are not annotated.",Conclusions,"But this missing annotation issue is still an open issue, and should be further explored.",NEG,
4967,"But this missing annotation issue is still an open issue, and should be further explored.",8054607,Conclusions,Conclusions,"These triples gives model wrong supervisions, which is very harmful for the performance.",Conclusions,"Second, in the DEE subtask, how to process long text is still a challenge that is worthy being further studied.",PROSP,
4968,"Second, in the DEE subtask, how to process long text is still a challenge that is worthy being further studied.",8054608,Conclusions,Conclusions,"But this missing annotation issue is still an open issue, and should be further explored.",Conclusions,"In addition, if two arguments of one event are far away in the given text (either a sentence or a document), it would be difficult to extract them correctly.",PROSP,
4969,"In addition, if two arguments of one event are far away in the given text (either a sentence or a document), it would be difficult to extract them correctly.",8054609,Conclusions,Conclusions,"Second, in the DEE subtask, how to process long text is still a challenge that is worthy being further studied.",Conclusions,This issue also should be well studied in the future.,PROSP,
4970,This issue also should be well studied in the future.,8054610,Conclusions,Conclusions,"In addition, if two arguments of one event are far away in the given text (either a sentence or a document), it would be difficult to extract them correctly.",,,PROSP,
