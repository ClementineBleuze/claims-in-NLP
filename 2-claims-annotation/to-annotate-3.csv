text,doc_id,paper_title,year,section,prev_text,prev_section,next_text,next_section,label
This paper focuses on domain specific use of MT with a special focus on SMT in the workflow of a Language Service Provider (LSP).,1378144,Domain specific {MT} in use,2008,abstract,,,"We report on the feedback of post-editors using fluency/adequacy evaluation and the evaluation metric 'Usability', understood in this context as where users on a three point scale evaluate the sentence from the point of view of the post-editor.",abstract,
"We report on the feedback of post-editors using fluency/adequacy evaluation and the evaluation metric 'Usability', understood in this context as where users on a three point scale evaluate the sentence from the point of view of the post-editor.",1378145,Domain specific {MT} in use,2008,abstract,This paper focuses on domain specific use of MT with a special focus on SMT in the workflow of a Language Service Provider (LSP).,abstract,The post-editor profile defined by the LSP is based on the experiences of introducing MT in the LSP workflow.,abstract,
The post-editor profile defined by the LSP is based on the experiences of introducing MT in the LSP workflow.,1378146,Domain specific {MT} in use,2008,abstract,"We report on the feedback of post-editors using fluency/adequacy evaluation and the evaluation metric 'Usability', understood in this context as where users on a three point scale evaluate the sentence from the point of view of the post-editor.",abstract,The relation between the Translation Edit Rate (TER) scores and 'Usability' scores is tested.,abstract,
The relation between the Translation Edit Rate (TER) scores and 'Usability' scores is tested.,1378147,Domain specific {MT} in use,2008,abstract,The post-editor profile defined by the LSP is based on the experiences of introducing MT in the LSP workflow.,abstract,We find TER a candidate for an automatic metric simulating the post-editors' usability judgements.,abstract,
We find TER a candidate for an automatic metric simulating the post-editors' usability judgements.,1378148,Domain specific {MT} in use,2008,abstract,The relation between the Translation Edit Rate (TER) scores and 'Usability' scores is tested.,abstract,LSP tests show 67% saved time in post-editing for the tested domain.,abstract,
LSP tests show 67% saved time in post-editing for the tested domain.,1378149,Domain specific {MT} in use,2008,abstract,We find TER a candidate for an automatic metric simulating the post-editors' usability judgements.,abstract,"Finally, the use of weighted sub-domain phrase tables in a SMT system is shown to improve translation quality.",abstract,
"Finally, the use of weighted sub-domain phrase tables in a SMT system is shown to improve translation quality.",1378150,Domain specific {MT} in use,2008,abstract,LSP tests show 67% saved time in post-editing for the tested domain.,abstract,"As part of a general strategy to strengthen cooperation between the research community and small and medium-sized enterprises, the Danish Council for Strategic Research has decided to co-finance two projects involving machine translation.",Introduction,
"As part of a general strategy to strengthen cooperation between the research community and small and medium-sized enterprises, the Danish Council for Strategic Research has decided to co-finance two projects involving machine translation.",1378151,Domain specific {MT} in use,2008,Introduction,"Finally, the use of weighted sub-domain phrase tables in a SMT system is shown to improve translation quality.",abstract,The aim of both projects has been to explore the possibilities of using statistical machine translation (SMT) approaches in small and medium sized companies (SMEs).,Introduction,
The aim of both projects has been to explore the possibilities of using statistical machine translation (SMT) approaches in small and medium sized companies (SMEs).,1378152,Domain specific {MT} in use,2008,Introduction,"As part of a general strategy to strengthen cooperation between the research community and small and medium-sized enterprises, the Danish Council for Strategic Research has decided to co-finance two projects involving machine translation.",Introduction,"The project goals were to find out not only whether it would possible for translation companies to integrate SMT systems in their daily translation flow, but also to assess whether it would be financially beneficial.",Introduction,
"The project goals were to find out not only whether it would possible for translation companies to integrate SMT systems in their daily translation flow, but also to assess whether it would be financially beneficial.",1378153,Domain specific {MT} in use,2008,Introduction,The aim of both projects has been to explore the possibilities of using statistical machine translation (SMT) approaches in small and medium sized companies (SMEs).,Introduction,The primary tasks of the involved translation companies have been to provide bilingual corpora consisting of sentence aligned documents and then subsequently to test and evaluate the translation results as a first step to uncover the commercial potential of using SMT in their translation process. ,Introduction,
The primary tasks of the involved translation companies have been to provide bilingual corpora consisting of sentence aligned documents and then subsequently to test and evaluate the translation results as a first step to uncover the commercial potential of using SMT in their translation process. ,1378154,Domain specific {MT} in use,2008,Introduction,"The project goals were to find out not only whether it would possible for translation companies to integrate SMT systems in their daily translation flow, but also to assess whether it would be financially beneficial.",Introduction,"This paper builds on the results of the second project, mainly carried out in 2007, involving at the research side Copenhagen Business School and the University of Copenhagen and on the business side Inter-Set, a medium-sized language service provider (LSP) with subsidiaries in other countries. ",Introduction,
"This paper builds on the results of the second project, mainly carried out in 2007, involving at the research side Copenhagen Business School and the University of Copenhagen and on the business side Inter-Set, a medium-sized language service provider (LSP) with subsidiaries in other countries. ",1378155,Domain specific {MT} in use,2008,Introduction,The primary tasks of the involved translation companies have been to provide bilingual corpora consisting of sentence aligned documents and then subsequently to test and evaluate the translation results as a first step to uncover the commercial potential of using SMT in their translation process. ,Introduction,The Open Source Moses MT system [1] was used both for training the SMT system and for translation.,Introduction,
The Open Source Moses MT system [1] was used both for training the SMT system and for translation.,1378156,Domain specific {MT} in use,2008,Introduction,"This paper builds on the results of the second project, mainly carried out in 2007, involving at the research side Copenhagen Business School and the University of Copenhagen and on the business side Inter-Set, a medium-sized language service provider (LSP) with subsidiaries in other countries. ",Introduction,"MOSES is currently mainly supported under the EuroMatrix project, funded by the European Commission.",Introduction,
"MOSES is currently mainly supported under the EuroMatrix project, funded by the European Commission.",1378157,Domain specific {MT} in use,2008,Introduction,The Open Source Moses MT system [1] was used both for training the SMT system and for translation.,Introduction,"The language model was 12th EAMT conference, 22-23 September 2008, Hamburg, Germany trained using the language modelling toolkit IRSTLM [2].",Introduction,
"The language model was 12th EAMT conference, 22-23 September 2008, Hamburg, Germany trained using the language modelling toolkit IRSTLM [2].",1378158,Domain specific {MT} in use,2008,Introduction,"MOSES is currently mainly supported under the EuroMatrix project, funded by the European Commission.",Introduction,The language models were trained with order 5.,Introduction,
The language models were trained with order 5.,1378159,Domain specific {MT} in use,2008,Introduction,"The language model was 12th EAMT conference, 22-23 September 2008, Hamburg, Germany trained using the language modelling toolkit IRSTLM [2].",Introduction,The maximum length of phrases in the phrase tables was set to 5.,Introduction,
The maximum length of phrases in the phrase tables was set to 5.,1378160,Domain specific {MT} in use,2008,Introduction,The language models were trained with order 5.,Introduction,The assumption that there would be a productivity gain using SMT was the prime motivation factor for the LSP to investigate the potential of SMT systems.,Domain Issues in SMT,
The assumption that there would be a productivity gain using SMT was the prime motivation factor for the LSP to investigate the potential of SMT systems.,1378161,Domain specific {MT} in use,2008,Domain Issues in SMT,The maximum length of phrases in the phrase tables was set to 5.,Introduction,One of the issues to be considered in this context was the handling of subject domains. ,Domain Issues in SMT,
One of the issues to be considered in this context was the handling of subject domains. ,1378162,Domain specific {MT} in use,2008,Domain Issues in SMT,The assumption that there would be a productivity gain using SMT was the prime motivation factor for the LSP to investigate the potential of SMT systems.,Domain Issues in SMT,"In an ideal world, all users involved with translation of technical documents would apply the same large-scale general subject classification system such as Lenoch [3].",Domain Issues in SMT,
"In an ideal world, all users involved with translation of technical documents would apply the same large-scale general subject classification system such as Lenoch [3].",1378163,Domain specific {MT} in use,2008,Domain Issues in SMT,One of the issues to be considered in this context was the handling of subject domains. ,Domain Issues in SMT,From an SMT point of view the advantages of a consistent use of a classification system would be obvious.,Domain Issues in SMT,
From an SMT point of view the advantages of a consistent use of a classification system would be obvious.,1378164,Domain specific {MT} in use,2008,Domain Issues in SMT,"In an ideal world, all users involved with translation of technical documents would apply the same large-scale general subject classification system such as Lenoch [3].",Domain Issues in SMT,"Not only would it ease the identification of consistent and representative bilingual training data, it would also, via the fine-grained subject classification, increase the probability that the lexical coverage of a given SMTsystem would be tuned for the texts to be translated. ",Domain Issues in SMT,
"Not only would it ease the identification of consistent and representative bilingual training data, it would also, via the fine-grained subject classification, increase the probability that the lexical coverage of a given SMTsystem would be tuned for the texts to be translated. ",1378165,Domain specific {MT} in use,2008,Domain Issues in SMT,From an SMT point of view the advantages of a consistent use of a classification system would be obvious.,Domain Issues in SMT,But unfortunately experiences show that use of a large universal classification system involves too much administrative work [4].,Domain Issues in SMT,
But unfortunately experiences show that use of a large universal classification system involves too much administrative work [4].,1378166,Domain specific {MT} in use,2008,Domain Issues in SMT,"Not only would it ease the identification of consistent and representative bilingual training data, it would also, via the fine-grained subject classification, increase the probability that the lexical coverage of a given SMTsystem would be tuned for the texts to be translated. ",Domain Issues in SMT,"In addition, subject classification systems do not take into account possible divergences in the data within the same subject domain, e.g. different companies may have chosen to use different specific company terminologies. ",Domain Issues in SMT,
"In addition, subject classification systems do not take into account possible divergences in the data within the same subject domain, e.g. different companies may have chosen to use different specific company terminologies. ",1378167,Domain specific {MT} in use,2008,Domain Issues in SMT,But unfortunately experiences show that use of a large universal classification system involves too much administrative work [4].,Domain Issues in SMT,"Besides, texts from the same subject domain will make use of very different writing styles in terms of sentence types and varieties in language usage according to the genre of the text.",Domain Issues in SMT,
"Besides, texts from the same subject domain will make use of very different writing styles in terms of sentence types and varieties in language usage according to the genre of the text.",1378168,Domain specific {MT} in use,2008,Domain Issues in SMT,"In addition, subject classification systems do not take into account possible divergences in the data within the same subject domain, e.g. different companies may have chosen to use different specific company terminologies. ",Domain Issues in SMT,"Marketing texts, for instance, may praise the features of the product while manuals focus on strict instructions on how to use the product. ",Domain Issues in SMT,
"Marketing texts, for instance, may praise the features of the product while manuals focus on strict instructions on how to use the product. ",1378169,Domain specific {MT} in use,2008,Domain Issues in SMT,"Besides, texts from the same subject domain will make use of very different writing styles in terms of sentence types and varieties in language usage according to the genre of the text.",Domain Issues in SMT,"Consequently, in principle it would be preferable to train an SMT system on texts with almost identical writing styles and within the same subject domain.",Domain Issues in SMT,
"Consequently, in principle it would be preferable to train an SMT system on texts with almost identical writing styles and within the same subject domain.",1378170,Domain specific {MT} in use,2008,Domain Issues in SMT,"Marketing texts, for instance, may praise the features of the product while manuals focus on strict instructions on how to use the product. ",Domain Issues in SMT,"On the other hand, for practical and financial reasons, it would desirable that the SMT system had a broad coverage being usable for different text types without a negative impact on the translation quality.",Domain Issues in SMT,
"On the other hand, for practical and financial reasons, it would desirable that the SMT system had a broad coverage being usable for different text types without a negative impact on the translation quality.",1378171,Domain specific {MT} in use,2008,Domain Issues in SMT,"Consequently, in principle it would be preferable to train an SMT system on texts with almost identical writing styles and within the same subject domain.",Domain Issues in SMT,"So, the solution is a compromise. ",Domain Issues in SMT,
"So, the solution is a compromise. ",1378172,Domain specific {MT} in use,2008,Domain Issues in SMT,"On the other hand, for practical and financial reasons, it would desirable that the SMT system had a broad coverage being usable for different text types without a negative impact on the translation quality.",Domain Issues in SMT,In general the LSPs are very aware that different products and clients use different writing styles in terms of sentence types and variation in language usage and terminology.,Domain Issues in SMT,
In general the LSPs are very aware that different products and clients use different writing styles in terms of sentence types and variation in language usage and terminology.,1378173,Domain specific {MT} in use,2008,Domain Issues in SMT,"So, the solution is a compromise. ",Domain Issues in SMT,"With a focus on delivering high quality translation, it is obvious that the clients' expectations regarding correct handling of terminology and writing style had to be met, also for the SMT system.",Domain Issues in SMT,
"With a focus on delivering high quality translation, it is obvious that the clients' expectations regarding correct handling of terminology and writing style had to be met, also for the SMT system.",1378174,Domain specific {MT} in use,2008,Domain Issues in SMT,In general the LSPs are very aware that different products and clients use different writing styles in terms of sentence types and variation in language usage and terminology.,Domain Issues in SMT,"Therefore, the researchers and the LSP in collaboration tested the suitability of 5 different sub-domains of manuals to see how well these different sub-domains could be translated with the SMT system.",Domain Issues in SMT,
"Therefore, the researchers and the LSP in collaboration tested the suitability of 5 different sub-domains of manuals to see how well these different sub-domains could be translated with the SMT system.",1378175,Domain specific {MT} in use,2008,Domain Issues in SMT,"With a focus on delivering high quality translation, it is obvious that the clients' expectations regarding correct handling of terminology and writing style had to be met, also for the SMT system.",Domain Issues in SMT,The LSP chose 5 candidate sub-domains within the domain of technical manuals and collected training material for these topics.,Selection of Sub-domains,
The LSP chose 5 candidate sub-domains within the domain of technical manuals and collected training material for these topics.,1378176,Domain specific {MT} in use,2008,Selection of Sub-domains,"Therefore, the researchers and the LSP in collaboration tested the suitability of 5 different sub-domains of manuals to see how well these different sub-domains could be translated with the SMT system.",Domain Issues in SMT,"The training data were aligned sentences extracted from the LSP's translation memories (TMs) and consisted of approximately 135,000 parallel sentences.",Selection of Sub-domains,
"The training data were aligned sentences extracted from the LSP's translation memories (TMs) and consisted of approximately 135,000 parallel sentences.",1378177,Domain specific {MT} in use,2008,Selection of Sub-domains,The LSP chose 5 candidate sub-domains within the domain of technical manuals and collected training material for these topics.,Selection of Sub-domains,From each sub-domain a development test corpus of 250 test sentences was extracted.,Selection of Sub-domains,
From each sub-domain a development test corpus of 250 test sentences was extracted.,1378178,Domain specific {MT} in use,2008,Selection of Sub-domains,"The training data were aligned sentences extracted from the LSP's translation memories (TMs) and consisted of approximately 135,000 parallel sentences.",Selection of Sub-domains,To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU,Selection of Sub-domains,
To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU,1378179,Domain specific {MT} in use,2008,Selection of Sub-domains,From each sub-domain a development test corpus of 250 test sentences was extracted.,Selection of Sub-domains,"As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below.",Selection of Sub-domains,
"As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below.",1378180,Domain specific {MT} in use,2008,Selection of Sub-domains,To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU,Selection of Sub-domains,"Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on.",Selection of Sub-domains,
"Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on.",1378181,Domain specific {MT} in use,2008,Selection of Sub-domains,"As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below.",Selection of Sub-domains,"For sub-domain B, however, the client had already started to use MT and an additional SMT system for this sub-domain therefore turned out to be without interest.",Selection of Sub-domains,
"For sub-domain B, however, the client had already started to use MT and an additional SMT system for this sub-domain therefore turned out to be without interest.",1378182,Domain specific {MT} in use,2008,Selection of Sub-domains,"Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on.",Selection of Sub-domains,"The LSP chose to focus on sub-domain E, since sub-domain D has shorter sentences and better match in TM.",Selection of Sub-domains,
"The LSP chose to focus on sub-domain E, since sub-domain D has shorter sentences and better match in TM.",1378183,Domain specific {MT} in use,2008,Selection of Sub-domains,"For sub-domain B, however, the client had already started to use MT and an additional SMT system for this sub-domain therefore turned out to be without interest.",Selection of Sub-domains,The focus point of MT evaluation differs dependent on your perspective.,Evaluation Test Results,
The focus point of MT evaluation differs dependent on your perspective.,1378184,Domain specific {MT} in use,2008,Evaluation Test Results,"The LSP chose to focus on sub-domain E, since sub-domain D has shorter sentences and better match in TM.",Selection of Sub-domains,"From the developers' point of view, evaluation as part of testing the MT system has to be quick and cheap.",Evaluation Test Results,
"From the developers' point of view, evaluation as part of testing the MT system has to be quick and cheap.",1378185,Domain specific {MT} in use,2008,Evaluation Test Results,The focus point of MT evaluation differs dependent on your perspective.,Evaluation Test Results,"While from the users' point of view, the evaluation has to focus on easier use, better translation quality, quicker post-editing etc.",Evaluation Test Results,
"While from the users' point of view, the evaluation has to focus on easier use, better translation quality, quicker post-editing etc.",1378186,Domain specific {MT} in use,2008,Evaluation Test Results,"From the developers' point of view, evaluation as part of testing the MT system has to be quick and cheap.",Evaluation Test Results,We have carried out both types of evaluation and compared results.,Evaluation Test Results,
We have carried out both types of evaluation and compared results.,1378187,Domain specific {MT} in use,2008,Evaluation Test Results,"While from the users' point of view, the evaluation has to focus on easier use, better translation quality, quicker post-editing etc.",Evaluation Test Results,For the system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements.,Automatic evaluation measures,
For the system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements.,1378188,Domain specific {MT} in use,2008,Automatic evaluation measures,We have carried out both types of evaluation and compared results.,Evaluation Test Results,"Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER",Automatic evaluation measures,
"Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER",1378189,Domain specific {MT} in use,2008,Automatic evaluation measures,For the system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements.,Automatic evaluation measures,"[6] There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9].",Automatic evaluation measures,
"[6] There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9].",1378190,Domain specific {MT} in use,2008,Automatic evaluation measures,"Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER",Automatic evaluation measures,"For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11].",Automatic evaluation measures,
"For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11].",1378191,Domain specific {MT} in use,2008,Automatic evaluation measures,"[6] There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9].",Automatic evaluation measures,"The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference.",Automatic evaluation measures,
"The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference.",1378192,Domain specific {MT} in use,2008,Automatic evaluation measures,"For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11].",Automatic evaluation measures,It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13].,Automatic evaluation measures,
It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13].,1378193,Domain specific {MT} in use,2008,Automatic evaluation measures,"The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference.",Automatic evaluation measures,"But nevertheless, the metric is still widely used to measure development improvements in systems.",Automatic evaluation measures,
"But nevertheless, the metric is still widely used to measure development improvements in systems.",1378194,Domain specific {MT} in use,2008,Automatic evaluation measures,It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13].,Automatic evaluation measures,"TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references.",Automatic evaluation measures,
"TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references.",1378195,Domain specific {MT} in use,2008,Automatic evaluation measures,"But nevertheless, the metric is still widely used to measure development improvements in systems.",Automatic evaluation measures,TER is stated to correlate reasonably well with human judgements,Automatic evaluation measures,
TER is stated to correlate reasonably well with human judgements,1378196,Domain specific {MT} in use,2008,Automatic evaluation measures,"TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references.",Automatic evaluation measures,"TER values will be in the range from 0 (translated sentence is exactly like the reference) to in principle more that 100, e.g. if the reference sentence consists of only a few words whereas the translation output contains too many words and therefore needs more edits that the length of the reference sentence.",Automatic evaluation measures,
"TER values will be in the range from 0 (translated sentence is exactly like the reference) to in principle more that 100, e.g. if the reference sentence consists of only a few words whereas the translation output contains too many words and therefore needs more edits that the length of the reference sentence.",1378197,Domain specific {MT} in use,2008,Automatic evaluation measures,TER is stated to correlate reasonably well with human judgements,Automatic evaluation measures,"From a user's point of view, automatic evaluation figures are somewhat abstract and difficult to comprehend and do not necessarily provide feedback to the questions raised above.",User evaluation measures,
"From a user's point of view, automatic evaluation figures are somewhat abstract and difficult to comprehend and do not necessarily provide feedback to the questions raised above.",1378198,Domain specific {MT} in use,2008,User evaluation measures,"TER values will be in the range from 0 (translated sentence is exactly like the reference) to in principle more that 100, e.g. if the reference sentence consists of only a few words whereas the translation output contains too many words and therefore needs more edits that the length of the reference sentence.",Automatic evaluation measures,Alternative evaluation metrics focussing much more on the human translation aspect have conceived in order to meet this problem.,User evaluation measures,
Alternative evaluation metrics focussing much more on the human translation aspect have conceived in order to meet this problem.,1378199,Domain specific {MT} in use,2008,User evaluation measures,"From a user's point of view, automatic evaluation figures are somewhat abstract and difficult to comprehend and do not necessarily provide feedback to the questions raised above.",User evaluation measures,The following metrics represent this alternative evaluation approach:  Fluency and adequacy scoring ,User evaluation measures,
The following metrics represent this alternative evaluation approach:  Fluency and adequacy scoring ,1378200,Domain specific {MT} in use,2008,User evaluation measures,Alternative evaluation metrics focussing much more on the human translation aspect have conceived in order to meet this problem.,User evaluation measures,Usability scoring  Post-editing time Fluency and adequacy have been defined using a five point scale [8].,User evaluation measures,
Usability scoring  Post-editing time Fluency and adequacy have been defined using a five point scale [8].,1378201,Domain specific {MT} in use,2008,User evaluation measures,The following metrics represent this alternative evaluation approach:  Fluency and adequacy scoring ,User evaluation measures,"Recent studies show that scores for fluency and adequacy apparently do not correlate very well between users 1 , and therefore these score results would be more difficult to use for system testing and tuning.",User evaluation measures,
"Recent studies show that scores for fluency and adequacy apparently do not correlate very well between users 1 , and therefore these score results would be more difficult to use for system testing and tuning.",1378202,Domain specific {MT} in use,2008,User evaluation measures,Usability scoring  Post-editing time Fluency and adequacy have been defined using a five point scale [8].,User evaluation measures,The LSP post-editors involved in evaluating SMT output stated that a five point scale would be much too difficult to use.,User evaluation measures,
The LSP post-editors involved in evaluating SMT output stated that a five point scale would be much too difficult to use.,1378203,Domain specific {MT} in use,2008,User evaluation measures,"Recent studies show that scores for fluency and adequacy apparently do not correlate very well between users 1 , and therefore these score results would be more difficult to use for system testing and tuning.",User evaluation measures,We therefore reduced the scale to a four point scale which gave the users an easier job and thereby probably more reliable results. ,User evaluation measures,
We therefore reduced the scale to a four point scale which gave the users an easier job and thereby probably more reliable results. ,1378204,Domain specific {MT} in use,2008,User evaluation measures,The LSP post-editors involved in evaluating SMT output stated that a five point scale would be much too difficult to use.,User evaluation measures,The measure that the users suggested themselves is here called Usability.,User evaluation measures,
The measure that the users suggested themselves is here called Usability.,1378205,Domain specific {MT} in use,2008,User evaluation measures,We therefore reduced the scale to a four point scale which gave the users an easier job and thereby probably more reliable results. ,User evaluation measures,"In an LSP environment, the conventional translation platform is a TM platform.",User evaluation measures,
"In an LSP environment, the conventional translation platform is a TM platform.",1378206,Domain specific {MT} in use,2008,User evaluation measures,The measure that the users suggested themselves is here called Usability.,User evaluation measures,'Usability' is a measure that allows post-editors to score a machine-translated translation unit in terms of usability compared to a fuzzy match in a TM tool.,User evaluation measures,
'Usability' is a measure that allows post-editors to score a machine-translated translation unit in terms of usability compared to a fuzzy match in a TM tool.,1378207,Domain specific {MT} in use,2008,User evaluation measures,"In an LSP environment, the conventional translation platform is a TM platform.",User evaluation measures,"A machine-translated translation unit may not be adequate or fluent, but it may be usable.",User evaluation measures,
"A machine-translated translation unit may not be adequate or fluent, but it may be usable.",1378208,Domain specific {MT} in use,2008,User evaluation measures,'Usability' is a measure that allows post-editors to score a machine-translated translation unit in terms of usability compared to a fuzzy match in a TM tool.,User evaluation measures,"When it is usable, the time needed to edit the machine-translated translation unit will be shorter than the time needed to translate the segment from scratch.",User evaluation measures,
"When it is usable, the time needed to edit the machine-translated translation unit will be shorter than the time needed to translate the segment from scratch.",1378209,Domain specific {MT} in use,2008,User evaluation measures,"A machine-translated translation unit may not be adequate or fluent, but it may be usable.",User evaluation measures,It is in this context defined as a three point scale.,User evaluation measures,
It is in this context defined as a three point scale.,1378210,Domain specific {MT} in use,2008,User evaluation measures,"When it is usable, the time needed to edit the machine-translated translation unit will be shorter than the time needed to translate the segment from scratch.",User evaluation measures,"The scale is focused on the post-editing process, and the user can use the following scores: 3: Good translation -few key strokes needed to edit translation.",User evaluation measures,
"The scale is focused on the post-editing process, and the user can use the following scores: 3: Good translation -few key strokes needed to edit translation.",1378211,Domain specific {MT} in use,2008,User evaluation measures,It is in this context defined as a three point scale.,User evaluation measures,Corrections of casing or layout can be needed.,User evaluation measures,
Corrections of casing or layout can be needed.,1378212,Domain specific {MT} in use,2008,User evaluation measures,"The scale is focused on the post-editing process, and the user can use the following scores: 3: Good translation -few key strokes needed to edit translation.",User evaluation measures,Use of terminology is correct. ,User evaluation measures,
Use of terminology is correct. ,1378213,Domain specific {MT} in use,2008,User evaluation measures,Corrections of casing or layout can be needed.,User evaluation measures,2: Translation can be post-edited using less time than a translation of the sentence from scratch -number of key strokes needed to edit translation is less than the key strokes needed to translate from scratch. ,User evaluation measures,
2: Translation can be post-edited using less time than a translation of the sentence from scratch -number of key strokes needed to edit translation is less than the key strokes needed to translate from scratch. ,1378214,Domain specific {MT} in use,2008,User evaluation measures,Use of terminology is correct. ,User evaluation measures,1: Translation quality is too poor.,User evaluation measures,
1: Translation quality is too poor.,1378215,Domain specific {MT} in use,2008,User evaluation measures,2: Translation can be post-edited using less time than a translation of the sentence from scratch -number of key strokes needed to edit translation is less than the key strokes needed to translate from scratch. ,User evaluation measures,"It will take more time to post-edit the sentence, than to translate the sentence from the source sentence -translation is discarded. ",User evaluation measures,
"It will take more time to post-edit the sentence, than to translate the sentence from the source sentence -translation is discarded. ",1378216,Domain specific {MT} in use,2008,User evaluation measures,1: Translation quality is too poor.,User evaluation measures,"The post-editors at the LSP found this scoring very useful as it is closely connected to their translation workflow, no matter whether they use TM as their translation tool or post-edit MT-output. ",User evaluation measures,
"The post-editors at the LSP found this scoring very useful as it is closely connected to their translation workflow, no matter whether they use TM as their translation tool or post-edit MT-output. ",1378217,Domain specific {MT} in use,2008,User evaluation measures,"It will take more time to post-edit the sentence, than to translate the sentence from the source sentence -translation is discarded. ",User evaluation measures,"The judgement tool for usability, adequacy and fluency shows the source sentence and the translation output.",User evaluation measures,
"The judgement tool for usability, adequacy and fluency shows the source sentence and the translation output.",1378218,Domain specific {MT} in use,2008,User evaluation measures,"The post-editors at the LSP found this scoring very useful as it is closely connected to their translation workflow, no matter whether they use TM as their translation tool or post-edit MT-output. ",User evaluation measures,The time used to do the scoring of all three measures is also measured.,User evaluation measures,
The time used to do the scoring of all three measures is also measured.,1378219,Domain specific {MT} in use,2008,User evaluation measures,"The judgement tool for usability, adequacy and fluency shows the source sentence and the translation output.",User evaluation measures,The goal of doing user evaluation is first of all to give the users tools to evaluate the SMT output in order to give feedback to system development and secondly to compare the judgements with the figures from automatic evaluation.,User evaluation measures,
The goal of doing user evaluation is first of all to give the users tools to evaluate the SMT output in order to give feedback to system development and secondly to compare the judgements with the figures from automatic evaluation.,1378220,Domain specific {MT} in use,2008,User evaluation measures,The time used to do the scoring of all three measures is also measured.,User evaluation measures,"As human evaluation is costly, we present the results we got in all the evaluation tasks although the amount of data is small.",User evaluation measures,
"As human evaluation is costly, we present the results we got in all the evaluation tasks although the amount of data is small.",1378221,Domain specific {MT} in use,2008,User evaluation measures,The goal of doing user evaluation is first of all to give the users tools to evaluate the SMT output in order to give feedback to system development and secondly to compare the judgements with the figures from automatic evaluation.,User evaluation measures,The test involved three in-house post-editors.,User evaluation measures,
The test involved three in-house post-editors.,1378222,Domain specific {MT} in use,2008,User evaluation measures,"As human evaluation is costly, we present the results we got in all the evaluation tasks although the amount of data is small.",User evaluation measures,All of them were experienced proof-readers.,User evaluation measures,
All of them were experienced proof-readers.,1378223,Domain specific {MT} in use,2008,User evaluation measures,The test involved three in-house post-editors.,User evaluation measures,Two of them were domain specialists.,User evaluation measures,
Two of them were domain specialists.,1378224,Domain specific {MT} in use,2008,User evaluation measures,All of them were experienced proof-readers.,User evaluation measures,One post-editor was a general quality assurance specialist. ,User evaluation measures,
One post-editor was a general quality assurance specialist. ,1378225,Domain specific {MT} in use,2008,User evaluation measures,Two of them were domain specialists.,User evaluation measures,"In table 3 the average scores for usability, adequacy and fluency together with the BLEU and TER scores are given.",User evaluation measures,
"In table 3 the average scores for usability, adequacy and fluency together with the BLEU and TER scores are given.",1378226,Domain specific {MT} in use,2008,User evaluation measures,One post-editor was a general quality assurance specialist. ,User evaluation measures,The BLEU and TER scores are not as good for the test sets as for the development test set.,User evaluation measures,
The BLEU and TER scores are not as good for the test sets as for the development test set.,1378227,Domain specific {MT} in use,2008,User evaluation measures,"In table 3 the average scores for usability, adequacy and fluency together with the BLEU and TER scores are given.",User evaluation measures,We also split up the fluency and adequacy scores in relation to the usability scores (Table 4).,User evaluation measures,
We also split up the fluency and adequacy scores in relation to the usability scores (Table 4).,1378228,Domain specific {MT} in use,2008,User evaluation measures,The BLEU and TER scores are not as good for the test sets as for the development test set.,User evaluation measures,A clear correlation between the fluency/adequacy scores and the usability scores can be seen.,User evaluation measures,
A clear correlation between the fluency/adequacy scores and the usability scores can be seen.,1378229,Domain specific {MT} in use,2008,User evaluation measures,We also split up the fluency and adequacy scores in relation to the usability scores (Table 4).,User evaluation measures,"The users' feedback on the fluency and adequacy scoring was that they would prefer only to use the usability score, as it is a simple measure.",User evaluation measures,
"The users' feedback on the fluency and adequacy scoring was that they would prefer only to use the usability score, as it is a simple measure.",1378230,Domain specific {MT} in use,2008,User evaluation measures,A clear correlation between the fluency/adequacy scores and the usability scores can be seen.,User evaluation measures,"In addition, it would be easier for the post-editor to do this kind of judgement because this is the way that they normally conduct translation quality assessments.",User evaluation measures,
"In addition, it would be easier for the post-editor to do this kind of judgement because this is the way that they normally conduct translation quality assessments.",1378231,Domain specific {MT} in use,2008,User evaluation measures,"The users' feedback on the fluency and adequacy scoring was that they would prefer only to use the usability score, as it is a simple measure.",User evaluation measures,"They also stressed that it is important that the scoring can be done fast for each sentence, preferably less than 30 sec.",User evaluation measures,
"They also stressed that it is important that the scoring can be done fast for each sentence, preferably less than 30 sec.",1378232,Domain specific {MT} in use,2008,User evaluation measures,"In addition, it would be easier for the post-editor to do this kind of judgement because this is the way that they normally conduct translation quality assessments.",User evaluation measures,on average.,User evaluation measures,
on average.,1378233,Domain specific {MT} in use,2008,User evaluation measures,"They also stressed that it is important that the scoring can be done fast for each sentence, preferably less than 30 sec.",User evaluation measures,As can be seen the scores are done in 14 sec.,User evaluation measures,
As can be seen the scores are done in 14 sec.,1378234,Domain specific {MT} in use,2008,User evaluation measures,on average.,User evaluation measures,"on average for sentences with usability=1, and in 20 sec.",User evaluation measures,
"on average for sentences with usability=1, and in 20 sec.",1378235,Domain specific {MT} in use,2008,User evaluation measures,As can be seen the scores are done in 14 sec.,User evaluation measures,for usability=3. ,User evaluation measures,
for usability=3. ,1378236,Domain specific {MT} in use,2008,User evaluation measures,"on average for sentences with usability=1, and in 20 sec.",User evaluation measures,"To compare the user evaluation scorings with the scorings done by automatic metrics, we focus on the TER metric.",User evaluation measures,
"To compare the user evaluation scorings with the scorings done by automatic metrics, we focus on the TER metric.",1378237,Domain specific {MT} in use,2008,User evaluation measures,for usability=3. ,User evaluation measures,The relation between the TER scoring and the usability score are given in Table 5.,User evaluation measures,
The relation between the TER scoring and the usability score are given in Table 5.,1378238,Domain specific {MT} in use,2008,User evaluation measures,"To compare the user evaluation scorings with the scorings done by automatic metrics, we focus on the TER metric.",User evaluation measures,It can be seen that when considering individual sentences there are significant noise on the TER scoring because the standard deviation is approximately as large as the distance between the usability classes.,User evaluation measures,
It can be seen that when considering individual sentences there are significant noise on the TER scoring because the standard deviation is approximately as large as the distance between the usability classes.,1378239,Domain specific {MT} in use,2008,User evaluation measures,The relation between the TER scoring and the usability score are given in Table 5.,User evaluation measures,"However, when considering a text with n sentences the noise on the overall TER scoring decreases by a factor n -½ .",User evaluation measures,
"However, when considering a text with n sentences the noise on the overall TER scoring decreases by a factor n -½ .",1378240,Domain specific {MT} in use,2008,User evaluation measures,It can be seen that when considering individual sentences there are significant noise on the TER scoring because the standard deviation is approximately as large as the distance between the usability classes.,User evaluation measures,"Hence, if we have 100 sentences the standard deviation on the overall TER scoring corresponds to approximately 0.1 usability units.",User evaluation measures,
"Hence, if we have 100 sentences the standard deviation on the overall TER scoring corresponds to approximately 0.1 usability units.",1378241,Domain specific {MT} in use,2008,User evaluation measures,"However, when considering a text with n sentences the noise on the overall TER scoring decreases by a factor n -½ .",User evaluation measures,Therefore we consider TER a promising candidate for an automatic metric for simulating the post-editors' judgements on text level.,User evaluation measures,
Therefore we consider TER a promising candidate for an automatic metric for simulating the post-editors' judgements on text level.,1378242,Domain specific {MT} in use,2008,User evaluation measures,"Hence, if we have 100 sentences the standard deviation on the overall TER scoring corresponds to approximately 0.1 usability units.",User evaluation measures,"Feedback from the post-editors 12th EAMT conference, 22-23 September 2008, Hamburg, Germany also mention that a very good automatic metric has to take into account that words from some word classes are more important to translate correctly than others.",User evaluation measures,
"Feedback from the post-editors 12th EAMT conference, 22-23 September 2008, Hamburg, Germany also mention that a very good automatic metric has to take into account that words from some word classes are more important to translate correctly than others.",1378243,Domain specific {MT} in use,2008,User evaluation measures,Therefore we consider TER a promising candidate for an automatic metric for simulating the post-editors' judgements on text level.,User evaluation measures,"Shortly after the LSP had joined the project, the LSP received a commercial order for a translation project involving machine translation.",Post-editing Experiences in a 'Real-life' Translation Project,
"Shortly after the LSP had joined the project, the LSP received a commercial order for a translation project involving machine translation.",1378244,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,"Feedback from the post-editors 12th EAMT conference, 22-23 September 2008, Hamburg, Germany also mention that a very good automatic metric has to take into account that words from some word classes are more important to translate correctly than others.",User evaluation measures,It was a Microsoft (sub-domain B) project in which the LSP's job was to post-edit already machine-translated text strings instead of translating text in a TM environment. ,Post-editing Experiences in a 'Real-life' Translation Project,
It was a Microsoft (sub-domain B) project in which the LSP's job was to post-edit already machine-translated text strings instead of translating text in a TM environment. ,1378245,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,"Shortly after the LSP had joined the project, the LSP received a commercial order for a translation project involving machine translation.",Post-editing Experiences in a 'Real-life' Translation Project,Post-editing was a more complex job than they had expected.,Post-editing Experiences in a 'Real-life' Translation Project,
Post-editing was a more complex job than they had expected.,1378246,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,It was a Microsoft (sub-domain B) project in which the LSP's job was to post-edit already machine-translated text strings instead of translating text in a TM environment. ,Post-editing Experiences in a 'Real-life' Translation Project,"In many ways, postediting is comparable to proof-reading human translations, however in some ways it differs.",Post-editing Experiences in a 'Real-life' Translation Project,
"In many ways, postediting is comparable to proof-reading human translations, however in some ways it differs.",1378247,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,Post-editing was a more complex job than they had expected.,Post-editing Experiences in a 'Real-life' Translation Project,The LSP set out by hiring experienced Microsoft translators to do the postediting.,Post-editing Experiences in a 'Real-life' Translation Project,
The LSP set out by hiring experienced Microsoft translators to do the postediting.,1378248,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,"In many ways, postediting is comparable to proof-reading human translations, however in some ways it differs.",Post-editing Experiences in a 'Real-life' Translation Project,"Unexpectedly, when delivering the first post-editing assignment, it did not pass Microsoft's validation process so they learned that good translators are not necessarily good post-editors. ",Post-editing Experiences in a 'Real-life' Translation Project,
"Unexpectedly, when delivering the first post-editing assignment, it did not pass Microsoft's validation process so they learned that good translators are not necessarily good post-editors. ",1378249,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,The LSP set out by hiring experienced Microsoft translators to do the postediting.,Post-editing Experiences in a 'Real-life' Translation Project,Another aspect of the post-editing project was that rates were lower than the rates of ordinary translation projects.,Post-editing Experiences in a 'Real-life' Translation Project,
Another aspect of the post-editing project was that rates were lower than the rates of ordinary translation projects.,1378250,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,"Unexpectedly, when delivering the first post-editing assignment, it did not pass Microsoft's validation process so they learned that good translators are not necessarily good post-editors. ",Post-editing Experiences in a 'Real-life' Translation Project,Low rates are based on the assumption that there is a productivity gain when using machine translation compared to human translation in a TM environment. ,Post-editing Experiences in a 'Real-life' Translation Project,
Low rates are based on the assumption that there is a productivity gain when using machine translation compared to human translation in a TM environment. ,1378251,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,Another aspect of the post-editing project was that rates were lower than the rates of ordinary translation projects.,Post-editing Experiences in a 'Real-life' Translation Project,Based on the lessons learned from this Microsoft project it can be concluded that good post-editing skills differ from good translation skills so other vendor profiles have to be chosen and secondly that low rates on post-editing require that less time can be spent on translation.,Post-editing Experiences in a 'Real-life' Translation Project,
Based on the lessons learned from this Microsoft project it can be concluded that good post-editing skills differ from good translation skills so other vendor profiles have to be chosen and secondly that low rates on post-editing require that less time can be spent on translation.,1378252,Domain specific {MT} in use,2008,Post-editing Experiences in a 'Real-life' Translation Project,Low rates are based on the assumption that there is a productivity gain when using machine translation compared to human translation in a TM environment. ,Post-editing Experiences in a 'Real-life' Translation Project,What is the ideal post-editor profile then?,Post-editor Profile,
What is the ideal post-editor profile then?,1378253,Domain specific {MT} in use,2008,Post-editor Profile,Based on the lessons learned from this Microsoft project it can be concluded that good post-editing skills differ from good translation skills so other vendor profiles have to be chosen and secondly that low rates on post-editing require that less time can be spent on translation.,Post-editing Experiences in a 'Real-life' Translation Project,"First of all, the ideal post-editor has to fulfil the requirements stated by clients in the localization industry.",Post-editor Profile,
"First of all, the ideal post-editor has to fulfil the requirements stated by clients in the localization industry.",1378254,Domain specific {MT} in use,2008,Post-editor Profile,What is the ideal post-editor profile then?,Post-editor Profile,These requirements include:  The use of consistent terminology  Continuity with existing translations => existing translated text strings cannot be altered  Compliance with client's style guides ,Post-editor Profile,
These requirements include:  The use of consistent terminology  Continuity with existing translations => existing translated text strings cannot be altered  Compliance with client's style guides ,1378255,Domain specific {MT} in use,2008,Post-editor Profile,"First of all, the ideal post-editor has to fulfil the requirements stated by clients in the localization industry.",Post-editor Profile,Ability to observe deadlines ,Post-editor Profile,
Ability to observe deadlines ,1378256,Domain specific {MT} in use,2008,Post-editor Profile,These requirements include:  The use of consistent terminology  Continuity with existing translations => existing translated text strings cannot be altered  Compliance with client's style guides ,Post-editor Profile,"Ability to adapt level of quality to price pressure restraints In our experience, this is the ideal post-editor profile: ",Post-editor Profile,
"Ability to adapt level of quality to price pressure restraints In our experience, this is the ideal post-editor profile: ",1378257,Domain specific {MT} in use,2008,Post-editor Profile,Ability to observe deadlines ,Post-editor Profile,A good post-editor knows the domain ,Post-editor Profile,
A good post-editor knows the domain ,1378258,Domain specific {MT} in use,2008,Post-editor Profile,"Ability to adapt level of quality to price pressure restraints In our experience, this is the ideal post-editor profile: ",Post-editor Profile,A good post-editor has very good skills in his/her native language ,Post-editor Profile,
A good post-editor has very good skills in his/her native language ,1378259,Domain specific {MT} in use,2008,Post-editor Profile,A good post-editor knows the domain ,Post-editor Profile,A good post-editor can implement style guides and consistent terminology.,Post-editor Profile,
A good post-editor can implement style guides and consistent terminology.,1378260,Domain specific {MT} in use,2008,Post-editor Profile,A good post-editor has very good skills in his/her native language ,Post-editor Profile,A good post-editor is an experienced proof-reader.,Post-editor Profile,
A good post-editor is an experienced proof-reader.,1378261,Domain specific {MT} in use,2008,Post-editor Profile,A good post-editor can implement style guides and consistent terminology.,Post-editor Profile,"He/she can make swift decisions on ""good -no good"" in order to be able to discard machinetranslated text strings that are not worthwhile post-editing, but need to be translated from scratch.",Post-editor Profile,
"He/she can make swift decisions on ""good -no good"" in order to be able to discard machinetranslated text strings that are not worthwhile post-editing, but need to be translated from scratch.",1378262,Domain specific {MT} in use,2008,Post-editor Profile,A good post-editor is an experienced proof-reader.,Post-editor Profile,"In many ways, the profile of a good post-editor fits a good translator.",Post-editor Profile,
"In many ways, the profile of a good post-editor fits a good translator.",1378263,Domain specific {MT} in use,2008,Post-editor Profile,"He/she can make swift decisions on ""good -no good"" in order to be able to discard machinetranslated text strings that are not worthwhile post-editing, but need to be translated from scratch.",Post-editor Profile,"However, there is one significant and very important difference: the ability to decide -within a few seconds -whether a translation should be discarded.",Post-editor Profile,
"However, there is one significant and very important difference: the ability to decide -within a few seconds -whether a translation should be discarded.",1378264,Domain specific {MT} in use,2008,Post-editor Profile,"In many ways, the profile of a good post-editor fits a good translator.",Post-editor Profile,Many translators tend to spend too much time on this decision process.,Post-editor Profile,
Many translators tend to spend too much time on this decision process.,1378265,Domain specific {MT} in use,2008,Post-editor Profile,"However, there is one significant and very important difference: the ability to decide -within a few seconds -whether a translation should be discarded.",Post-editor Profile,"What really matters for the users is the amount of time spent on post-editing the output, compared with the amount of time spent on translation with the tools they are used to in the translation process.",Comparing SMT with TMs,
"What really matters for the users is the amount of time spent on post-editing the output, compared with the amount of time spent on translation with the tools they are used to in the translation process.",1378266,Domain specific {MT} in use,2008,Comparing SMT with TMs,Many translators tend to spend too much time on this decision process.,Post-editor Profile,"As an LSP, the TM environment is the standard translation platform.",Comparing SMT with TMs,
"As an LSP, the TM environment is the standard translation platform.",1378267,Domain specific {MT} in use,2008,Comparing SMT with TMs,"What really matters for the users is the amount of time spent on post-editing the output, compared with the amount of time spent on translation with the tools they are used to in the translation process.",Comparing SMT with TMs,"Consequently, one of Inter-Set's goals in the project was to compare SMT with TM.",Comparing SMT with TMs,
"Consequently, one of Inter-Set's goals in the project was to compare SMT with TM.",1378268,Domain specific {MT} in use,2008,Comparing SMT with TMs,"As an LSP, the TM environment is the standard translation platform.",Comparing SMT with TMs,"Also, based on the low post-editing rates compared to standard translation rates, it was -from a financial point of view -interesting to see whether using SMT did actually produce a productivity gain compared to the traditional TM environment. ",Comparing SMT with TMs,
"Also, based on the low post-editing rates compared to standard translation rates, it was -from a financial point of view -interesting to see whether using SMT did actually produce a productivity gain compared to the traditional TM environment. ",1378269,Domain specific {MT} in use,2008,Comparing SMT with TMs,"Consequently, one of Inter-Set's goals in the project was to compare SMT with TM.",Comparing SMT with TMs,"In the domain of mobile phones (sub-domain E), a TM translation was compared to an SMT translation.",Comparing SMT with TMs,
"In the domain of mobile phones (sub-domain E), a TM translation was compared to an SMT translation.",1378270,Domain specific {MT} in use,2008,Comparing SMT with TMs,"Also, based on the low post-editing rates compared to standard translation rates, it was -from a financial point of view -interesting to see whether using SMT did actually produce a productivity gain compared to the traditional TM environment. ",Comparing SMT with TMs,First the text to be translated in the TM tool was analysed.,Comparing SMT with TMs,
First the text to be translated in the TM tool was analysed.,1378271,Domain specific {MT} in use,2008,Comparing SMT with TMs,"In the domain of mobile phones (sub-domain E), a TM translation was compared to an SMT translation.",Comparing SMT with TMs,The result of the analysis can be seen in table 6.,Comparing SMT with TMs,
The result of the analysis can be seen in table 6.,1378272,Domain specific {MT} in use,2008,Comparing SMT with TMs,First the text to be translated in the TM tool was analysed.,Comparing SMT with TMs,Then it was estimated how much time would be needed to translate the text in a TM environment. ,Comparing SMT with TMs,
Then it was estimated how much time would be needed to translate the text in a TM environment. ,1378273,Domain specific {MT} in use,2008,Comparing SMT with TMs,The result of the analysis can be seen in table 6.,Comparing SMT with TMs,"It turned out that translating the remaining 1,749 words in a TM environment took 6 hours, while the translation task using SMT took 2 hours giving a productivity gain of 67%.",Comparing SMT with TMs,
"It turned out that translating the remaining 1,749 words in a TM environment took 6 hours, while the translation task using SMT took 2 hours giving a productivity gain of 67%.",1378274,Domain specific {MT} in use,2008,Comparing SMT with TMs,Then it was estimated how much time would be needed to translate the text in a TM environment. ,Comparing SMT with TMs,"Even though the amount of test data is sparse, this investigation indicates that a productivity gain can be used shifting from a TM environment to an SMT system. ",Comparing SMT with TMs,
"Even though the amount of test data is sparse, this investigation indicates that a productivity gain can be used shifting from a TM environment to an SMT system. ",1378275,Domain specific {MT} in use,2008,Comparing SMT with TMs,"It turned out that translating the remaining 1,749 words in a TM environment took 6 hours, while the translation task using SMT took 2 hours giving a productivity gain of 67%.",Comparing SMT with TMs,We also investigated the connection between the TM match percentages and the Usability scores of the two test documents.,Comparing SMT with TMs,
We also investigated the connection between the TM match percentages and the Usability scores of the two test documents.,1378276,Domain specific {MT} in use,2008,Comparing SMT with TMs,"Even though the amount of test data is sparse, this investigation indicates that a productivity gain can be used shifting from a TM environment to an SMT system. ",Comparing SMT with TMs,"Table 7 shows that in the two tests 16% (A) and 8% (B) have a match of 95%-100% in the TM, but a larger percentage of the sentences get the score usability=3",Comparing SMT with TMs,
"Table 7 shows that in the two tests 16% (A) and 8% (B) have a match of 95%-100% in the TM, but a larger percentage of the sentences get the score usability=3",1378277,Domain specific {MT} in use,2008,Comparing SMT with TMs,We also investigated the connection between the TM match percentages and the Usability scores of the two test documents.,Comparing SMT with TMs,(Good translation): 17% and 20%.,Comparing SMT with TMs,
(Good translation): 17% and 20%.,1378278,Domain specific {MT} in use,2008,Comparing SMT with TMs,"Table 7 shows that in the two tests 16% (A) and 8% (B) have a match of 95%-100% in the TM, but a larger percentage of the sentences get the score usability=3",Comparing SMT with TMs,"It can also be seen that 67% and 81% of the sentences have ""No match"" in the TMs.",Comparing SMT with TMs,
"It can also be seen that 67% and 81% of the sentences have ""No match"" in the TMs.",1378279,Domain specific {MT} in use,2008,Comparing SMT with TMs,(Good translation): 17% and 20%.,Comparing SMT with TMs,Compared with the Usability scores only 23% and 19% of the sentences get the score Usablity=1.,Comparing SMT with TMs,
Compared with the Usability scores only 23% and 19% of the sentences get the score Usablity=1.,1378280,Domain specific {MT} in use,2008,Comparing SMT with TMs,"It can also be seen that 67% and 81% of the sentences have ""No match"" in the TMs.",Comparing SMT with TMs,This also indicates that using SMT for this text type could be beneficial.,Comparing SMT with TMs,
This also indicates that using SMT for this text type could be beneficial.,1378281,Domain specific {MT} in use,2008,Comparing SMT with TMs,Compared with the Usability scores only 23% and 19% of the sentences get the score Usablity=1.,Comparing SMT with TMs,"The use of sub-domain phrase tables addresses the point that the LSP would like to gain as much as possible from the client/text type specific training data, but on the other hand also would like to have a broad coverage of words and phrases.",Use of Sub-domain Phrase Tables,
"The use of sub-domain phrase tables addresses the point that the LSP would like to gain as much as possible from the client/text type specific training data, but on the other hand also would like to have a broad coverage of words and phrases.",1378282,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,This also indicates that using SMT for this text type could be beneficial.,Comparing SMT with TMs,Training an SMT system on a small amount of training material for a given sub-domain leads to a narrow lexical coverage which again results in low translation quality.,Use of Sub-domain Phrase Tables,
Training an SMT system on a small amount of training material for a given sub-domain leads to a narrow lexical coverage which again results in low translation quality.,1378283,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,"The use of sub-domain phrase tables addresses the point that the LSP would like to gain as much as possible from the client/text type specific training data, but on the other hand also would like to have a broad coverage of words and phrases.",Use of Sub-domain Phrase Tables,In order to cope with this problem and to get a better system performance a number of phrase tables were combined.,Use of Sub-domain Phrase Tables,
In order to cope with this problem and to get a better system performance a number of phrase tables were combined.,1378284,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,Training an SMT system on a small amount of training material for a given sub-domain leads to a narrow lexical coverage which again results in low translation quality.,Use of Sub-domain Phrase Tables,"For the training material available (Table 1) four out of five sub-domain corpora are very small (less than 300,000 words for each sub-domain). ",Use of Sub-domain Phrase Tables,
"For the training material available (Table 1) four out of five sub-domain corpora are very small (less than 300,000 words for each sub-domain). ",1378285,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,In order to cope with this problem and to get a better system performance a number of phrase tables were combined.,Use of Sub-domain Phrase Tables,As expected we got poor results when translating the development test set based only on these small sub-corpora.,Use of Sub-domain Phrase Tables,
As expected we got poor results when translating the development test set based only on these small sub-corpora.,1378286,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,"For the training material available (Table 1) four out of five sub-domain corpora are very small (less than 300,000 words for each sub-domain). ",Use of Sub-domain Phrase Tables,Consequently we decided to combine the sub-domain phrase tables with the phrase table based on all the five sub-domain corpora.,Use of Sub-domain Phrase Tables,
Consequently we decided to combine the sub-domain phrase tables with the phrase table based on all the five sub-domain corpora.,1378287,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,As expected we got poor results when translating the development test set based only on these small sub-corpora.,Use of Sub-domain Phrase Tables,This combination can be done using the MOSES decoder as MOSES has an option allowing more phrase tables to be used in the same translation process.,Use of Sub-domain Phrase Tables,
This combination can be done using the MOSES decoder as MOSES has an option allowing more phrase tables to be used in the same translation process.,1378288,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,Consequently we decided to combine the sub-domain phrase tables with the phrase table based on all the five sub-domain corpora.,Use of Sub-domain Phrase Tables,In this test the sub-domain specific translation table is given a weight of 0.5 while the general phrase table is given a weight of 0.2.,Use of Sub-domain Phrase Tables,
In this test the sub-domain specific translation table is given a weight of 0.5 while the general phrase table is given a weight of 0.2.,1378289,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,This combination can be done using the MOSES decoder as MOSES has an option allowing more phrase tables to be used in the same translation process.,Use of Sub-domain Phrase Tables,The results of translating the five sub-domain test corpora compared with the results using only one phrase table is shown in Table 8.,Use of Sub-domain Phrase Tables,
The results of translating the five sub-domain test corpora compared with the results using only one phrase table is shown in Table 8.,1378290,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,In this test the sub-domain specific translation table is given a weight of 0.5 while the general phrase table is given a weight of 0.2.,Use of Sub-domain Phrase Tables,"It can be seen that for the sub-domains A, B, C and E both the BLEU and the TER scores are significantly better yielding an increase of 2.2 to 3.7 BLEU points and of 1.8 to 2.8 TER points.",Use of Sub-domain Phrase Tables,
"It can be seen that for the sub-domains A, B, C and E both the BLEU and the TER scores are significantly better yielding an increase of 2.2 to 3.7 BLEU points and of 1.8 to 2.8 TER points.",1378291,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,The results of translating the five sub-domain test corpora compared with the results using only one phrase table is shown in Table 8.,Use of Sub-domain Phrase Tables,We find it likely that further improvement can be obtained by such a type of optimization and regard it as a promising strategy for at the same time to give sub-domain training material priority and obtain a broader lexical coverage.,Use of Sub-domain Phrase Tables,
We find it likely that further improvement can be obtained by such a type of optimization and regard it as a promising strategy for at the same time to give sub-domain training material priority and obtain a broader lexical coverage.,1378292,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,"It can be seen that for the sub-domains A, B, C and E both the BLEU and the TER scores are significantly better yielding an increase of 2.2 to 3.7 BLEU points and of 1.8 to 2.8 TER points.",Use of Sub-domain Phrase Tables,In a study done by [12] test of domain corpus adaptation for broader domain coverage is reported with an increase of 1.7 BLEU points.,Use of Sub-domain Phrase Tables,
In a study done by [12] test of domain corpus adaptation for broader domain coverage is reported with an increase of 1.7 BLEU points.,1378293,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,We find it likely that further improvement can be obtained by such a type of optimization and regard it as a promising strategy for at the same time to give sub-domain training material priority and obtain a broader lexical coverage.,Use of Sub-domain Phrase Tables,"Combinations of domain corpora with corpora covering general language are under further investigation in collaboration with the LSP, including weighting of the phrase tables that favours the use of the domain terminology rather than the general vocabulary.",Use of Sub-domain Phrase Tables,
"Combinations of domain corpora with corpora covering general language are under further investigation in collaboration with the LSP, including weighting of the phrase tables that favours the use of the domain terminology rather than the general vocabulary.",1378294,Domain specific {MT} in use,2008,Use of Sub-domain Phrase Tables,In a study done by [12] test of domain corpus adaptation for broader domain coverage is reported with an increase of 1.7 BLEU points.,Use of Sub-domain Phrase Tables,We have described the post-editor profile based on experiences conducted in connection with the introduction of SMT in the translation workflow of an LSP.,Conclusion,
We have described the post-editor profile based on experiences conducted in connection with the introduction of SMT in the translation workflow of an LSP.,1378295,Domain specific {MT} in use,2008,Conclusion,"Combinations of domain corpora with corpora covering general language are under further investigation in collaboration with the LSP, including weighting of the phrase tables that favours the use of the domain terminology rather than the general vocabulary.",Use of Sub-domain Phrase Tables,"The LSP post-editors preferred the evaluation metric 'Usability' rather than fluency/adequacy, as it is more closely related to their every-day workflow when evaluating translations.",Conclusion,
"The LSP post-editors preferred the evaluation metric 'Usability' rather than fluency/adequacy, as it is more closely related to their every-day workflow when evaluating translations.",1378296,Domain specific {MT} in use,2008,Conclusion,We have described the post-editor profile based on experiences conducted in connection with the introduction of SMT in the translation workflow of an LSP.,Conclusion,"The automatic metric TER was tested as a candidate for an automatic metric with correlation to human judgement, and the TER appeared to be a promising candidate. ",Conclusion,
"The automatic metric TER was tested as a candidate for an automatic metric with correlation to human judgement, and the TER appeared to be a promising candidate. ",1378297,Domain specific {MT} in use,2008,Conclusion,"The LSP post-editors preferred the evaluation metric 'Usability' rather than fluency/adequacy, as it is more closely related to their every-day workflow when evaluating translations.",Conclusion,"With a productivity gain of 67 % saved time in post-editing in the test of SMT, the obvious next step for the LSP would be to integrate SMT in their workflow for some domains.",Conclusion,
"With a productivity gain of 67 % saved time in post-editing in the test of SMT, the obvious next step for the LSP would be to integrate SMT in their workflow for some domains.",1378298,Domain specific {MT} in use,2008,Conclusion,"The automatic metric TER was tested as a candidate for an automatic metric with correlation to human judgement, and the TER appeared to be a promising candidate. ",Conclusion,Gaining benefits of machine translation however depends on the data that are available and used for the training of the SMT engine. ,Conclusion,
Gaining benefits of machine translation however depends on the data that are available and used for the training of the SMT engine. ,1378299,Domain specific {MT} in use,2008,Conclusion,"With a productivity gain of 67 % saved time in post-editing in the test of SMT, the obvious next step for the LSP would be to integrate SMT in their workflow for some domains.",Conclusion,"TMs may be client-specific including all text types, from user manuals to meeting notes.",Conclusion,
"TMs may be client-specific including all text types, from user manuals to meeting notes.",1378300,Domain specific {MT} in use,2008,Conclusion,Gaining benefits of machine translation however depends on the data that are available and used for the training of the SMT engine. ,Conclusion,"In these TMs, terminology may be consistent, but sentence structures may differ widely.",Conclusion,
"In these TMs, terminology may be consistent, but sentence structures may differ widely.",1378301,Domain specific {MT} in use,2008,Conclusion,"TMs may be client-specific including all text types, from user manuals to meeting notes.",Conclusion,"For an LSP with many clients and many rather small client-specific TMs (each containing a few hundreds of thousands of words), the target of millions of words for the training of an SMT engine is simply out of reach.",Conclusion,
"For an LSP with many clients and many rather small client-specific TMs (each containing a few hundreds of thousands of words), the target of millions of words for the training of an SMT engine is simply out of reach.",1378302,Domain specific {MT} in use,2008,Conclusion,"In these TMs, terminology may be consistent, but sentence structures may differ widely.",Conclusion,Here the combination of weighted phrase tables looks as a promising strategy to overcome this threshold.,Conclusion,
Here the combination of weighted phrase tables looks as a promising strategy to overcome this threshold.,1378303,Domain specific {MT} in use,2008,Conclusion,"For an LSP with many clients and many rather small client-specific TMs (each containing a few hundreds of thousands of words), the target of millions of words for the training of an SMT engine is simply out of reach.",Conclusion,,,
This paper describes a semi-supervised approach to improving statistical dependency parsing using dependency-based word clusters.,2588492,Improving cross-domain dependency parsing with dependency-derived clusters,2015,abstract,,,"After applying a baseline parser to unlabeled text, clusters are induced using K-means with word features based on the dependency structures.",abstract,
"After applying a baseline parser to unlabeled text, clusters are induced using K-means with word features based on the dependency structures.",2588493,Improving cross-domain dependency parsing with dependency-derived clusters,2015,abstract,This paper describes a semi-supervised approach to improving statistical dependency parsing using dependency-based word clusters.,abstract,"The parser is then re-trained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank.",abstract,
"The parser is then re-trained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank.",2588494,Improving cross-domain dependency parsing with dependency-derived clusters,2015,abstract,"After applying a baseline parser to unlabeled text, clusters are induced using K-means with word features based on the dependency structures.",abstract,"We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram-based Brown clustering.",abstract,
"We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram-based Brown clustering.",2588495,Improving cross-domain dependency parsing with dependency-derived clusters,2015,abstract,"The parser is then re-trained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank.",abstract,Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models.,Introduction,
Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models.,2588496,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram-based Brown clustering.",abstract,"This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data.",Introduction,
"This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data.",2588497,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models.,Introduction,"As argued by Koo et al. (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful.",Introduction,
"As argued by Koo et al. (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful.",2588498,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data.",Introduction,"The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain.",Introduction,
"The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain.",2588499,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"As argued by Koo et al. (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful.",Introduction,"By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. ",Introduction,
"By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. ",2588500,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain.",Introduction,"While previous approaches have typically relied on the n-gram-based Brown clustering (Brown et al., 1992), this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans.",Introduction,
"While previous approaches have typically relied on the n-gram-based Brown clustering (Brown et al., 1992), this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans.",2588501,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. ",Introduction,"After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser.",Introduction,
"After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser.",2588502,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"While previous approaches have typically relied on the n-gram-based Brown clustering (Brown et al., 1992), this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans.",Introduction,"The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining.",Introduction,
"The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining.",2588503,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser.",Introduction,"The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) (Bies et al., 2012) and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993).",Introduction,
"The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) (Bies et al., 2012) and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993).",2588504,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining.",Introduction,"We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters.",Introduction,
"We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters.",2588505,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) (Bies et al., 2012) and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993).",Introduction,"All our parsing experiments use MaltParser (Nivre et al., 2007), a data-driven transition-based dependency parser. ",Introduction,
"All our parsing experiments use MaltParser (Nivre et al., 2007), a data-driven transition-based dependency parser. ",2588506,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters.",Introduction,The rest of the paper is structured as follows.,Introduction,
The rest of the paper is structured as follows.,2588507,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"All our parsing experiments use MaltParser (Nivre et al., 2007), a data-driven transition-based dependency parser. ",Introduction,Section 2 provides an overview of previous work.,Introduction,
Section 2 provides an overview of previous work.,2588508,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,The rest of the paper is structured as follows.,Introduction,"Section 3 details the data sets we use, including comments on the pre-processing.",Introduction,
"Section 3 details the data sets we use, including comments on the pre-processing.",2588509,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,Section 2 provides an overview of previous work.,Introduction,"Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5.",Introduction,
"Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5.",2588510,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"Section 3 details the data sets we use, including comments on the pre-processing.",Introduction,A summary with thoughts about future directions is provided in Section 6.,Introduction,
A summary with thoughts about future directions is provided in Section 6.,2588511,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Introduction,"Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5.",Introduction,"The task of assigning word-to-word relations is at the core of dependency parsing, and statistics regarding relations between different words in the training data therefore provide vital information.",Related work,
The the parser's SVM classifier to find the configuration with the highest LAS.,2588512,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Experiments and results,For all parsing results reported elsewhere in this paper automatically predicted PoS tags are used in both training and testing.,Gold vs. predicted PoS tags,The best configuration found for the development data is then applied to the held-out data.,Experiments and results,
The best configuration found for the development data is then applied to the held-out data.,2588513,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Experiments and results,The the parser's SVM classifier to find the configuration with the highest LAS.,Experiments and results,"Instantiating the clustering features described in Section 4.1 for the top 50k lemmas of Reuters resulted in a total of 1,673,744 feature types.",Reuters clusters,
"Instantiating the clustering features described in Section 4.1 for the top 50k lemmas of Reuters resulted in a total of 1,673,744 feature types.",2588514,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,The best configuration found for the development data is then applied to the held-out data.,Experiments and results,"Specifying a feature frequency cut-off of >= 10 brought this down to a more manageable set of 339,473 features.",Reuters clusters,
"Specifying a feature frequency cut-off of >= 10 brought this down to a more manageable set of 339,473 features.",2588515,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"Instantiating the clustering features described in Section 4.1 for the top 50k lemmas of Reuters resulted in a total of 1,673,744 feature types.",Reuters clusters,"After running K-means for 10, 50 and 100 clusters and tuning the SVM penalty parameter of the parser, the best configuration for all the development data sets was found to be K = 50 and C = 0.0625.",Reuters clusters,
"After running K-means for 10, 50 and 100 clusters and tuning the SVM penalty parameter of the parser, the best configuration for all the development data sets was found to be K = 50 and C = 0.0625.",2588516,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"Specifying a feature frequency cut-off of >= 10 brought this down to a more manageable set of 339,473 features.",Reuters clusters,"The results can be seen in Table 5, including the scores of the initial baseline parser.",Reuters clusters,
"The results can be seen in Table 5, including the scores of the initial baseline parser.",2588517,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"After running K-means for 10, 50 and 100 clusters and tuning the SVM penalty parameter of the parser, the best configuration for all the development data sets was found to be K = 50 and C = 0.0625.",Reuters clusters,"6 Looking at the baseline scores, the results clearly demonstrates the difficulty in applying parsers to text outside the domain of the training data, combined with the added noise we can expect to find in web data text types compared to newswire text:",Reuters clusters,
"6 Looking at the baseline scores, the results clearly demonstrates the difficulty in applying parsers to text outside the domain of the training data, combined with the added noise we can expect to find in web data text types compared to newswire text:",2588518,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"The results can be seen in Table 5, including the scores of the initial baseline parser.",Reuters clusters,There is a clear drop in performance for the web data compared to WSJ 22.,Reuters clusters,
There is a clear drop in performance for the web data compared to WSJ 22.,2588519,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"6 Looking at the baseline scores, the results clearly demonstrates the difficulty in applying parsers to text outside the domain of the training data, combined with the added noise we can expect to find in web data text types compared to newswire text:",Reuters clusters,"While we see that the cluster-informed parser improves over the baseline across all data sets, we also see that the improvements are larger for the web data than for WSJ 22: For Weblogs and Emails the relative reductions of error rate (RER) crements of 0.015:",Reuters clusters,
"While we see that the cluster-informed parser improves over the baseline across all data sets, we also see that the improvements are larger for the web data than for WSJ 22: For Weblogs and Emails the relative reductions of error rate (RER) crements of 0.015:",2588520,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,There is a clear drop in performance for the web data compared to WSJ 22.,Reuters clusters,The best performance was typically found for C = 0.0625. ,Reuters clusters,
The best performance was typically found for C = 0.0625. ,2588521,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"While we see that the cluster-informed parser improves over the baseline across all data sets, we also see that the improvements are larger for the web data than for WSJ 22: For Weblogs and Emails the relative reductions of error rate (RER) crements of 0.015:",Reuters clusters,"6 For the baseline model, the best C value varied slightly across the different development sets, with C = 0.0625 for WSJ 22, 0.0775 for Weblogs, and 0.0925 for Emails.",Reuters clusters,
"6 For the baseline model, the best C value varied slightly across the different development sets, with C = 0.0625 for WSJ 22, 0.0775 for Weblogs, and 0.0925 for Emails.",2588522,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,The best performance was typically found for C = 0.0625. ,Reuters clusters,In subsequent held-out testing for the baseline we use the model trained with C = 0.0625 for the WSJ data and 0.0775 for the web data.,Reuters clusters,
In subsequent held-out testing for the baseline we use the model trained with C = 0.0625 for the WSJ data and 0.0775 for the web data.,2588523,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"6 For the baseline model, the best C value varied slightly across the different development sets, with C = 0.0625 for WSJ 22, 0.0775 for Weblogs, and 0.0925 for Emails.",Reuters clusters,"Table 6: Held-out LAS evaluation on the three SANCL test domains using the baseline parser compared to parsers re-trained with information about word clusters generated from various sources: the Reuters corpus, the unlabeled SANCL data for the respective test domains, or clustering all the unlabeled SANCL data from all five domains together.",Reuters clusters,
"Table 6: Held-out LAS evaluation on the three SANCL test domains using the baseline parser compared to parsers re-trained with information about word clusters generated from various sources: the Reuters corpus, the unlabeled SANCL data for the respective test domains, or clustering all the unlabeled SANCL data from all five domains together.",2588524,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,In subsequent held-out testing for the baseline we use the model trained with C = 0.0625 for the WSJ data and 0.0775 for the web data.,Reuters clusters,"(All data sets are PoStagged automatically using SVMTool.) are 1.7% and 0.96% respectively, compared to RER = 0.53% for WSJ.",Reuters clusters,
"(All data sets are PoStagged automatically using SVMTool.) are 1.7% and 0.96% respectively, compared to RER = 0.53% for WSJ.",2588525,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"Table 6: Held-out LAS evaluation on the three SANCL test domains using the baseline parser compared to parsers re-trained with information about word clusters generated from various sources: the Reuters corpus, the unlabeled SANCL data for the respective test domains, or clustering all the unlabeled SANCL data from all five domains together.",Reuters clusters,"When applying these models to the held-out data, the gains of the clusterinformed models are even larger, as shown in Table 6, with error reductions of up to 3.52%.",Reuters clusters,
"When applying these models to the held-out data, the gains of the clusterinformed models are even larger, as shown in Table 6, with error reductions of up to 3.52%.",2588526,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"(All data sets are PoStagged automatically using SVMTool.) are 1.7% and 0.96% respectively, compared to RER = 0.53% for WSJ.",Reuters clusters,"When testing on WSJ 23 (not included in the table), the baseline obtained LAS = 86.88, compared to 87.16 for the cluster model, amounting to RER = 2.13%.",Reuters clusters,
"When testing on WSJ 23 (not included in the table), the baseline obtained LAS = 86.88, compared to 87.16 for the cluster model, amounting to RER = 2.13%.",2588527,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"When applying these models to the held-out data, the gains of the clusterinformed models are even larger, as shown in Table 6, with error reductions of up to 3.52%.",Reuters clusters,The differences in held-out performance where detected as statistically significant across all the data sets. ,Reuters clusters,
The differences in held-out performance where detected as statistically significant across all the data sets. ,2588528,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"When testing on WSJ 23 (not included in the table), the baseline obtained LAS = 86.88, compared to 87.16 for the cluster model, amounting to RER = 2.13%.",Reuters clusters,Note that one complication with respect to assessing the effect of K-means clustering is the fact that the algorithm is sensitive to the initial random seeding of the cluster centers.,Reuters clusters,
Note that one complication with respect to assessing the effect of K-means clustering is the fact that the algorithm is sensitive to the initial random seeding of the cluster centers.,2588529,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,The differences in held-out performance where detected as statistically significant across all the data sets. ,Reuters clusters,"Using the minibatch implementation in scikit-learn alleviates this problem to some degree in that it will compute a handful of different seedings and choose the one with the lowest inertia (i.e., within-cluster sumof-squares) before starting the clustering.",Reuters clusters,
"Using the minibatch implementation in scikit-learn alleviates this problem to some degree in that it will compute a handful of different seedings and choose the one with the lowest inertia (i.e., within-cluster sumof-squares) before starting the clustering.",2588530,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,Note that one complication with respect to assessing the effect of K-means clustering is the fact that the algorithm is sensitive to the initial random seeding of the cluster centers.,Reuters clusters,"Still, repeated runs with the same parameters and the same input can generate different outputs.",Reuters clusters,
"Still, repeated runs with the same parameters and the same input can generate different outputs.",2588531,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"Using the minibatch implementation in scikit-learn alleviates this problem to some degree in that it will compute a handful of different seedings and choose the one with the lowest inertia (i.e., within-cluster sumof-squares) before starting the clustering.",Reuters clusters,In order to quantify the extent of this effect we run K-means for K = 100 clusters 10 times on the Reuters data and parsed WSJ 22 with the retrained parser (fixing the SVM parameter C to 0.0625).,Reuters clusters,
In order to quantify the extent of this effect we run K-means for K = 100 clusters 10 times on the Reuters data and parsed WSJ 22 with the retrained parser (fixing the SVM parameter C to 0.0625).,2588532,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,"Still, repeated runs with the same parameters and the same input can generate different outputs.",Reuters clusters,"This resulted in mean and median LAS scores of 86.78 and 86.81 respectively, with a variance of 0.009 and a standard deviation of 0.095.",Reuters clusters,
"This resulted in mean and median LAS scores of 86.78 and 86.81 respectively, with a variance of 0.009 and a standard deviation of 0.095.",2588533,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Reuters clusters,In order to quantify the extent of this effect we run K-means for K = 100 clusters 10 times on the Reuters data and parsed WSJ 22 with the retrained parser (fixing the SVM parameter C to 0.0625).,Reuters clusters,"While we already see improvements in parsing accuracy for the web data, one could expect to see even greater gains when using clusters generated from texts in the same domain that is to be parsed. ",Per-domain SANCL clusters,
"While we already see improvements in parsing accuracy for the web data, one could expect to see even greater gains when using clusters generated from texts in the same domain that is to be parsed. ",2588534,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"This resulted in mean and median LAS scores of 86.78 and 86.81 respectively, with a variance of 0.009 and a standard deviation of 0.095.",Reuters clusters,We therefore tried running K-means on the unlabeled SANCL data from respective test domains 7 (while still training the parser on WSJ like above).,Per-domain SANCL clusters,
We therefore tried running K-means on the unlabeled SANCL data from respective test domains 7 (while still training the parser on WSJ like above).,2588535,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"While we already see improvements in parsing accuracy for the web data, one could expect to see even greater gains when using clusters generated from texts in the same domain that is to be parsed. ",Per-domain SANCL clusters,"This means that, for example, the 4,060 sentences in the labeled Weblogs data is parsed using clusters generated for the 50K most frequent lemmas of the 524,834 sentences in the unlabeled Weblogs data.",Per-domain SANCL clusters,
"This means that, for example, the 4,060 sentences in the labeled Weblogs data is parsed using clusters generated for the 50K most frequent lemmas of the 524,834 sentences in the unlabeled Weblogs data.",2588536,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,We therefore tried running K-means on the unlabeled SANCL data from respective test domains 7 (while still training the parser on WSJ like above).,Per-domain SANCL clusters,"After empirically tuning the parameters, the highest LAS scores on the development sets were observed for the configuration of K = 100 and C = 0.0625.",Per-domain SANCL clusters,
"After empirically tuning the parameters, the highest LAS scores on the development sets were observed for the configuration of K = 100 and C = 0.0625.",2588537,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"This means that, for example, the 4,060 sentences in the labeled Weblogs data is parsed using clusters generated for the 50K most frequent lemmas of the 524,834 sentences in the unlabeled Weblogs data.",Per-domain SANCL clusters,(As for all clustering results reported here we use the Form all feature set of Table 3.) ,Per-domain SANCL clusters,
(As for all clustering results reported here we use the Form all feature set of Table 3.) ,2588538,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"After empirically tuning the parameters, the highest LAS scores on the development sets were observed for the configuration of K = 100 and C = 0.0625.",Per-domain SANCL clusters,"Development results are provided in Table 5 and held-out results in Table 6, see the row SANCL per-domain.",Per-domain SANCL clusters,
"Development results are provided in Table 5 and held-out results in Table 6, see the row SANCL per-domain.",2588539,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,(As for all clustering results reported here we use the Form all feature set of Table 3.) ,Per-domain SANCL clusters,"Although the re-trained parser with per-domain clusters again significantly outperforms the baseline across all data sets, there is no clear advantage to using per-domain clusters compared to the Reuters cluster of the news domain.",Per-domain SANCL clusters,
"Although the re-trained parser with per-domain clusters again significantly outperforms the baseline across all data sets, there is no clear advantage to using per-domain clusters compared to the Reuters cluster of the news domain.",2588540,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"Development results are provided in Table 5 and held-out results in Table 6, see the row SANCL per-domain.",Per-domain SANCL clusters,The parser using per-domain web clusters improves on the parser using Reuters clusters for two out of five domains: Emails (development) and Reviews (held-out).,Per-domain SANCL clusters,
The parser using per-domain web clusters improves on the parser using Reuters clusters for two out of five domains: Emails (development) and Reviews (held-out).,2588541,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"Although the re-trained parser with per-domain clusters again significantly outperforms the baseline across all data sets, there is no clear advantage to using per-domain clusters compared to the Reuters cluster of the news domain.",Per-domain SANCL clusters,"Interestingly, these are also the two domains with the largest unlabeled data sets, as shown in Table 1.",Per-domain SANCL clusters,
"Interestingly, these are also the two domains with the largest unlabeled data sets, as shown in Table 1.",2588542,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,The parser using per-domain web clusters improves on the parser using Reuters clusters for two out of five domains: Emails (development) and Reviews (held-out).,Per-domain SANCL clusters,"At the same time, we see that the Reuters corpus is vastly larger than any of the unlabeled SANCL corpora.",Per-domain SANCL clusters,
"At the same time, we see that the Reuters corpus is vastly larger than any of the unlabeled SANCL corpora.",2588543,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"Interestingly, these are also the two domains with the largest unlabeled data sets, as shown in Table 1.",Per-domain SANCL clusters,"For our next round of experiments we therefore wanted to see whether we could compensate for this difference in size by clustering all the unlabeled SANCL data combined, while still hoping to see positive effects of using data closer to the test domain.",Per-domain SANCL clusters,
"For our next round of experiments we therefore wanted to see whether we could compensate for this difference in size by clustering all the unlabeled SANCL data combined, while still hoping to see positive effects of using data closer to the test domain.",2588544,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Per-domain SANCL clusters,"At the same time, we see that the Reuters corpus is vastly larger than any of the unlabeled SANCL corpora.",Per-domain SANCL clusters,The motivation of the experiments in this section is to see whether using word clusters generated from all the five unlabeled SANCL sections together yields better parsing performance than using clusters from each domain individually.,All-in-one SANCL clusters,
The motivation of the experiments in this section is to see whether using word clusters generated from all the five unlabeled SANCL sections together yields better parsing performance than using clusters from each domain individually.,2588545,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"For our next round of experiments we therefore wanted to see whether we could compensate for this difference in size by clustering all the unlabeled SANCL data combined, while still hoping to see positive effects of using data closer to the test domain.",Per-domain SANCL clusters,"Using a feature cut-off of ≥ 3, a total of 375,793 feature types are extracted for clustering the 50K most frequent noun/verb/adjective lemmas in the concatenated SANCL data.",All-in-one SANCL clusters,
"Using a feature cut-off of ≥ 3, a total of 375,793 feature types are extracted for clustering the 50K most frequent noun/verb/adjective lemmas in the concatenated SANCL data.",2588546,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,The motivation of the experiments in this section is to see whether using word clusters generated from all the five unlabeled SANCL sections together yields better parsing performance than using clusters from each domain individually.,All-in-one SANCL clusters,"Using 100 clusters generated by K-means and setting C = 0.0625 for the SVM classifier in MaltParser, the results are shown as SANCL all in Tables 5-6. ",All-in-one SANCL clusters,
"Using 100 clusters generated by K-means and setting C = 0.0625 for the SVM classifier in MaltParser, the results are shown as SANCL all in Tables 5-6. ",2588547,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"Using a feature cut-off of ≥ 3, a total of 375,793 feature types are extracted for clustering the 50K most frequent noun/verb/adjective lemmas in the concatenated SANCL data.",All-in-one SANCL clusters,"We see that for all but one data set, the use of all-in-one SANCL clusters yield better results than per-domain clusters.",All-in-one SANCL clusters,
"We see that for all but one data set, the use of all-in-one SANCL clusters yield better results than per-domain clusters.",2588548,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"Using 100 clusters generated by K-means and setting C = 0.0625 for the SVM classifier in MaltParser, the results are shown as SANCL all in Tables 5-6. ",All-in-one SANCL clusters,"The exception is the Emails (development) data, where the per-domain clusters still yields the highest LAS overall.",All-in-one SANCL clusters,
"The exception is the Emails (development) data, where the per-domain clusters still yields the highest LAS overall.",2588549,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"We see that for all but one data set, the use of all-in-one SANCL clusters yield better results than per-domain clusters.",All-in-one SANCL clusters,"At the same time, we see that the initial Reuters clusters still provide the highest score for three of the data sets, while the all-in-one SANCL model has the highest overall score for the (held-out) Reviews section.",All-in-one SANCL clusters,
"At the same time, we see that the initial Reuters clusters still provide the highest score for three of the data sets, while the all-in-one SANCL model has the highest overall score for the (held-out) Reviews section.",2588550,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"The exception is the Emails (development) data, where the per-domain clusters still yields the highest LAS overall.",All-in-one SANCL clusters,"It is also worth noting that at 75 million tokens, the concatenated unlabeled SANCL data is still a third of the size of Reuters.",All-in-one SANCL clusters,
"It is also worth noting that at 75 million tokens, the concatenated unlabeled SANCL data is still a third of the size of Reuters.",2588551,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"At the same time, we see that the initial Reuters clusters still provide the highest score for three of the data sets, while the all-in-one SANCL model has the highest overall score for the (held-out) Reviews section.",All-in-one SANCL clusters,"When testing for statistical significance on the held-out data, none of the differences between the Reuters and SANCL runs are detected as being significant. ",All-in-one SANCL clusters,
"When testing for statistical significance on the held-out data, none of the differences between the Reuters and SANCL runs are detected as being significant. ",2588552,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"It is also worth noting that at 75 million tokens, the concatenated unlabeled SANCL data is still a third of the size of Reuters.",All-in-one SANCL clusters,"In sum, it is not possible to conclude anything about which data set provides the optimal source for generating the dependency-based word clusters for the parser, although it is clear that whichever data set is used, the re-trained parser with cluster features improves significantly on the baseline parser.",All-in-one SANCL clusters,
"In sum, it is not possible to conclude anything about which data set provides the optimal source for generating the dependency-based word clusters for the parser, although it is clear that whichever data set is used, the re-trained parser with cluster features improves significantly on the baseline parser.",2588553,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"When testing for statistical significance on the held-out data, none of the differences between the Reuters and SANCL runs are detected as being significant. ",All-in-one SANCL clusters,"For the final round of experiments, we investigate the use of the dependency-based clusters compared to n-gram Brown clusters as used in most previous studies.",All-in-one SANCL clusters,
"For the final round of experiments, we investigate the use of the dependency-based clusters compared to n-gram Brown clusters as used in most previous studies.",2588554,Improving cross-domain dependency parsing with dependency-derived clusters,2015,All-in-one SANCL clusters,"In sum, it is not possible to conclude anything about which data set provides the optimal source for generating the dependency-based word clusters for the parser, although it is clear that whichever data set is used, the re-trained parser with cluster features improves significantly on the baseline parser.",All-in-one SANCL clusters,"In this section, we report the results of parsing the English web data of the OntoNotes corpus as described in Section 3, in addition to the OntoNotes version of WSJ section 23, mirroring the data sets used by Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,
"In this section, we report the results of parsing the English web data of the OntoNotes corpus as described in Section 3, in addition to the OntoNotes version of WSJ section 23, mirroring the data sets used by Øvrelid and Skjaerholt (2012).",2588555,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"For the final round of experiments, we investigate the use of the dependency-based clusters compared to n-gram Brown clusters as used in most previous studies.",All-in-one SANCL clusters,The purpose is to compare the results obtained using our dependency-based clusters and the Brown clusters used by Øvrelid and Skjaerholt (2012) and several previous studies.,Comparison to using Brown clusters,
The purpose is to compare the results obtained using our dependency-based clusters and the Brown clusters used by Øvrelid and Skjaerholt (2012) and several previous studies.,2588556,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"In this section, we report the results of parsing the English web data of the OntoNotes corpus as described in Section 3, in addition to the OntoNotes version of WSJ section 23, mirroring the data sets used by Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,"As to isolate the effect of the clustering approach as best as possible, we here use the same version of MaltParser as used by Øvrelid and Skjaerholt (2012) (i.e., v.1.4.1).",Comparison to using Brown clusters,
"As to isolate the effect of the clustering approach as best as possible, we here use the same version of MaltParser as used by Øvrelid and Skjaerholt (2012) (i.e., v.1.4.1).",2588557,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,The purpose is to compare the results obtained using our dependency-based clusters and the Brown clusters used by Øvrelid and Skjaerholt (2012) and several previous studies.,Comparison to using Brown clusters,"We otherwise apply the model configuration that was found to give the best results for the development experiments in Table 5, i.e., K = 100 and C = 0.0625, and apply models based on both the Reuters clusters and the all-in-one SANCL clusters. ",Comparison to using Brown clusters,
"We otherwise apply the model configuration that was found to give the best results for the development experiments in Table 5, i.e., K = 100 and C = 0.0625, and apply models based on both the Reuters clusters and the all-in-one SANCL clusters. ",2588558,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"As to isolate the effect of the clustering approach as best as possible, we here use the same version of MaltParser as used by Øvrelid and Skjaerholt (2012) (i.e., v.1.4.1).",Comparison to using Brown clusters,The LAS results for the different models are compared in Table 7.,Comparison to using Brown clusters,
The LAS results for the different models are compared in Table 7.,2588559,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"We otherwise apply the model configuration that was found to give the best results for the development experiments in Table 5, i.e., K = 100 and C = 0.0625, and apply models based on both the Reuters clusters and the all-in-one SANCL clusters. ",Comparison to using Brown clusters,"It is important to note that while the scores for the dependency-based clusters represent strict held-out results, the results for Øvrelid and Skjaerholt (2012) are to be regarded as development results: The scores of Øvrelid and Skjaerholt (2012)  parameters include the number of clusters and the choice of feature set for the parser, corresponding to the various options listed in Table 3.",Comparison to using Brown clusters,
"It is important to note that while the scores for the dependency-based clusters represent strict held-out results, the results for Øvrelid and Skjaerholt (2012) are to be regarded as development results: The scores of Øvrelid and Skjaerholt (2012)  parameters include the number of clusters and the choice of feature set for the parser, corresponding to the various options listed in Table 3.",2588560,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,The LAS results for the different models are compared in Table 7.,Comparison to using Brown clusters,"In spite of this, we find that all the models using dependencybased clusters yield quite a bit higher LAS than the Brown-based models of Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,
"In spite of this, we find that all the models using dependencybased clusters yield quite a bit higher LAS than the Brown-based models of Øvrelid and Skjaerholt (2012).",2588561,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"It is important to note that while the scores for the dependency-based clusters represent strict held-out results, the results for Øvrelid and Skjaerholt (2012) are to be regarded as development results: The scores of Øvrelid and Skjaerholt (2012)  parameters include the number of clusters and the choice of feature set for the parser, corresponding to the various options listed in Table 3.",Comparison to using Brown clusters,"At the same time, even our baseline models perform on par with or better than the Brown models, so it is likely that other factors not accounted for are also affecting the results reported in Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,
"At the same time, even our baseline models perform on par with or better than the Brown models, so it is likely that other factors not accounted for are also affecting the results reported in Øvrelid and Skjaerholt (2012).",2588562,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"In spite of this, we find that all the models using dependencybased clusters yield quite a bit higher LAS than the Brown-based models of Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,Note that the table include baseline results for both our own set-up and the scores provided in Øvrelid and Skjaerholt (2012).,Comparison to using Brown clusters,
Note that the table include baseline results for both our own set-up and the scores provided in Øvrelid and Skjaerholt (2012).,2588563,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,"At the same time, even our baseline models perform on par with or better than the Brown models, so it is likely that other factors not accounted for are also affecting the results reported in Øvrelid and Skjaerholt (2012).",Comparison to using Brown clusters,Despite our efforts to replicate the setup described by Øvrelid and Skjaerholt (2012) we were not able to reproduce the results.,Comparison to using Brown clusters,
Despite our efforts to replicate the setup described by Øvrelid and Skjaerholt (2012) we were not able to reproduce the results.,2588564,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,Note that the table include baseline results for both our own set-up and the scores provided in Øvrelid and Skjaerholt (2012).,Comparison to using Brown clusters,"The scores shown for our own baseline in Table 7 were produced using our tuned C parameters the SVMs (though using the same version of the parser and tagger), but even when using the default parameters like reported by Øvrelid and Skjaerholt (2012) our scores diverged.",Comparison to using Brown clusters,
"The scores shown for our own baseline in Table 7 were produced using our tuned C parameters the SVMs (though using the same version of the parser and tagger), but even when using the default parameters like reported by Øvrelid and Skjaerholt (2012) our scores diverged.",2588565,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Comparison to using Brown clusters,Despite our efforts to replicate the setup described by Øvrelid and Skjaerholt (2012) we were not able to reproduce the results.,Comparison to using Brown clusters,This paper has described a semi-supervised approach for improving a data-driven dependency parser using dependency-based clusters.,Summary and future work,
This paper has described a semi-supervised approach for improving a data-driven dependency parser using dependency-based clusters.,2588566,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"The scores shown for our own baseline in Table 7 were produced using our tuned C parameters the SVMs (though using the same version of the parser and tagger), but even when using the default parameters like reported by Øvrelid and Skjaerholt (2012) our scores diverged.",Comparison to using Brown clusters,"The parser is first applied to a large corpus of unlabeled text, providing the input to K-means clustering of lemmas using features extracted from the dependency structures.",Summary and future work,
"The parser is first applied to a large corpus of unlabeled text, providing the input to K-means clustering of lemmas using features extracted from the dependency structures.",2588567,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,This paper has described a semi-supervised approach for improving a data-driven dependency parser using dependency-based clusters.,Summary and future work,"The parser is the re-trained with new features that include information about the word clusters, thereby introducing an element of self-training.",Summary and future work,
"The parser is the re-trained with new features that include information about the word clusters, thereby introducing an element of self-training.",2588568,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"The parser is first applied to a large corpus of unlabeled text, providing the input to K-means clustering of lemmas using features extracted from the dependency structures.",Summary and future work,"The cluster-informed parser is shown to improve significantly over the baseline on both in-and out-of-domain tests, including a wide range of web texts.",Summary and future work,
"The cluster-informed parser is shown to improve significantly over the baseline on both in-and out-of-domain tests, including a wide range of web texts.",2588569,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"The parser is the re-trained with new features that include information about the word clusters, thereby introducing an element of self-training.",Summary and future work,For held-out tests on the web data the use of clusters yields error reductions of up to 3.52% relative to the baseline.,Summary and future work,
For held-out tests on the web data the use of clusters yields error reductions of up to 3.52% relative to the baseline.,2588570,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"The cluster-informed parser is shown to improve significantly over the baseline on both in-and out-of-domain tests, including a wide range of web texts.",Summary and future work,The results of using our dependency-based clusters also compare favorably to previous studies using the ngram based Brown clusters. ,Summary and future work,
The results of using our dependency-based clusters also compare favorably to previous studies using the ngram based Brown clusters. ,2588571,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,For held-out tests on the web data the use of clusters yields error reductions of up to 3.52% relative to the baseline.,Summary and future work,There are several directions we wish to pursue in follow-up work.,Summary and future work,
There are several directions we wish to pursue in follow-up work.,2588572,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,The results of using our dependency-based clusters also compare favorably to previous studies using the ngram based Brown clusters. ,Summary and future work,The experiments in this paper were based on the feature set described by Øvrelid and Skjaerholt (2012).,Summary and future work,
The experiments in this paper were based on the feature set described by Øvrelid and Skjaerholt (2012).,2588573,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,There are several directions we wish to pursue in follow-up work.,Summary and future work,"Further work will give priority to the design and experimentation with additional cluster-based features in the parser, preferably informed by an analysis of the parser errors.",Summary and future work,
"Further work will give priority to the design and experimentation with additional cluster-based features in the parser, preferably informed by an analysis of the parser errors.",2588574,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,The experiments in this paper were based on the feature set described by Øvrelid and Skjaerholt (2012).,Summary and future work,"The clustering described above comprise a fairly large vocabulary of 50,000 lemmas.",Summary and future work,
"The clustering described above comprise a fairly large vocabulary of 50,000 lemmas.",2588575,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"Further work will give priority to the design and experimentation with additional cluster-based features in the parser, preferably informed by an analysis of the parser errors.",Summary and future work,"In future experiments we would like to gauge the trade-off between the vocabulary size N and the number of clusters K: Decreasing N would allow us to specify a higher K. Moreover, when inspecting the the word clusters many of them can be seen to be fairly specific to distinct parts-of-speech -unsurprisingly, given the feature templates described in Section 4.1.",Summary and future work,
"In future experiments we would like to gauge the trade-off between the vocabulary size N and the number of clusters K: Decreasing N would allow us to specify a higher K. Moreover, when inspecting the the word clusters many of them can be seen to be fairly specific to distinct parts-of-speech -unsurprisingly, given the feature templates described in Section 4.1.",2588576,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"The clustering described above comprise a fairly large vocabulary of 50,000 lemmas.",Summary and future work,In further experiments we therefore plan on performing the clustering separately for lemmas of different parts-of-speech.,Summary and future work,
In further experiments we therefore plan on performing the clustering separately for lemmas of different parts-of-speech.,2588577,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,"In future experiments we would like to gauge the trade-off between the vocabulary size N and the number of clusters K: Decreasing N would allow us to specify a higher K. Moreover, when inspecting the the word clusters many of them can be seen to be fairly specific to distinct parts-of-speech -unsurprisingly, given the feature templates described in Section 4.1.",Summary and future work,"This will also be beneficial in terms of scalability: Computational considerations otherwise enforce limitations on vocabulary size, the number of clusters, and the size of the feature space, but running multiple and separate K-means clusterings for different PoS classes means we can increase the number of total clusters used and the lexical coverage of the clusters.",Summary and future work,
"This will also be beneficial in terms of scalability: Computational considerations otherwise enforce limitations on vocabulary size, the number of clusters, and the size of the feature space, but running multiple and separate K-means clusterings for different PoS classes means we can increase the number of total clusters used and the lexical coverage of the clusters.",2588578,Improving cross-domain dependency parsing with dependency-derived clusters,2015,Summary and future work,In further experiments we therefore plan on performing the clustering separately for lemmas of different parts-of-speech.,Summary and future work,,,
  We investigate the automatic classification of patient discharge notes into standard disease labels.,6306806,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,abstract,,,"We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement. ",abstract,
"We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement. ",6306807,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,abstract,  We investigate the automatic classification of patient discharge notes into standard disease labels.,abstract,"Electronic Health Records (EHRs) have grown significantly over the years and now include an unprecedented amount and variety of patient information, including demographics, vital sign measurements, laboratory test results, prescriptions, procedures performed, digitized notes, imaging reports, mortality etc.",Introduction,
"Electronic Health Records (EHRs) have grown significantly over the years and now include an unprecedented amount and variety of patient information, including demographics, vital sign measurements, laboratory test results, prescriptions, procedures performed, digitized notes, imaging reports, mortality etc.",6306808,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement. ",abstract,They usually contain both structured data (e.g. admission dates) as well as unstructured data (e.g. notes written by doctors). ,Introduction,
They usually contain both structured data (e.g. admission dates) as well as unstructured data (e.g. notes written by doctors). ,6306809,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"Electronic Health Records (EHRs) have grown significantly over the years and now include an unprecedented amount and variety of patient information, including demographics, vital sign measurements, laboratory test results, prescriptions, procedures performed, digitized notes, imaging reports, mortality etc.",Introduction,"Provided it can be processed, the information in these records -especially the unstructured data -holds the promise of new medical insights and improved medical care, such as faster detection of epidemics, identification of symptoms, personalized treatment, or a more detailed understanding of treatment outcomes. ",Introduction,
"Provided it can be processed, the information in these records -especially the unstructured data -holds the promise of new medical insights and improved medical care, such as faster detection of epidemics, identification of symptoms, personalized treatment, or a more detailed understanding of treatment outcomes. ",6306810,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,They usually contain both structured data (e.g. admission dates) as well as unstructured data (e.g. notes written by doctors). ,Introduction,One such gains is a more automated and accurate way to report diseases.,Introduction,
One such gains is a more automated and accurate way to report diseases.,6306811,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"Provided it can be processed, the information in these records -especially the unstructured data -holds the promise of new medical insights and improved medical care, such as faster detection of epidemics, identification of symptoms, personalized treatment, or a more detailed understanding of treatment outcomes. ",Introduction,"Since 1967, the World Health Organization (WHO) has developed an International Classification of Diseases (ICD) to ""monitor the incidence and prevalence of diseases, observe reimbursements and resource allocation trends, and keep track of safety and quality guidelines"" 1 .",Introduction,
"Since 1967, the World Health Organization (WHO) has developed an International Classification of Diseases (ICD) to ""monitor the incidence and prevalence of diseases, observe reimbursements and resource allocation trends, and keep track of safety and quality guidelines"" 1 .",6306812,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,One such gains is a more automated and accurate way to report diseases.,Introduction,Currently this ICD labeling is done manually by administrative personnel based on definitions and is subject to interpretation and errors 2 . ,Introduction,
Currently this ICD labeling is done manually by administrative personnel based on definitions and is subject to interpretation and errors 2 . ,6306813,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"Since 1967, the World Health Organization (WHO) has developed an International Classification of Diseases (ICD) to ""monitor the incidence and prevalence of diseases, observe reimbursements and resource allocation trends, and keep track of safety and quality guidelines"" 1 .",Introduction,"In this paper, we focus our efforts on the automatic labeling of discharge notes from the MIMIC 3 Database into ICD codes.",Introduction,
"In this paper, we focus our efforts on the automatic labeling of discharge notes from the MIMIC 3 Database into ICD codes.",6306814,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,Currently this ICD labeling is done manually by administrative personnel based on definitions and is subject to interpretation and errors 2 . ,Introduction,"This public database of EHRs contains data points on about 41,000 patients from an intensive care units between 2001 and 2012, including notes on close ot 53,000 admissions.",Introduction,
"This public database of EHRs contains data points on about 41,000 patients from an intensive care units between 2001 and 2012, including notes on close ot 53,000 admissions.",6306815,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"In this paper, we focus our efforts on the automatic labeling of discharge notes from the MIMIC 3 Database into ICD codes.",Introduction,"MIMIC has already been 1 http://www.who.int/classifications/icd/en/ 2 See recent articles on opiod overdose statistics in the US https://tinyurl.com/y8zc3xcq 3 Medical Information Mart for Intensive Care https://mimic.physionet.org proven valuable for efforts similar to ours, which will make comparisons more accurate.",Introduction,
"MIMIC has already been 1 http://www.who.int/classifications/icd/en/ 2 See recent articles on opiod overdose statistics in the US https://tinyurl.com/y8zc3xcq 3 Medical Information Mart for Intensive Care https://mimic.physionet.org proven valuable for efforts similar to ours, which will make comparisons more accurate.",6306816,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Introduction,"This public database of EHRs contains data points on about 41,000 patients from an intensive care units between 2001 and 2012, including notes on close ot 53,000 admissions.",Introduction,The problem of assigning ICD codes automatically to discharge summaries has previously been studied.,Background,
"The ICD-9 nomenclature applied by MIMIC III contains about 14,000 numerical codes representing all possible diagnoses and procedures 5 .",6306817,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"We can broadly break our approach to this multilabel multi-class problem into the steps detailed below: output labeling, input preprocessing, training and output metrics, and algorithms.",Background,"Out of those 14,000, 5,932 distinct codes are used to describe the 52,696 hospital admissions of the database, with 1,112 codes appearing only once. ",Output labeling,
"Out of those 14,000, 5,932 distinct codes are used to describe the 52,696 hospital admissions of the database, with 1,112 codes appearing only once. ",6306818,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"The ICD-9 nomenclature applied by MIMIC III contains about 14,000 numerical codes representing all possible diagnoses and procedures 5 .",Output labeling,This creates an issue for classification algorithms since many codes would need to be predicted with few or no example in the training set.,Output labeling,
This creates an issue for classification algorithms since many codes would need to be predicted with few or no example in the training set.,6306819,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"Out of those 14,000, 5,932 distinct codes are used to describe the 52,696 hospital admissions of the database, with 1,112 codes appearing only once. ",Output labeling,"Fortunately, the ICD codes are organized in a hierarchical tree, see Figure 1.",Output labeling,
"Fortunately, the ICD codes are organized in a hierarchical tree, see Figure 1.",6306820,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,This creates an issue for classification algorithms since many codes would need to be predicted with few or no example in the training set.,Output labeling,"As a result, we identified 3 mains methods to deal with the high number of classes: • Restrict the labels to the most common Level 5 codes, method used by some project reports.",Output labeling,
"As a result, we identified 3 mains methods to deal with the high number of classes: • Restrict the labels to the most common Level 5 codes, method used by some project reports.",6306821,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"Fortunately, the ICD codes are organized in a hierarchical tree, see Figure 1.",Output labeling,We start by selecting the 20 most common codes (see Figure 2) • Relabel all codes into a smaller class of codes.,Output labeling,
We start by selecting the 20 most common codes (see Figure 2) • Relabel all codes into a smaller class of codes.,6306822,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"As a result, we identified 3 mains methods to deal with the high number of classes: • Restrict the labels to the most common Level 5 codes, method used by some project reports.",Output labeling,"This approach was done manually by (Gehrmann et al., 2017).",Output labeling,
"This approach was done manually by (Gehrmann et al., 2017).",6306823,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,We start by selecting the 20 most common codes (see Figure 2) • Relabel all codes into a smaller class of codes.,Output labeling,"Here, we take advantage of the ICD hierarchy, and simply relabel notes into the 17 nodes of depth 1. 8 • A third possible approach -to explore in further work -would be to keep all labels, but use a ""hierarchical metric"", i.e. discounting errors if labels are in the same ICD branch. ",Output labeling,
"Here, we take advantage of the ICD hierarchy, and simply relabel notes into the 17 nodes of depth 1. 8 • A third possible approach -to explore in further work -would be to keep all labels, but use a ""hierarchical metric"", i.e. discounting errors if labels are in the same ICD branch. ",6306824,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"This approach was done manually by (Gehrmann et al., 2017).",Output labeling,"Intuitively, we can expect the second approach to perform better, since i) the codes represent very different realities, whereas common codes may be related, and ii) the distribution is less balanced.",Output labeling,
"Intuitively, we can expect the second approach to perform better, since i) the codes represent very different realities, whereas common codes may be related, and ii) the distribution is less balanced.",6306825,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"Here, we take advantage of the ICD hierarchy, and simply relabel notes into the 17 nodes of depth 1. 8 • A third possible approach -to explore in further work -would be to keep all labels, but use a ""hierarchical metric"", i.e. discounting errors if labels are in the same ICD branch. ",Output labeling,An additional benefit is that we have access to the full dataset for training (53K) instead of just a subset if we had re annotated the dataset manually or if we take the most common codes (46k). ,Output labeling,
An additional benefit is that we have access to the full dataset for training (53K) instead of just a subset if we had re annotated the dataset manually or if we take the most common codes (46k). ,6306826,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,"Intuitively, we can expect the second approach to perform better, since i) the codes represent very different realities, whereas common codes may be related, and ii) the distribution is less balanced.",Output labeling,"The third untested approach would allow to keep all codes intact, and hence be more precise in the labeling.",Output labeling,
"The third untested approach would allow to keep all codes intact, and hence be more precise in the labeling.",6306827,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Output labeling,An additional benefit is that we have access to the full dataset for training (53K) instead of just a subset if we had re annotated the dataset manually or if we take the most common codes (46k). ,Output labeling,"The database presents multiple clinical notes categories including things like Radiology, Nutrition, Pharmacy, or Social Work.",Note preprocessing,
"Unfortunately, even after the previous steps, the wording is still not standardized.",6306828,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"This is done without loss of generality, since 99.5% of the notes meet this criteria.",Note preprocessing,"Like some unpublished papers, we can see at least 13 ways that write hypercholesterolemia, for instance.",Embedding,
"Like some unpublished papers, we can see at least 13 ways that write hypercholesterolemia, for instance.",6306829,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"Unfortunately, even after the previous steps, the wording is still not standardized.",Embedding,10  One way to solve this issue would be to use Named Entity Recognition (NER).,Embedding,
10  One way to solve this issue would be to use Named Entity Recognition (NER).,6306830,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"Like some unpublished papers, we can see at least 13 ways that write hypercholesterolemia, for instance.",Embedding,"Some implementations exist which are tailored to the medical realm, such as Apache cTAKES or MetaMap.",Embedding,
"Some implementations exist which are tailored to the medical realm, such as Apache cTAKES or MetaMap.",6306831,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,10  One way to solve this issue would be to use Named Entity Recognition (NER).,Embedding,"However, previous papers (Gehrmann et al., 2017) find that embeddings perform better, trusting embeddings' ability to make ""misspellings, synonyms and abbreviations of an original word learn similar embeddings"".",Embedding,
"However, previous papers (Gehrmann et al., 2017) find that embeddings perform better, trusting embeddings' ability to make ""misspellings, synonyms and abbreviations of an original word learn similar embeddings"".",6306832,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"Some implementations exist which are tailored to the medical realm, such as Apache cTAKES or MetaMap.",Embedding,"Therefore we used trainable embeddings, sometimes pre-trained with the Glove algorithm on Wiki 11 or on the MIMIC notes to account for the vocabulary specificity. ",Embedding,
"Therefore we used trainable embeddings, sometimes pre-trained with the Glove algorithm on Wiki 11 or on the MIMIC notes to account for the vocabulary specificity. ",6306833,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"However, previous papers (Gehrmann et al., 2017) find that embeddings perform better, trusting embeddings' ability to make ""misspellings, synonyms and abbreviations of an original word learn similar embeddings"".",Embedding,"Note that some papers such as (Perotte et al., 2014) use TF-IDF, either to restrict the original vocabulary size or to transform notes into continuous components. ",Embedding,
"Note that some papers such as (Perotte et al., 2014) use TF-IDF, either to restrict the original vocabulary size or to transform notes into continuous components. ",6306834,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"Therefore we used trainable embeddings, sometimes pre-trained with the Glove algorithm on Wiki 11 or on the MIMIC notes to account for the vocabulary specificity. ",Embedding,"Here since we use embeddings of size 100, we can keep",Embedding,
"Here since we use embeddings of size 100, we can keep",6306835,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Embedding,"Note that some papers such as (Perotte et al., 2014) use TF-IDF, either to restrict the original vocabulary size or to transform notes into continuous components. ",Embedding,"For multi-label classifications like this one, an approach is to convert the problem into single binary classification tasks.",Training Loss Function,
"The resulting vector from the neural network may be interpreted as a probability of the individual ICD-9 codes (each cell has a value 0-1, but does not sum to 1).",6306836,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Threshold Calibration,Yang et al. (2016) uses Bidirectional GRUs while we use LSTMs for a fair comparision with the flat LSTM models.,Attention,"To complete the prediction, we must convert the vector to binary values. ",Threshold Calibration,
"To complete the prediction, we must convert the vector to binary values. ",6306837,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Threshold Calibration,"The resulting vector from the neural network may be interpreted as a probability of the individual ICD-9 codes (each cell has a value 0-1, but does not sum to 1).",Threshold Calibration,"There are several methods for selecting a Threshold (Zhang et al., 2016).",Threshold Calibration,
"There are several methods for selecting a Threshold (Zhang et al., 2016).",6306838,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Threshold Calibration,"To complete the prediction, we must convert the vector to binary values. ",Threshold Calibration,We used a constant threshold maximizing the overall F1-score.,Threshold Calibration,
We used a constant threshold maximizing the overall F1-score.,6306839,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Threshold Calibration,"There are several methods for selecting a Threshold (Zhang et al., 2016).",Threshold Calibration,"In future work, we could explore methods building a (linear) model on top of the intermediate vector.",Threshold Calibration,
"In future work, we could explore methods building a (linear) model on top of the intermediate vector.",6306840,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Threshold Calibration,We used a constant threshold maximizing the overall F1-score.,Threshold Calibration,We use the F1 metric on the validation data to evaluate performance in all models and compare results with previous work on classifying MIMIC clinical notes and text classification in general.,Performance Metrics,
We use the F1 metric on the validation data to evaluate performance in all models and compare results with previous work on classifying MIMIC clinical notes and text classification in general.,6306841,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Performance Metrics,"In future work, we could explore methods building a (linear) model on top of the intermediate vector.",Threshold Calibration,"In order to compare F1 performance results with the CNN model built by Gehrmann et al. (2017), we took into consideration the dataset size and number of classes. ",Comparing CNN with previous work,
"In order to compare F1 performance results with the CNN model built by Gehrmann et al. (2017), we took into consideration the dataset size and number of classes. ",6306842,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Comparing CNN with previous work,We use the F1 metric on the validation data to evaluate performance in all models and compare results with previous work on classifying MIMIC clinical notes and text classification in general.,Performance Metrics,Gehrmann's re-labeling approach is similar to our relabeling using the first-level ICD-9 codes in the ICD code hierarchy.,Comparing CNN with previous work,
Gehrmann's re-labeling approach is similar to our relabeling using the first-level ICD-9 codes in the ICD code hierarchy.,6306843,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Comparing CNN with previous work,"In order to compare F1 performance results with the CNN model built by Gehrmann et al. (2017), we took into consideration the dataset size and number of classes. ",Comparing CNN with previous work,"Even though we have access to 52.6K records, we use a subset to relate to the 1.6K records used by Gehrmann et al. (2017).",Comparing CNN with previous work,
"Even though we have access to 52.6K records, we use a subset to relate to the 1.6K records used by Gehrmann et al. (2017).",6306844,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Comparing CNN with previous work,Gehrmann's re-labeling approach is similar to our relabeling using the first-level ICD-9 codes in the ICD code hierarchy.,Comparing CNN with previous work,"Since we have 17 classes, 7 more than the ones used by Gehrmann , we run our model with a dataset of 5K records. ",Comparing CNN with previous work,
"Since we have 17 classes, 7 more than the ones used by Gehrmann , we run our model with a dataset of 5K records. ",6306845,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Comparing CNN with previous work,"Even though we have access to 52.6K records, we use a subset to relate to the 1.6K records used by Gehrmann et al. (2017).",Comparing CNN with previous work,"Our CNN obtains similar result to Gehrmann et al., with a F1 score of 76.2%, compare to their F1 score of 76% (see Table 1).",Comparing CNN with previous work,
"Our CNN obtains similar result to Gehrmann et al., with a F1 score of 76.2%, compare to their F1 score of 76% (see Table 1).",6306846,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Comparing CNN with previous work,"Since we have 17 classes, 7 more than the ones used by Gehrmann , we run our model with a dataset of 5K records. ",Comparing CNN with previous work,"To improve on this initial result, we ran experiments with the different models to identify the two more promising.","Testing CNN, LSTM and Attention",
"To improve on this initial result, we ran experiments with the different models to identify the two more promising.",6306847,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention","Our CNN obtains similar result to Gehrmann et al., with a F1 score of 76.2%, compare to their F1 score of 76% (see Table 1).",Comparing CNN with previous work,"These experiments run with a 5K notes, the 17 first level ICD-9 codes, using 5 epochs.","Testing CNN, LSTM and Attention",
"These experiments run with a 5K notes, the 17 first level ICD-9 codes, using 5 epochs.",6306848,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention","To improve on this initial result, we ran experiments with the different models to identify the two more promising.","Testing CNN, LSTM and Attention","The results are presented in Table 2 We tested LSTMs with and without attention mechanisms, CNN with and without attention mechanisms and a Hierarchical LSTM model with Attention layers. ","Testing CNN, LSTM and Attention",
"The results are presented in Table 2 We tested LSTMs with and without attention mechanisms, CNN with and without attention mechanisms and a Hierarchical LSTM model with Attention layers. ",6306849,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention","These experiments run with a 5K notes, the 17 first level ICD-9 codes, using 5 epochs.","Testing CNN, LSTM and Attention",From the results in Table 2 we can see that CNN models do perform better than LSTM on classifying the MIMIC medical notes.,"Testing CNN, LSTM and Attention",
From the results in Table 2 we can see that CNN models do perform better than LSTM on classifying the MIMIC medical notes.,6306850,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention","The results are presented in Table 2 We tested LSTMs with and without attention mechanisms, CNN with and without attention mechanisms and a Hierarchical LSTM model with Attention layers. ","Testing CNN, LSTM and Attention",We can also see that there is a significant improvement on F1 scores when applying attention mechanism to LSTM and CNN models.,"Testing CNN, LSTM and Attention",
We can also see that there is a significant improvement on F1 scores when applying attention mechanism to LSTM and CNN models.,6306851,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention",From the results in Table 2 we can see that CNN models do perform better than LSTM on classifying the MIMIC medical notes.,"Testing CNN, LSTM and Attention",The LSTM with Attention model outperforms the standard LSTM by 2.4% and the CNN with Attention model outperforms the standard CNN model by 3.8%.,"Testing CNN, LSTM and Attention",
The LSTM with Attention model outperforms the standard LSTM by 2.4% and the CNN with Attention model outperforms the standard CNN model by 3.8%.,6306852,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,"Testing CNN, LSTM and Attention",We can also see that there is a significant improvement on F1 scores when applying attention mechanism to LSTM and CNN models.,"Testing CNN, LSTM and Attention","On the other hand, the Hierarchical LSTM with Attention mechanisms had only a small increase (0.6%) in performance results on regards to the Flat LSTM with Attention.",Source,
"On the other hand, the Hierarchical LSTM with Attention mechanisms had only a small increase (0.6%) in performance results on regards to the Flat LSTM with Attention.",6306853,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,The LSTM with Attention model outperforms the standard LSTM by 2.4% and the CNN with Attention model outperforms the standard CNN model by 3.8%.,"Testing CNN, LSTM and Attention","This is smaller than we expected based on similar classification tasks by Yang et al. (2016), where a difference of 3% is reported, but on larger datasets.",Source,
"This is smaller than we expected based on similar classification tasks by Yang et al. (2016), where a difference of 3% is reported, but on larger datasets.",6306854,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"On the other hand, the Hierarchical LSTM with Attention mechanisms had only a small increase (0.6%) in performance results on regards to the Flat LSTM with Attention.",Source,"This model has twice the number of parameters than the flat models, which would impact performance for relatevely small files like the one we are using, this could be a reason for just a small improvement in the f1 score.",Source,
"This model has twice the number of parameters than the flat models, which would impact performance for relatevely small files like the one we are using, this could be a reason for just a small improvement in the f1 score.",6306855,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"This is smaller than we expected based on similar classification tasks by Yang et al. (2016), where a difference of 3% is reported, but on larger datasets.",Source,We also tried GRUs instead of LSTMs to compare with Yang et al.,Source,
We also tried GRUs instead of LSTMs to compare with Yang et al.,6306856,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"This model has twice the number of parameters than the flat models, which would impact performance for relatevely small files like the one we are using, this could be a reason for just a small improvement in the f1 score.",Source,(2016) results and the difference was still the same. ,Source,
(2016) results and the difference was still the same. ,6306857,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,We also tried GRUs instead of LSTMs to compare with Yang et al.,Source,Another possible reason our Hierarchical model is not performing much better is the tokenization of sentences.,Source,
Another possible reason our Hierarchical model is not performing much better is the tokenization of sentences.,6306858,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,(2016) results and the difference was still the same. ,Source,"The model bases its predictions on the results on each sentence, and if the sentences are not identified correctly in the first place, then the rest of the model will not perform well.",Source,
"The model bases its predictions on the results on each sentence, and if the sentences are not identified correctly in the first place, then the rest of the model will not perform well.",6306859,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,Another possible reason our Hierarchical model is not performing much better is the tokenization of sentences.,Source,"We did inspect suspicious long sentences which were not incorrect, they were lab reports.",Source,
"We did inspect suspicious long sentences which were not incorrect, they were lab reports.",6306860,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"The model bases its predictions on the results on each sentence, and if the sentences are not identified correctly in the first place, then the rest of the model will not perform well.",Source,We would inspect closely the sentence tokenization process in further work. ,Source,
We would inspect closely the sentence tokenization process in further work. ,6306861,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"We did inspect suspicious long sentences which were not incorrect, they were lab reports.",Source,"The two most promising models are CNN and CNN with Attention, even the standard CNN model outperforms the Hierarchical model. ",Source,
"The two most promising models are CNN and CNN with Attention, even the standard CNN model outperforms the Hierarchical model. ",6306862,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,We would inspect closely the sentence tokenization process in further work. ,Source,CNN models could be seen as hierarchical: the convolutional sliding windows create segments of the document (like sentences do) and they are collapsed into vectors representing a higher level of abstraction.,Source,
CNN models could be seen as hierarchical: the convolutional sliding windows create segments of the document (like sentences do) and they are collapsed into vectors representing a higher level of abstraction.,6306863,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"The two most promising models are CNN and CNN with Attention, even the standard CNN model outperforms the Hierarchical model. ",Source,In as sense CNN are finding the best segments in the document regardless of sentences separations.,Source,
In as sense CNN are finding the best segments in the document regardless of sentences separations.,6306864,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,CNN models could be seen as hierarchical: the convolutional sliding windows create segments of the document (like sentences do) and they are collapsed into vectors representing a higher level of abstraction.,Source,This may explain why the CNN models are getting a better performance than the Hierarchical models.,Source,
This may explain why the CNN models are getting a better performance than the Hierarchical models.,6306865,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,In as sense CNN are finding the best segments in the document regardless of sentences separations.,Source,Here we show results from running the CNN models with the full data set. ,CNN performance with full data set,
Here we show results from running the CNN models with the full data set. ,6306866,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,CNN performance with full data set,This may explain why the CNN models are getting a better performance than the Hierarchical models.,Source,First we classify clinical notes into the 20 most common Level 5 ICD-9 codes for comparison purposes: we can see that our model outperformed previous work (see Table 3).,CNN performance with full data set,
First we classify clinical notes into the 20 most common Level 5 ICD-9 codes for comparison purposes: we can see that our model outperformed previous work (see Table 3).,6306867,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,CNN performance with full data set,Here we show results from running the CNN models with the full data set. ,CNN performance with full data set,"However at this stage, the CNN ATT model still overfits: even though it had the highest score during the experimental runs with 5K records and 5 epochs, it didn't reach the best f1-score when running it with the full data set.",CNN performance with full data set,
"However at this stage, the CNN ATT model still overfits: even though it had the highest score during the experimental runs with 5K records and 5 epochs, it didn't reach the best f1-score when running it with the full data set.",6306868,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,CNN performance with full data set,First we classify clinical notes into the 20 most common Level 5 ICD-9 codes for comparison purposes: we can see that our model outperformed previous work (see Table 3).,CNN performance with full data set,Further work would explore hyperparameters tuning and evaluating the number of parameters to attempt undoing the over fitting situation.,CNN performance with full data set,
Further work would explore hyperparameters tuning and evaluating the number of parameters to attempt undoing the over fitting situation.,6306869,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,CNN performance with full data set,"However at this stage, the CNN ATT model still overfits: even though it had the highest score during the experimental runs with 5K records and 5 epochs, it didn't reach the best f1-score when running it with the full data set.",CNN performance with full data set,"We believe the CNN models can still be improved by inspecting in more detail cases where the model predicted a false positive or false negative, and working on hyper-parameters.",Source,
"We believe the CNN models can still be improved by inspecting in more detail cases where the model predicted a false positive or false negative, and working on hyper-parameters.",6306870,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,Further work would explore hyperparameters tuning and evaluating the number of parameters to attempt undoing the over fitting situation.,CNN performance with full data set,This would be one of the first tasks to do in further work regarding these models.,Source,
This would be one of the first tasks to do in further work regarding these models.,6306871,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Source,"We believe the CNN models can still be improved by inspecting in more detail cases where the model predicted a false positive or false negative, and working on hyper-parameters.",Source,"We used trainable embeddings, as described in Section 3.3.",Pre-trained Embeddings,
"We used trainable embeddings, as described in Section 3.3.",6306872,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,This would be one of the first tasks to do in further work regarding these models.,Source,Our two attempts to initialize them with pretrained values were unsuccessful. ,Pre-trained Embeddings,
Our two attempts to initialize them with pretrained values were unsuccessful. ,6306873,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,"We used trainable embeddings, as described in Section 3.3.",Pre-trained Embeddings,"Using the Wiki Glove pre-trained embeddings led to a minor decrease in performance (about 0.001%) compared to an empty embedding matrix, which could be expected since Medical clinical notes have a vocabulary that differs from most Wiki pages.",Pre-trained Embeddings,
"Using the Wiki Glove pre-trained embeddings led to a minor decrease in performance (about 0.001%) compared to an empty embedding matrix, which could be expected since Medical clinical notes have a vocabulary that differs from most Wiki pages.",6306874,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,Our two attempts to initialize them with pretrained values were unsuccessful. ,Pre-trained Embeddings,"In fact, half of our vocabulary was not found on the Wiki Glove pretrained embeddings. ",Pre-trained Embeddings,
"In fact, half of our vocabulary was not found on the Wiki Glove pretrained embeddings. ",6306875,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,"Using the Wiki Glove pre-trained embeddings led to a minor decrease in performance (about 0.001%) compared to an empty embedding matrix, which could be expected since Medical clinical notes have a vocabulary that differs from most Wiki pages.",Pre-trained Embeddings,We then created our own pre-trained embeddings using the Glove algorithm on all the MIMIC discharge notes.,Pre-trained Embeddings,
We then created our own pre-trained embeddings using the Glove algorithm on all the MIMIC discharge notes.,6306876,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,"In fact, half of our vocabulary was not found on the Wiki Glove pretrained embeddings. ",Pre-trained Embeddings,The result was a small performance improvement of 0.01%. ,Pre-trained Embeddings,
The result was a small performance improvement of 0.01%. ,6306877,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,We then created our own pre-trained embeddings using the Glove algorithm on all the MIMIC discharge notes.,Pre-trained Embeddings,"As part of future work, we think that using pretrained embeddings on millions of clinical notes would improve the performance of models processing clinical notes, this is an example of such type of work 16 .",Pre-trained Embeddings,
"As part of future work, we think that using pretrained embeddings on millions of clinical notes would improve the performance of models processing clinical notes, this is an example of such type of work 16 .",6306878,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Pre-trained Embeddings,The result was a small performance improvement of 0.01%. ,Pre-trained Embeddings,"In this paper, we tested several alternative approaches for classifying ICD-9 codes. ",Conclusions and outlook,
"In this paper, we tested several alternative approaches for classifying ICD-9 codes. ",6306879,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Conclusions and outlook,"As part of future work, we think that using pretrained embeddings on millions of clinical notes would improve the performance of models processing clinical notes, this is an example of such type of work 16 .",Pre-trained Embeddings,"We showed that our a CNN models outperform significantly the F1 scores reported by previous work on Level 1 or Level 5 codes, while LSTMs and Hierarchical model displayed lower performance. ",Conclusions and outlook,
"We showed that our a CNN models outperform significantly the F1 scores reported by previous work on Level 1 or Level 5 codes, while LSTMs and Hierarchical model displayed lower performance. ",6306880,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Conclusions and outlook,"In this paper, we tested several alternative approaches for classifying ICD-9 codes. ",Conclusions and outlook,"However for the problem of automatic labeling to be solved, models need to increase both in performance and in the precision of the codes that they allocate.",Conclusions and outlook,
"However for the problem of automatic labeling to be solved, models need to increase both in performance and in the precision of the codes that they allocate.",6306881,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Conclusions and outlook,"We showed that our a CNN models outperform significantly the F1 scores reported by previous work on Level 1 or Level 5 codes, while LSTMs and Hierarchical model displayed lower performance. ",Conclusions and outlook,"Our results highlight several areas to further that goal: • optimization of CNN model with Attention, given promising results on small datasets • better adapt embeddings to clinical notes • broaden the number of ICD codes, gradually or by adapting the training metric Finally we note that ICD codes are associated with a textual definition which could be directly compared with the clinical notes themselves.",Conclusions and outlook,
"Our results highlight several areas to further that goal: • optimization of CNN model with Attention, given promising results on small datasets • better adapt embeddings to clinical notes • broaden the number of ICD codes, gradually or by adapting the training metric Finally we note that ICD codes are associated with a textual definition which could be directly compared with the clinical notes themselves.",6306882,"Classifying medical notes into standard disease codes using Machine
  Learning",2018,Conclusions and outlook,"However for the problem of automatic labeling to be solved, models need to increase both in performance and in the precision of the codes that they allocate.",Conclusions and outlook,,,
"  Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).",6429967,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,abstract,,,"When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. ",abstract,
"When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. ",6429968,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,abstract,"  Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).",abstract,"While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents.",abstract,
"While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents.",6429969,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,abstract,"When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. ",abstract,"This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated.",abstract,
"This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated.",6429970,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,abstract,"While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents.",abstract,"Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. ",abstract,
"Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. ",6429971,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,abstract,"This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated.",abstract,"Natural language has a sequential overt form as spoken and written, but the underlying structure of language is not strictly sequential.",INTRODUCTION,
"Natural language has a sequential overt form as spoken and written, but the underlying structure of language is not strictly sequential.",6429972,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. ",abstract,This structure is usually tree-like.,INTRODUCTION,
This structure is usually tree-like.,6429973,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Natural language has a sequential overt form as spoken and written, but the underlying structure of language is not strictly sequential.",INTRODUCTION,"Linguists agree on a set of rules, or syntax, that determine this structure (Chomsky, 1956;1965;Sandra & Taft, 2014) and dictate how single words compose to form meaningful larger units, also called ""constituents"" (Koopman et al., 2013).",INTRODUCTION,
"Linguists agree on a set of rules, or syntax, that determine this structure (Chomsky, 1956;1965;Sandra & Taft, 2014) and dictate how single words compose to form meaningful larger units, also called ""constituents"" (Koopman et al., 2013).",6429974,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,This structure is usually tree-like.,INTRODUCTION,"The human brain can also implicitly acquire the latent structure of language (Dehaene et al., 2015): during language acquisition, children are not given annotated parse trees.",INTRODUCTION,
"The human brain can also implicitly acquire the latent structure of language (Dehaene et al., 2015): during language acquisition, children are not given annotated parse trees.",6429975,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Linguists agree on a set of rules, or syntax, that determine this structure (Chomsky, 1956;1965;Sandra & Taft, 2014) and dictate how single words compose to form meaningful larger units, also called ""constituents"" (Koopman et al., 2013).",INTRODUCTION,"This observation brings more interest in latent structure induction with artificial neural network approaches, which are inspired by information processing and communication patterns in biological nervous systems.",INTRODUCTION,
"This observation brings more interest in latent structure induction with artificial neural network approaches, which are inspired by information processing and communication patterns in biological nervous systems.",6429976,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"The human brain can also implicitly acquire the latent structure of language (Dehaene et al., 2015): during language acquisition, children are not given annotated parse trees.",INTRODUCTION,"From a practical point of view, integrating a tree structure into a neural network language model may be important for multiple reasons: (i) to obtain a hierarchical representation with increasing levels of abstraction, a key feature of deep neural networks (Bengio et al., 2009;LeCun et al., 2015;Schmidhuber, 2015); (ii) to model the compositional effects of language (Koopman et al., 2013;Socher et al., 2013) and help with the long-term dependency problem (Bengio et al., 2009;Tai et al., 2015) by providing shortcuts for gradient backpropagation (Chung et al., 2016); (iii) to improve generalization via a better inductive bias and at the same time potentially reducing the need of a large amount of training data. ",INTRODUCTION,
"From a practical point of view, integrating a tree structure into a neural network language model may be important for multiple reasons: (i) to obtain a hierarchical representation with increasing levels of abstraction, a key feature of deep neural networks (Bengio et al., 2009;LeCun et al., 2015;Schmidhuber, 2015); (ii) to model the compositional effects of language (Koopman et al., 2013;Socher et al., 2013) and help with the long-term dependency problem (Bengio et al., 2009;Tai et al., 2015) by providing shortcuts for gradient backpropagation (Chung et al., 2016); (iii) to improve generalization via a better inductive bias and at the same time potentially reducing the need of a large amount of training data. ",6429977,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"This observation brings more interest in latent structure induction with artificial neural network approaches, which are inspired by information processing and communication patterns in biological nervous systems.",INTRODUCTION,The study of deep neural network techniques that can infer and use tree structures to form better representations of natural language sentences has received a great deal of attention in recent Figure 1: Binary parse tree inferred by our model (left) and its corresponding ground-truth (right). ,INTRODUCTION,
The study of deep neural network techniques that can infer and use tree structures to form better representations of natural language sentences has received a great deal of attention in recent Figure 1: Binary parse tree inferred by our model (left) and its corresponding ground-truth (right). ,6429978,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"From a practical point of view, integrating a tree structure into a neural network language model may be important for multiple reasons: (i) to obtain a hierarchical representation with increasing levels of abstraction, a key feature of deep neural networks (Bengio et al., 2009;LeCun et al., 2015;Schmidhuber, 2015); (ii) to model the compositional effects of language (Koopman et al., 2013;Socher et al., 2013) and help with the long-term dependency problem (Bengio et al., 2009;Tai et al., 2015) by providing shortcuts for gradient backpropagation (Chung et al., 2016); (iii) to improve generalization via a better inductive bias and at the same time potentially reducing the need of a large amount of training data. ",INTRODUCTION,"years (Bowman et al., 2016;Yogatama et al., 2016;Shen et al., 2017;Jacob et al., 2018;Choi et al., 2018;Williams et al., 2018;Shi et al., 2018). ",INTRODUCTION,
"years (Bowman et al., 2016;Yogatama et al., 2016;Shen et al., 2017;Jacob et al., 2018;Choi et al., 2018;Williams et al., 2018;Shi et al., 2018). ",6429979,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,The study of deep neural network techniques that can infer and use tree structures to form better representations of natural language sentences has received a great deal of attention in recent Figure 1: Binary parse tree inferred by our model (left) and its corresponding ground-truth (right). ,INTRODUCTION,"Given a sentence, one straightforward way of predicting the corresponding latent tree structure is through a supervised syntactic parser.",INTRODUCTION,
"Given a sentence, one straightforward way of predicting the corresponding latent tree structure is through a supervised syntactic parser.",6429980,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"years (Bowman et al., 2016;Yogatama et al., 2016;Shen et al., 2017;Jacob et al., 2018;Choi et al., 2018;Williams et al., 2018;Shi et al., 2018). ",INTRODUCTION,"Trees produced by these parsers have been used to guide the composition of word semantics into sentence semantics (Socher et al., 2013;Bowman et al., 2015), or even to help next word prediction given previous words (Wu et al., 2017).",INTRODUCTION,
"Trees produced by these parsers have been used to guide the composition of word semantics into sentence semantics (Socher et al., 2013;Bowman et al., 2015), or even to help next word prediction given previous words (Wu et al., 2017).",6429981,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Given a sentence, one straightforward way of predicting the corresponding latent tree structure is through a supervised syntactic parser.",INTRODUCTION,"However, supervised parsers are limiting for several reasons: i) few languages have comprehensive annotated data for supervised parser training; ii) in some domains, syntax rules tend to be broken (e.g. in tweets); and iii) languages change over time with use, so syntax rules may evolve. ",INTRODUCTION,
"However, supervised parsers are limiting for several reasons: i) few languages have comprehensive annotated data for supervised parser training; ii) in some domains, syntax rules tend to be broken (e.g. in tweets); and iii) languages change over time with use, so syntax rules may evolve. ",6429982,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Trees produced by these parsers have been used to guide the composition of word semantics into sentence semantics (Socher et al., 2013;Bowman et al., 2015), or even to help next word prediction given previous words (Wu et al., 2017).",INTRODUCTION,"On the other hand, grammar induction, defined as the task of learning the syntactic structure from raw corpora without access to expert-labeled data, remains an open problem.",INTRODUCTION,
"On the other hand, grammar induction, defined as the task of learning the syntactic structure from raw corpora without access to expert-labeled data, remains an open problem.",6429983,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"However, supervised parsers are limiting for several reasons: i) few languages have comprehensive annotated data for supervised parser training; ii) in some domains, syntax rules tend to be broken (e.g. in tweets); and iii) languages change over time with use, so syntax rules may evolve. ",INTRODUCTION,"Many such recent attempts suffer from inducing a trivial structure (e.g., a left-branching or right-branching tree (Williams et al., 2018)), or encounter difficulties in training caused by learning branching policies with Reinforcement Learning (RL) (Yogatama et al., 2016).",INTRODUCTION,
"Many such recent attempts suffer from inducing a trivial structure (e.g., a left-branching or right-branching tree (Williams et al., 2018)), or encounter difficulties in training caused by learning branching policies with Reinforcement Learning (RL) (Yogatama et al., 2016).",6429984,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"On the other hand, grammar induction, defined as the task of learning the syntactic structure from raw corpora without access to expert-labeled data, remains an open problem.",INTRODUCTION,"Furthermore, some methods are relatively complex to implement and train, like the PRPN model proposed in Shen et al. (2017). ",INTRODUCTION,
"Furthermore, some methods are relatively complex to implement and train, like the PRPN model proposed in Shen et al. (2017). ",6429985,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Many such recent attempts suffer from inducing a trivial structure (e.g., a left-branching or right-branching tree (Williams et al., 2018)), or encounter difficulties in training caused by learning branching policies with Reinforcement Learning (RL) (Yogatama et al., 2016).",INTRODUCTION,"Recurrent neural networks (RNNs) have proven highly effective at the task of language modeling (Merity et al., 2017;Melis et al., 2017).",INTRODUCTION,
"Recurrent neural networks (RNNs) have proven highly effective at the task of language modeling (Merity et al., 2017;Melis et al., 2017).",6429986,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Furthermore, some methods are relatively complex to implement and train, like the PRPN model proposed in Shen et al. (2017). ",INTRODUCTION,RNNs explicitly impose a chain structure on the data.,INTRODUCTION,
RNNs explicitly impose a chain structure on the data.,6429987,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Recurrent neural networks (RNNs) have proven highly effective at the task of language modeling (Merity et al., 2017;Melis et al., 2017).",INTRODUCTION,"This assumption may seem at odds with the latent non-sequential structure of language and may pose several difficulties for the processing of natural language data with deep learning methods, giving rise to problems such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",INTRODUCTION,
"This assumption may seem at odds with the latent non-sequential structure of language and may pose several difficulties for the processing of natural language data with deep learning methods, giving rise to problems such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",6429988,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,RNNs explicitly impose a chain structure on the data.,INTRODUCTION,"Meanwhile, some evidence exists that LSTMs with sufficient capacity potentially implement syntactic processing mechanisms by encoding the tree structure implicitly, as shown by Gulordava et al. (2018); Kuncoro et al.",INTRODUCTION,
"Meanwhile, some evidence exists that LSTMs with sufficient capacity potentially implement syntactic processing mechanisms by encoding the tree structure implicitly, as shown by Gulordava et al. (2018); Kuncoro et al.",6429989,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"This assumption may seem at odds with the latent non-sequential structure of language and may pose several difficulties for the processing of natural language data with deep learning methods, giving rise to problems such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",INTRODUCTION,(2018) and very recently by Lakretz et al. (2019).,INTRODUCTION,
(2018) and very recently by Lakretz et al. (2019).,6429990,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Meanwhile, some evidence exists that LSTMs with sufficient capacity potentially implement syntactic processing mechanisms by encoding the tree structure implicitly, as shown by Gulordava et al. (2018); Kuncoro et al.",INTRODUCTION,We believe that the following question remains: Can better models of language be obtained by architectures equipped with an inductive bias towards learning such latent tree structures? ,INTRODUCTION,
We believe that the following question remains: Can better models of language be obtained by architectures equipped with an inductive bias towards learning such latent tree structures? ,6429991,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,(2018) and very recently by Lakretz et al. (2019).,INTRODUCTION,"In this work, we introduce ordered neurons, a new inductive bias for recurrent neural networks.",INTRODUCTION,
"In this work, we introduce ordered neurons, a new inductive bias for recurrent neural networks.",6429992,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,We believe that the following question remains: Can better models of language be obtained by architectures equipped with an inductive bias towards learning such latent tree structures? ,INTRODUCTION,"This inductive bias promotes differentiation of the life cycle of information stored inside each neuron: high-ranking neurons will store long-term information which is kept for a large number of steps, while low-ranking neurons will store short-term information that can be rapidly forgotten.",INTRODUCTION,
"This inductive bias promotes differentiation of the life cycle of information stored inside each neuron: high-ranking neurons will store long-term information which is kept for a large number of steps, while low-ranking neurons will store short-term information that can be rapidly forgotten.",6429993,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"In this work, we introduce ordered neurons, a new inductive bias for recurrent neural networks.",INTRODUCTION,"To avoid a strict division between high-ranking and low-ranking neurons, we propose a new activation function, the cumulative softmax, or cumax(), to actively allocate neurons to store long/short-term information.",INTRODUCTION,
"To avoid a strict division between high-ranking and low-ranking neurons, we propose a new activation function, the cumulative softmax, or cumax(), to actively allocate neurons to store long/short-term information.",6429994,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"This inductive bias promotes differentiation of the life cycle of information stored inside each neuron: high-ranking neurons will store long-term information which is kept for a large number of steps, while low-ranking neurons will store short-term information that can be rapidly forgotten.",INTRODUCTION,"We use the cumax() function to produce a vector of master input and forget gates ensuring that when a given neuron is updated (erased), all of the neurons that follow it in the ordering are also updated (erased).",INTRODUCTION,
"We use the cumax() function to produce a vector of master input and forget gates ensuring that when a given neuron is updated (erased), all of the neurons that follow it in the ordering are also updated (erased).",6429995,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"To avoid a strict division between high-ranking and low-ranking neurons, we propose a new activation function, the cumulative softmax, or cumax(), to actively allocate neurons to store long/short-term information.",INTRODUCTION,"Based on the cumax() and the LSTM architecture, we have designed a new model, ON-LSTM, that is biased towards performing tree-like composition operations.",INTRODUCTION,
"Based on the cumax() and the LSTM architecture, we have designed a new model, ON-LSTM, that is biased towards performing tree-like composition operations.",6429996,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"We use the cumax() function to produce a vector of master input and forget gates ensuring that when a given neuron is updated (erased), all of the neurons that follow it in the ordering are also updated (erased).",INTRODUCTION,"Our model achieves good performance on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation (Marvin & Linzen, 2018) and logical inference (Bowman et al., 2015).",INTRODUCTION,
"Our model achieves good performance on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation (Marvin & Linzen, 2018) and logical inference (Bowman et al., 2015).",6429997,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Based on the cumax() and the LSTM architecture, we have designed a new model, ON-LSTM, that is biased towards performing tree-like composition operations.",INTRODUCTION,The result on unsupervised constituency parsing suggests that the proposed inductive bias aligns with the syntax principles proposed by human experts better than previously proposed models.,INTRODUCTION,
The result on unsupervised constituency parsing suggests that the proposed inductive bias aligns with the syntax principles proposed by human experts better than previously proposed models.,6429998,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,"Our model achieves good performance on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation (Marvin & Linzen, 2018) and logical inference (Bowman et al., 2015).",INTRODUCTION,The experiments also show that ON-LSTM performs better than standard LSTM models in tasks requiring capturing long-term dependencies and achieves better generalization to longer sequences.,INTRODUCTION,
The experiments also show that ON-LSTM performs better than standard LSTM models in tasks requiring capturing long-term dependencies and achieves better generalization to longer sequences.,6429999,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,INTRODUCTION,The result on unsupervised constituency parsing suggests that the proposed inductive bias aligns with the syntax principles proposed by human experts better than previously proposed models.,INTRODUCTION,There has been prior work leveraging tree structures for natural language tasks in the literature.,RELATED WORK,
"Given a sequence of tokens S = (x 1 , . . .",6430000,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"Therefore, our model can adapt the structure to the observed data, while both Clockwork RNN and nested dropout impose a predefined hierarchy to hidden representations.",RELATED WORK,", x T ) and its corresponding constituency tree (Figure 2(a)), our goal is to infer the unobserved tree structure while processing the observed sequence, i.e. while computing the hidden state h t for each time step t.",ORDERED NEURONS,
", x T ) and its corresponding constituency tree (Figure 2(a)), our goal is to infer the unobserved tree structure while processing the observed sequence, i.e. while computing the hidden state h t for each time step t.",6430001,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"Given a sequence of tokens S = (x 1 , . . .",ORDERED NEURONS,"At each time step, h t would ideally contain a information about all the nodes on the path between the current leaf node x t and the root S. In Figure 2(c), we illustrate how h t would contain information about all the constituents that include the current token x t even if those are only partially observed.",ORDERED NEURONS,
"At each time step, h t would ideally contain a information about all the nodes on the path between the current leaf node x t and the root S. In Figure 2(c), we illustrate how h t would contain information about all the constituents that include the current token x t even if those are only partially observed.",6430002,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,", x T ) and its corresponding constituency tree (Figure 2(a)), our goal is to infer the unobserved tree structure while processing the observed sequence, i.e. while computing the hidden state h t for each time step t.",ORDERED NEURONS,This intuition suggests that each node in the tree can be represented by a set of neurons in the hidden states.,ORDERED NEURONS,
This intuition suggests that each node in the tree can be represented by a set of neurons in the hidden states.,6430003,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"At each time step, h t would ideally contain a information about all the nodes on the path between the current leaf node x t and the root S. In Figure 2(c), we illustrate how h t would contain information about all the constituents that include the current token x t even if those are only partially observed.",ORDERED NEURONS,"However, while the dimensionality of the hidden state is fixed in advance, the length of the path connecting the leaf to the root of the tree may be different across different time steps and sentences.",ORDERED NEURONS,
"However, while the dimensionality of the hidden state is fixed in advance, the length of the path connecting the leaf to the root of the tree may be different across different time steps and sentences.",6430004,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,This intuition suggests that each node in the tree can be represented by a set of neurons in the hidden states.,ORDERED NEURONS,"Therefore, a desiderata for the model is to dynamically reallocate the dimensions of the hidden state to each node. ",ORDERED NEURONS,
"Therefore, a desiderata for the model is to dynamically reallocate the dimensions of the hidden state to each node. ",6430005,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"However, while the dimensionality of the hidden state is fixed in advance, the length of the path connecting the leaf to the root of the tree may be different across different time steps and sentences.",ORDERED NEURONS,"Given these requirements, we introduce ordered neurons, an inductive bias that forces neurons to represent information at different time-scales.",ORDERED NEURONS,
"Given these requirements, we introduce ordered neurons, an inductive bias that forces neurons to represent information at different time-scales.",6430006,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"Therefore, a desiderata for the model is to dynamically reallocate the dimensions of the hidden state to each node. ",ORDERED NEURONS,"In our model, high-ranking neurons contain long-term or global information that will last anywhere from several time steps to the entire sentence, representing nodes near the root of the tree.",ORDERED NEURONS,
"In our model, high-ranking neurons contain long-term or global information that will last anywhere from several time steps to the entire sentence, representing nodes near the root of the tree.",6430007,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"Given these requirements, we introduce ordered neurons, an inductive bias that forces neurons to represent information at different time-scales.",ORDERED NEURONS,"Low-ranking neurons encode short-term or local information that only last one or a few time steps, representing smaller constituents, as shown in Figure 2(b).",ORDERED NEURONS,
"Low-ranking neurons encode short-term or local information that only last one or a few time steps, representing smaller constituents, as shown in Figure 2(b).",6430008,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"In our model, high-ranking neurons contain long-term or global information that will last anywhere from several time steps to the entire sentence, representing nodes near the root of the tree.",ORDERED NEURONS,"The differentiation between high-ranking and low-ranking neurons is learnt in a completely data-driven fashion by controlling the update frequency of single neurons: to erase (or update) high-ranking neurons, the model should first erase (or update) all lower-ranking neurons.",ORDERED NEURONS,
"The differentiation between high-ranking and low-ranking neurons is learnt in a completely data-driven fashion by controlling the update frequency of single neurons: to erase (or update) high-ranking neurons, the model should first erase (or update) all lower-ranking neurons.",6430009,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"Low-ranking neurons encode short-term or local information that only last one or a few time steps, representing smaller constituents, as shown in Figure 2(b).",ORDERED NEURONS,"In other words, some neurons always update more (or less) frequently than the others, and that order is pre-determined as part of the model architecture.",ORDERED NEURONS,
"In other words, some neurons always update more (or less) frequently than the others, and that order is pre-determined as part of the model architecture.",6430010,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ORDERED NEURONS,"The differentiation between high-ranking and low-ranking neurons is learnt in a completely data-driven fashion by controlling the update frequency of single neurons: to erase (or update) high-ranking neurons, the model should first erase (or update) all lower-ranking neurons.",ORDERED NEURONS,"In this section, we present a new RNN unit, ON-LSTM (""ordered neurons LSTM"").",ON-LSTM,
"In this section, we present a new RNN unit, ON-LSTM (""ordered neurons LSTM"").",6430011,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,"In other words, some neurons always update more (or less) frequently than the others, and that order is pre-determined as part of the model architecture.",ORDERED NEURONS,"The new model uses an architecture similar to the standard LSTM, reported below: f t = σ(W f x t + U f h t−1 + b f )",ON-LSTM,
"The new model uses an architecture similar to the standard LSTM, reported below: f t = σ(W f x t + U f h t−1 + b f )",6430012,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,"In this section, we present a new RNN unit, ON-LSTM (""ordered neurons LSTM"").",ON-LSTM,The difference with the LSTM is that we replace the update function for the cell state c t with a new function that will be explained in the following sections.,ON-LSTM,
The difference with the LSTM is that we replace the update function for the cell state c t with a new function that will be explained in the following sections.,6430013,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,"The new model uses an architecture similar to the standard LSTM, reported below: f t = σ(W f x t + U f h t−1 + b f )",ON-LSTM,"The forget gates f t and input gates i t are used to control the erasing and writing operation on cell states c t , as before.",ON-LSTM,
"The forget gates f t and input gates i t are used to control the erasing and writing operation on cell states c t , as before.",6430014,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,The difference with the LSTM is that we replace the update function for the cell state c t with a new function that will be explained in the following sections.,ON-LSTM,"Since the gates in the LSTM act independently on each neuron, it may be difficult in general to discern a hierarchy of information between the neurons.",ON-LSTM,
"Since the gates in the LSTM act independently on each neuron, it may be difficult in general to discern a hierarchy of information between the neurons.",6430015,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,"The forget gates f t and input gates i t are used to control the erasing and writing operation on cell states c t , as before.",ON-LSTM,"To this end, we propose to make the gate for each neuron dependent on the others by enforcing the order in which neurons should be updated.",ON-LSTM,
"To this end, we propose to make the gate for each neuron dependent on the others by enforcing the order in which neurons should be updated.",6430016,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ON-LSTM,"Since the gates in the LSTM act independently on each neuron, it may be difficult in general to discern a hierarchy of information between the neurons.",ON-LSTM,"To enforce an order to the update frequency, we introduce a new activation function: ĝ = cumax(. . .)",ACTIVATION FUNCTION: cumax(),
"To enforce an order to the update frequency, we introduce a new activation function: ĝ = cumax(. . .)",6430017,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"To this end, we propose to make the gate for each neuron dependent on the others by enforcing the order in which neurons should be updated.",ON-LSTM,"= cumsum(softmax(. . .)), where cumsum denotes the cumulative sum.",ACTIVATION FUNCTION: cumax(),
"= cumsum(softmax(. . .)), where cumsum denotes the cumulative sum.",6430018,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"To enforce an order to the update frequency, we introduce a new activation function: ĝ = cumax(. . .)",ACTIVATION FUNCTION: cumax(),"We will show that the vector ĝ can be seen as the expectation of a binary gate g = (0, ..., 0, 1, ..., 1).",ACTIVATION FUNCTION: cumax(),
"We will show that the vector ĝ can be seen as the expectation of a binary gate g = (0, ..., 0, 1, ..., 1).",6430019,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"= cumsum(softmax(. . .)), where cumsum denotes the cumulative sum.",ACTIVATION FUNCTION: cumax(),This binary gate splits the cell state into two segments: the 0-segment and the 1-segment.,ACTIVATION FUNCTION: cumax(),
This binary gate splits the cell state into two segments: the 0-segment and the 1-segment.,6430020,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"We will show that the vector ĝ can be seen as the expectation of a binary gate g = (0, ..., 0, 1, ..., 1).",ACTIVATION FUNCTION: cumax(),"Thus, the model can apply different update rules on the two segments to differentiate long/short-term information.",ACTIVATION FUNCTION: cumax(),
"Thus, the model can apply different update rules on the two segments to differentiate long/short-term information.",6430021,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),This binary gate splits the cell state into two segments: the 0-segment and the 1-segment.,ACTIVATION FUNCTION: cumax(),Denote by d a categorical random variable representing the index for the first 1 in g: p(d) = softmax(. . .) ,ACTIVATION FUNCTION: cumax(),
Denote by d a categorical random variable representing the index for the first 1 in g: p(d) = softmax(. . .) ,6430022,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"Thus, the model can apply different update rules on the two segments to differentiate long/short-term information.",ACTIVATION FUNCTION: cumax(),The variable d represents the split point between the two segments.,ACTIVATION FUNCTION: cumax(),
The variable d represents the split point between the two segments.,6430023,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),Denote by d a categorical random variable representing the index for the first 1 in g: p(d) = softmax(. . .) ,ACTIVATION FUNCTION: cumax(),"We can compute the probability of the k-th value in g being 1 by evaluating the probability of the disjunction of any of the values before the k-th being the split point, that is d ≤ k = (d = 0) ∨",ACTIVATION FUNCTION: cumax(),
"We can compute the probability of the k-th value in g being 1 by evaluating the probability of the disjunction of any of the values before the k-th being the split point, that is d ≤ k = (d = 0) ∨",6430024,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),The variable d represents the split point between the two segments.,ACTIVATION FUNCTION: cumax(),"Since the categories are mutually exclusive, we can do this by computing the cumulative distribution function: p(g k = 1) = p(d ≤ k) = i≤k p(d",ACTIVATION FUNCTION: cumax(),
"Since the categories are mutually exclusive, we can do this by computing the cumulative distribution function: p(g k = 1) = p(d ≤ k) = i≤k p(d",6430025,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"We can compute the probability of the k-th value in g being 1 by evaluating the probability of the disjunction of any of the values before the k-th being the split point, that is d ≤ k = (d = 0) ∨",ACTIVATION FUNCTION: cumax(),"Ideally, g should take the form of a discrete variable.",ACTIVATION FUNCTION: cumax(),
"Ideally, g should take the form of a discrete variable.",6430026,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"Since the categories are mutually exclusive, we can do this by computing the cumulative distribution function: p(g k = 1) = p(d ≤ k) = i≤k p(d",ACTIVATION FUNCTION: cumax(),"Unfortunately, computing gradients when a discrete variable is included in the computation graph is not trivial (Schulman et al., 2015), so in practice we use a continuous relaxation by computing the quantity p(d ≤ k), obtained by taking a cumulative sum of the softmax.",ACTIVATION FUNCTION: cumax(),
"Unfortunately, computing gradients when a discrete variable is included in the computation graph is not trivial (Schulman et al., 2015), so in practice we use a continuous relaxation by computing the quantity p(d ≤ k), obtained by taking a cumulative sum of the softmax.",6430027,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"Ideally, g should take the form of a discrete variable.",ACTIVATION FUNCTION: cumax(),"As g k is binary, this is equivalent to computing E[g k ].",ACTIVATION FUNCTION: cumax(),
"As g k is binary, this is equivalent to computing E[g k ].",6430028,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,ACTIVATION FUNCTION: cumax(),"Unfortunately, computing gradients when a discrete variable is included in the computation graph is not trivial (Schulman et al., 2015), so in practice we use a continuous relaxation by computing the quantity p(d ≤ k), obtained by taking a cumulative sum of the softmax.",ACTIVATION FUNCTION: cumax(),"Based on the cumax() function, we introduce a master forget gate ft and a master input gate ĩt : ft = cumax(W f x t",STRUCTURED GATING MECHANISM,
"Based on the cumax() function, we introduce a master forget gate ft and a master input gate ĩt : ft = cumax(W f x t",6430029,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"As g k is binary, this is equivalent to computing E[g k ].",ACTIVATION FUNCTION: cumax(),"+ U ĩh t−1 + b ĩ) (10) Following the properties of the cumax() activation, the values in the master forget gate are monotonically increasing from 0 to 1, and those in the master input gate are monotonically decreasing from 1 to 0.",STRUCTURED GATING MECHANISM,
"+ U ĩh t−1 + b ĩ) (10) Following the properties of the cumax() activation, the values in the master forget gate are monotonically increasing from 0 to 1, and those in the master input gate are monotonically decreasing from 1 to 0.",6430030,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Based on the cumax() function, we introduce a master forget gate ft and a master input gate ĩt : ft = cumax(W f x t",STRUCTURED GATING MECHANISM,These gates serve as high-level control for the update operations of cell states.,STRUCTURED GATING MECHANISM,
These gates serve as high-level control for the update operations of cell states.,6430031,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"+ U ĩh t−1 + b ĩ) (10) Following the properties of the cumax() activation, the values in the master forget gate are monotonically increasing from 0 to 1, and those in the master input gate are monotonically decreasing from 1 to 0.",STRUCTURED GATING MECHANISM,"Using the master gates, we define a new update rule: ω t = ft • ĩt (11) ft = f t • ω t",STRUCTURED GATING MECHANISM,
"Using the master gates, we define a new update rule: ω t = ft • ĩt (11) ft = f t • ω t",6430032,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,These gates serve as high-level control for the update operations of cell states.,STRUCTURED GATING MECHANISM,"− ft ) ( ) c t = ft • c t−1 + ît • ĉt In order to explain the intuition behind the new update rule, we assume that the master gates are binary: •",STRUCTURED GATING MECHANISM,
"− ft ) ( ) c t = ft • c t−1 + ît • ĉt In order to explain the intuition behind the new update rule, we assume that the master gates are binary: •",6430033,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Using the master gates, we define a new update rule: ω t = ft • ĩt (11) ft = f t • ω t",STRUCTURED GATING MECHANISM,The master forget gate ft controls the erasing behavior of the model.,STRUCTURED GATING MECHANISM,
The master forget gate ft controls the erasing behavior of the model.,6430034,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"− ft ) ( ) c t = ft • c t−1 + ît • ĉt In order to explain the intuition behind the new update rule, we assume that the master gates are binary: •",STRUCTURED GATING MECHANISM,", 1) and the split point is d f t .",STRUCTURED GATING MECHANISM,
", 1) and the split point is d f t .",6430035,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,The master forget gate ft controls the erasing behavior of the model.,STRUCTURED GATING MECHANISM,Given the Eq.,STRUCTURED GATING MECHANISM,
Given the Eq.,6430036,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,", 1) and the split point is d f t .",STRUCTURED GATING MECHANISM,"( 12) and ( 14), the information stored in the first d f t neurons of the previous cell state c t−1 will be completely erased.",STRUCTURED GATING MECHANISM,
"( 12) and ( 14), the information stored in the first d f t neurons of the previous cell state c t−1 will be completely erased.",6430037,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,Given the Eq.,STRUCTURED GATING MECHANISM,"In a parse tree (e.g. Figure 2(a)), this operation is akin to closing previous constituents.",STRUCTURED GATING MECHANISM,
"In a parse tree (e.g. Figure 2(a)), this operation is akin to closing previous constituents.",6430038,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"( 12) and ( 14), the information stored in the first d f t neurons of the previous cell state c t−1 will be completely erased.",STRUCTURED GATING MECHANISM,"A large number of zeroed neurons, i.e. a large d f t , represents the end of a high-level constituent in the parse tree, as most of the information in the state will be discarded.",STRUCTURED GATING MECHANISM,
"A large number of zeroed neurons, i.e. a large d f t , represents the end of a high-level constituent in the parse tree, as most of the information in the state will be discarded.",6430039,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"In a parse tree (e.g. Figure 2(a)), this operation is akin to closing previous constituents.",STRUCTURED GATING MECHANISM,"Conversely, a small d f t represents the end of a low-level constituent as high-level information is kept for further processing. ",STRUCTURED GATING MECHANISM,
"Conversely, a small d f t represents the end of a low-level constituent as high-level information is kept for further processing. ",6430040,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"A large number of zeroed neurons, i.e. a large d f t , represents the end of a high-level constituent in the parse tree, as most of the information in the state will be discarded.",STRUCTURED GATING MECHANISM,The master input gate ĩt is meant to control the writing mechanism of the model.,STRUCTURED GATING MECHANISM,
The master input gate ĩt is meant to control the writing mechanism of the model.,6430041,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Conversely, a small d f t represents the end of a low-level constituent as high-level information is kept for further processing. ",STRUCTURED GATING MECHANISM,", 0) and the split point is d",STRUCTURED GATING MECHANISM,
", 0) and the split point is d",6430042,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,The master input gate ĩt is meant to control the writing mechanism of the model.,STRUCTURED GATING MECHANISM,a large d i t means that the current input x t contains long-term information that needs to be preserved for several time steps.,STRUCTURED GATING MECHANISM,
a large d i t means that the current input x t contains long-term information that needs to be preserved for several time steps.,6430043,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,", 0) and the split point is d",STRUCTURED GATING MECHANISM,"Conversely, a small d i t means that the current input x t just provides local information that could be erased by ft in the next few time steps. ",STRUCTURED GATING MECHANISM,
"Conversely, a small d i t means that the current input x t just provides local information that could be erased by ft in the next few time steps. ",6430044,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,a large d i t means that the current input x t contains long-term information that needs to be preserved for several time steps.,STRUCTURED GATING MECHANISM,The product of the two master gates ω t represents the overlap of ft and ĩt .,STRUCTURED GATING MECHANISM,
The product of the two master gates ω t represents the overlap of ft and ĩt .,6430045,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Conversely, a small d i t means that the current input x t just provides local information that could be erased by ft in the next few time steps. ",STRUCTURED GATING MECHANISM,"Whenever an overlap exists (∃k, ω tk > 0), the corresponding segment of neurons encodes the incomplete constituents that contain some previous words and the current input word x t .",STRUCTURED GATING MECHANISM,
"Whenever an overlap exists (∃k, ω tk > 0), the corresponding segment of neurons encodes the incomplete constituents that contain some previous words and the current input word x t .",6430046,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,The product of the two master gates ω t represents the overlap of ft and ĩt .,STRUCTURED GATING MECHANISM,"Since these constituents are incomplete, we want to update the information inside the respective blocks. ",STRUCTURED GATING MECHANISM,
"Since these constituents are incomplete, we want to update the information inside the respective blocks. ",6430047,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Whenever an overlap exists (∃k, ω tk > 0), the corresponding segment of neurons encodes the incomplete constituents that contain some previous words and the current input word x t .",STRUCTURED GATING MECHANISM,The segment is further controlled by the f t and i t in the standard LSTM model to enable more fine-grained operations within blocks.,STRUCTURED GATING MECHANISM,
The segment is further controlled by the f t and i t in the standard LSTM model to enable more fine-grained operations within blocks.,6430048,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"Since these constituents are incomplete, we want to update the information inside the respective blocks. ",STRUCTURED GATING MECHANISM,"For example, in Figure 2, the word x 3 is nested As the master gates only focus on coarse-grained control, modeling them with the same dimensions as the hidden states is computationally expensive and unnecessary.",STRUCTURED GATING MECHANISM,
"For example, in Figure 2, the word x 3 is nested As the master gates only focus on coarse-grained control, modeling them with the same dimensions as the hidden states is computationally expensive and unnecessary.",6430049,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,The segment is further controlled by the f t and i t in the standard LSTM model to enable more fine-grained operations within blocks.,STRUCTURED GATING MECHANISM,"In practice, we set ft and ĩt to be D m",STRUCTURED GATING MECHANISM,
"In practice, we set ft and ĩt to be D m",6430050,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"For example, in Figure 2, the word x 3 is nested As the master gates only focus on coarse-grained control, modeling them with the same dimensions as the hidden states is computationally expensive and unnecessary.",STRUCTURED GATING MECHANISM,"= D C dimensional vectors, where D is the dimension of hidden state, and C is a chunk size factor.",STRUCTURED GATING MECHANISM,
"= D C dimensional vectors, where D is the dimension of hidden state, and C is a chunk size factor.",6430051,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"In practice, we set ft and ĩt to be D m",STRUCTURED GATING MECHANISM,"We repeat each dimension C times, before the element-wise multiplication with f t and i t .",STRUCTURED GATING MECHANISM,
"We repeat each dimension C times, before the element-wise multiplication with f t and i t .",6430052,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"= D C dimensional vectors, where D is the dimension of hidden state, and C is a chunk size factor.",STRUCTURED GATING MECHANISM,The downsizing significantly reduces the number of extra parameters that we need to add to the LSTM.,STRUCTURED GATING MECHANISM,
The downsizing significantly reduces the number of extra parameters that we need to add to the LSTM.,6430053,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,"We repeat each dimension C times, before the element-wise multiplication with f t and i t .",STRUCTURED GATING MECHANISM,"Therefore, every neuron within each C-sized chunk shares the same master gates.",STRUCTURED GATING MECHANISM,
"Therefore, every neuron within each C-sized chunk shares the same master gates.",6430054,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,STRUCTURED GATING MECHANISM,The downsizing significantly reduces the number of extra parameters that we need to add to the LSTM.,STRUCTURED GATING MECHANISM,"We evaluate the proposed model on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation (Marvin & Linzen, 2018), and logical inference (Bowman et al., 2015).",EXPERIMENTS,
"In this paper, we propose ordered neurons, a novel inductive bias for recurrent neural networks. ",6430055,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,CONCLUSION,"Note that the results may not be comparable, because the hyper-parameters for training were not provided.",LOGICAL INFERENCE,"Based on this idea, we propose a novel recurrent unit, the ON-LSTM, which includes a new gating mechanism and a new activation function cumax(•).",CONCLUSION,
"Based on this idea, we propose a novel recurrent unit, the ON-LSTM, which includes a new gating mechanism and a new activation function cumax(•).",6430056,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,CONCLUSION,"In this paper, we propose ordered neurons, a novel inductive bias for recurrent neural networks. ",CONCLUSION,"This brings recurrent neural networks closer to performing tree-like composition operations, by separately allocating hidden state neurons with long and short-term information.",CONCLUSION,
"This brings recurrent neural networks closer to performing tree-like composition operations, by separately allocating hidden state neurons with long and short-term information.",6430057,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,CONCLUSION,"Based on this idea, we propose a novel recurrent unit, the ON-LSTM, which includes a new gating mechanism and a new activation function cumax(•).",CONCLUSION,The model performance on unsupervised constituency parsing shows that the ON-LSTM induces the latent structure of natural language in a way that is coherent with human expert annotation.,CONCLUSION,
The model performance on unsupervised constituency parsing shows that the ON-LSTM induces the latent structure of natural language in a way that is coherent with human expert annotation.,6430058,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,CONCLUSION,"This brings recurrent neural networks closer to performing tree-like composition operations, by separately allocating hidden state neurons with long and short-term information.",CONCLUSION,"The inductive bias also enables ON-LSTM to achieve good performance on language modeling, long-term dependency, and logical inference tasks.",CONCLUSION,
"The inductive bias also enables ON-LSTM to achieve good performance on language modeling, long-term dependency, and logical inference tasks.",6430059,"Ordered Neurons: Integrating Tree Structures into Recurrent Neural
  Networks",2018,CONCLUSION,The model performance on unsupervised constituency parsing shows that the ON-LSTM induces the latent structure of natural language in a way that is coherent with human expert annotation.,CONCLUSION,,,
