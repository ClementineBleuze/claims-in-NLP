{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\") \n",
    "from utils.ClaimDB import ClaimDB\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only IRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ACL and arXiv corpora\n",
    "with open(\"../data/acl/corpus_ACL.pkl\", \"rb\") as f:\n",
    "    corpus_ACL = pickle.load(f)\n",
    "\n",
    "with open(\"../data/arxiv/corpus_arxiv.pkl\", \"rb\") as f:\n",
    "    corpus_arxiv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_section_hierarchy(sections):\n",
    "\n",
    "    # initialize section hierarchy with abstract\n",
    "    d = {0: {\"header\" : \"abstract\", \"subsections\" : {}}}\n",
    "    i = 1\n",
    "\n",
    "    # add the other sections\n",
    "    for _, s in sections.items():\n",
    "        \n",
    "        if s[\"head_n\"] == None:\n",
    "\n",
    "            if s[\"n\"] != None or s[\"header\"] != \"unidentified-section\":\n",
    "                d[i] = {\"header\" : s[\"header\"],\n",
    "                        \"subsections\": {}}\n",
    "                i += 1\n",
    "            \n",
    "            else:\n",
    "                j = len(d[i-1][\"subsections\"])\n",
    "\n",
    "                if j > 0:\n",
    "                    k = len(d[i-1][\"subsections\"][j-1][\"subsections\"])\n",
    "                    if k > 0:\n",
    "                        d[i-1][\"subsections\"][j-1][\"subsections\"][k] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "                    else:\n",
    "                        d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "                else:\n",
    "                    d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "        \n",
    "        else:\n",
    "            j = len(d[i-1][\"subsections\"])\n",
    "            d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "\n",
    "    return d\n",
    "\n",
    "def pretty_print_sections_dict(d, indent = 0):\n",
    "    s = \"\"\n",
    "    for i, (k, v) in enumerate(d.items()):\n",
    "        if v[\"header\"] != \"abstract\":\n",
    "            i = i + 1\n",
    "        s += \"\\t\"*indent + f\"{i}. \" + v[\"header\"] + \"\\n\"\n",
    "        if \"subsections\" in v:\n",
    "            s += pretty_print_sections_dict(v[\"subsections\"], indent + 1)\n",
    "\n",
    "    return s\n",
    "\n",
    "def find_introduction(d):\n",
    "    for k, v in d.items():\n",
    "        if \"introduction\" in v[\"header\"].lower():\n",
    "            return v[\"header\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_head(d, section, direct = False):\n",
    "    \n",
    "    for i, sec in d.items():\n",
    "\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        subsections = sec[\"subsections\"]\n",
    "        subsections_names = [v[\"header\"] for k, v in subsections.items()]\n",
    "\n",
    "        if section in subsections_names:\n",
    "            return sec[\"header\"]\n",
    "\n",
    "        else:\n",
    "            # check in the subsections\n",
    "            to_explore = list(subsections.items())\n",
    "            while len(to_explore) > 0:\n",
    "                k, v = to_explore.pop(0)\n",
    "                if section in [v[\"header\"] for k, v in v[\"subsections\"].items()]:\n",
    "                    return v[\"header\"]\n",
    "                else:\n",
    "                    to_explore.extend(v[\"subsections\"].items())\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_all_children(d, section):\n",
    "    \n",
    "    for i, sec in d.items():\n",
    "\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        subsections = sec[\"subsections\"]\n",
    "        subsections_names = [v[\"header\"] for k, v in subsections.items()]\n",
    "\n",
    "        if sec[\"header\"] == section:\n",
    "            return subsections_names\n",
    "        \n",
    "        else:\n",
    "            # check in the subsections\n",
    "            to_explore = list(subsections.items())\n",
    "            while len(to_explore) > 0:\n",
    "                k, v = to_explore.pop(0)\n",
    "                if v[\"header\"] == section:\n",
    "                    return [v[\"header\"] for k, v in v[\"subsections\"].items()]\n",
    "                else:\n",
    "                    to_explore.extend(v[\"subsections\"].items())\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 'ACL' was filled with 73496 papers:\n",
      "  - 60123 papers were successfully loaded\n",
      "  - 13373 papers could not be loaded\n"
     ]
    }
   ],
   "source": [
    "corpus_ACL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60123/60123 [00:17<00:00, 3409.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41633 / 60123 papers have an introduction, results and conclusion section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n = 0\n",
    "RES = [\"result\", \"performance\", \"evaluation\", \"experiment\"]\n",
    "CONCL = [\"analysis\", \"discussion\", \"limit\", \"ethic\", \"conclusion\", \"concluding\", \"future\"]\n",
    "all_IRC = []\n",
    "\n",
    "# for p in corpus_ACL.papers_with_errors:\n",
    "#     if p.init_error.startswith(\"Too many candidate sentences\") or p.init_error.startswith(\"Not enough candidate sentences\"):\n",
    "#         p.init_error = None\n",
    "#         corpus_ACL.papers.append(p)\n",
    "\n",
    "corpus_ACL.papers_with_errors = [p for p in [p_ for p_ in corpus_ACL.papers_with_errors if p_.init_error is not None] if not p.init_error.startswith(\"Too many candidate sentences\") and not p.init_error.startswith(\"Not enough candidate sentences\")]\n",
    "\n",
    "for p in tqdm(corpus_ACL.papers):\n",
    "    sections = [v[\"header\"] for k, v in p.sections.items()]\n",
    "\n",
    "    sections_d = reorder_section_hierarchy(p.sections)\n",
    "    \n",
    "    found_intro = False\n",
    "    found_results = False\n",
    "    found_conclusion = False\n",
    "\n",
    "    sections_to_keep = set()\n",
    "\n",
    "    # introduction\n",
    "    for i, section in enumerate(sections):\n",
    "        if \"introduction\" in section.lower():\n",
    "            found_intro = True\n",
    "            sections_to_keep.add((i, section))\n",
    "            break\n",
    "\n",
    "    # results\n",
    "    for j, section in enumerate(sections):\n",
    "        if j > i:\n",
    "            for res in RES:\n",
    "                if res in section.lower():\n",
    "                    found_results = True\n",
    "\n",
    "\n",
    "                    head = find_main_head(sections_d, section)\n",
    "                    if head is None:\n",
    "                        head = section\n",
    "\n",
    "                    children = find_all_children(sections_d, head)\n",
    "\n",
    "                    if children is None:\n",
    "                        sections_to_keep.add((j, section))\n",
    "                    else:\n",
    "                        sections_to_keep.add((j, head))\n",
    "                        for child in children:\n",
    "                            sections_to_keep.add((j, child))\n",
    "\n",
    "\n",
    "    # conclusion\n",
    "    for k, section in enumerate(sections):\n",
    "        for concl in CONCL:\n",
    "            if concl in section.lower():\n",
    "                found_conclusion = True\n",
    "                \n",
    "                head = find_main_head(sections_d, section)\n",
    "                if head is None:\n",
    "                    head = section\n",
    "\n",
    "                children = find_all_children(sections_d, head)\n",
    "\n",
    "                if children is None:\n",
    "                    sections_to_keep.add((k, section))\n",
    "                else:\n",
    "                    sections_to_keep.add((k, head))\n",
    "                    for child in children:\n",
    "                        sections_to_keep.add((k, child))\n",
    "\n",
    "\n",
    "    \n",
    "    if found_intro and found_results and found_conclusion:\n",
    "        all_IRC.append(list(sections_to_keep))\n",
    "        n += 1\n",
    "        p.content[\"candidate\"] = p.content[\"section\"].apply(lambda x: x in [s[1] for s in sections_to_keep])\n",
    "\n",
    "    else:\n",
    "        p.init_error = \"Not following IRC structure\"\n",
    "        corpus_ACL.papers_with_errors.append(p)\n",
    "\n",
    "\n",
    "print(f\"{n} / {len(corpus_ACL.papers)} papers have an introduction, results and conclusion section\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0 132.0\n",
      "-43.0 237.0\n",
      "Corpus 'ACL' was filled with 73055 papers:\n",
      "  - 40966 papers were successfully loaded\n",
      "  - 32089 papers could not be loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nb_cands = [len(p.content[p.content[\"candidate\"] == True]) for p in corpus_ACL.papers]\n",
    "\n",
    "q1 = np.percentile(nb_cands, 25)\n",
    "q3 = np.percentile(nb_cands, 75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "min_cands = q1 - 1.5*iqr\n",
    "max_cands = q3 + 1.5*iqr\n",
    "print(q1, q3)\n",
    "print(min_cands, max_cands)\n",
    "\n",
    "for p in corpus_ACL.papers:\n",
    "    nb_cands = len(p.content[p.content[\"candidate\"] == True])\n",
    "\n",
    "    if nb_cands < 20:\n",
    "        p.init_error = \"Not enough candidate sentences (less than 20)\"\n",
    "        corpus_ACL.papers_with_errors.append(p)\n",
    "\n",
    "    elif nb_cands > max_cands:\n",
    "        p.init_error = f\"Too many candidate sentences (more than Q3 + 1.5*IQR = {max_cands})\"\n",
    "        corpus_ACL.papers_with_errors.append(p)  \n",
    "\n",
    "no_errors = [p for p in corpus_ACL.papers if p.init_error is None]\n",
    "corpus_ACL.papers = no_errors\n",
    "\n",
    "corpus_ACL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/acl/corpus_ACL_IRC.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_ACL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30361/30361 [00:15<00:00, 1974.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23809 / 30361 papers have an introduction, results and conclusion section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "RES = [\"result\", \"performance\", \"evaluation\", \"experiment\"]\n",
    "CONCL = [\"analysis\", \"discussion\", \"limit\", \"ethic\", \"conclusion\", \"concluding\", \"future\"]\n",
    "all_IRC = []\n",
    "\n",
    "for p in corpus_arxiv.papers_with_errors:\n",
    "    if p.init_error.startswith(\"Too many candidate sentences\") or p.init_error.startswith(\"Not enough candidate sentences\"):\n",
    "        p.init_error = None\n",
    "        corpus_arxiv.papers.append(p)\n",
    "\n",
    "for p in tqdm(corpus_arxiv.papers):\n",
    "    sections = [v[\"header\"] for k, v in p.sections.items()]\n",
    "\n",
    "    sections_d = reorder_section_hierarchy(p.sections)\n",
    "    \n",
    "    found_intro = False\n",
    "    found_results = False\n",
    "    found_conclusion = False\n",
    "\n",
    "    sections_to_keep = set()\n",
    "\n",
    "    # introduction\n",
    "    for i, section in enumerate(sections):\n",
    "        if \"introduction\" in section.lower():\n",
    "            found_intro = True\n",
    "            sections_to_keep.add((i, section))\n",
    "            break\n",
    "\n",
    "    # results\n",
    "    for j, section in enumerate(sections):\n",
    "        if j > i:\n",
    "            for res in RES:\n",
    "                if res in section.lower():\n",
    "                    found_results = True\n",
    "\n",
    "\n",
    "                    head = find_main_head(sections_d, section)\n",
    "                    if head is None:\n",
    "                        head = section\n",
    "\n",
    "                    children = find_all_children(sections_d, head)\n",
    "\n",
    "                    if children is None:\n",
    "                        sections_to_keep.add((j, section))\n",
    "                    else:\n",
    "                        sections_to_keep.add((j, head))\n",
    "                        for child in children:\n",
    "                            sections_to_keep.add((j, child))\n",
    "\n",
    "\n",
    "    # conclusion\n",
    "    for k, section in enumerate(sections):\n",
    "        for concl in CONCL:\n",
    "            if concl in section.lower():\n",
    "                found_conclusion = True\n",
    "                \n",
    "                head = find_main_head(sections_d, section)\n",
    "                if head is None:\n",
    "                    head = section\n",
    "\n",
    "                children = find_all_children(sections_d, head)\n",
    "\n",
    "                if children is None:\n",
    "                    sections_to_keep.add((k, section))\n",
    "                else:\n",
    "                    sections_to_keep.add((k, head))\n",
    "                    for child in children:\n",
    "                        sections_to_keep.add((k, child))\n",
    "\n",
    "\n",
    "    \n",
    "    if found_intro and found_results and found_conclusion:\n",
    "        all_IRC.append(list(sections_to_keep))\n",
    "        n += 1\n",
    "        p.content[\"candidate\"] = p.content[\"section\"].apply(lambda x: x in [s[1] for s in sections_to_keep])\n",
    "\n",
    "    else:\n",
    "        p.init_error = \"Not following IRC structure\"\n",
    "        corpus_arxiv.papers_with_errors.append(p)\n",
    "\n",
    "print(f\"{n} / {len(corpus_arxiv.papers)} papers have an introduction, results and conclusion section\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers without errors: 23809\n",
      "Corpus 'arXiv' was filled with 35719 papers:\n",
      "  - 23809 papers were successfully loaded\n",
      "  - 11910 papers could not be loaded\n",
      "\n",
      "Errors:\n",
      "  - FileNotFoundError: XML file does not exist : 3471\n",
      "  - Noisy data: wrong language (fr) : 48\n",
      "  - Noisy data: wrong language (uk) : 5\n",
      "  - Noisy data: wrong language (ru) : 9\n",
      "  - Noisy data: wrong language (da) : 4\n",
      "  - parsing error: not enough paper content found (<2 distinct sections) : 14\n",
      "  - Noisy data: wrong language (hi) : 1\n",
      "  - Noisy data: wrong language (de) : 8\n",
      "  - Noisy data: wrong language (tr) : 12\n",
      "  - Noisy data: wrong language (id) : 4\n",
      "  - Noisy data: wrong language (pt) : 8\n",
      "  - Noisy data: wrong language (pl) : 2\n",
      "  - Noisy data: wrong language (es) : 12\n",
      "  - Noisy data: wrong language (it) : 8\n",
      "  - Noisy data: wrong language (zh-cn) : 1\n",
      "  - Noisy data: wrong language (et) : 3\n",
      "  - Noisy data: wrong language (tl) : 1\n",
      "  - Noisy data: wrong language (hu) : 1\n",
      "  - Noisy data: wrong language (ko) : 1\n",
      "  - Noisy data: wrong language (no) : 1\n",
      "  - Noisy data: wrong language (ja) : 1\n",
      "  - Noisy data: wrong language (mk) : 1\n",
      "  - Noisy data: wrong language (sl) : 2\n",
      "  - Noisy data: wrong language (so) : 2\n",
      "  - Duplicate error: also found in ACL corpus : 1\n",
      "  - None : 1163\n",
      "  - Not following IRC structure : 7126\n"
     ]
    }
   ],
   "source": [
    "without_errors = [p for p in corpus_arxiv.papers if p.init_error == None]\n",
    "print(f\"Number of papers without errors: {len(without_errors)}\")\n",
    "corpus_arxiv.papers = without_errors\n",
    "\n",
    "corpus_arxiv.describe(error_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0 144.0\n",
      "-41.0 255.0\n",
      "Corpus 'arXiv' was filled with 35719 papers:\n",
      "  - 22826 papers were successfully loaded\n",
      "  - 12893 papers could not be loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nb_cands = [len(p.content[p.content[\"candidate\"] == True]) for p in corpus_arxiv.papers]\n",
    "\n",
    "q1 = np.percentile(nb_cands, 25)\n",
    "q3 = np.percentile(nb_cands, 75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "min_cands = q1 - 1.5*iqr\n",
    "max_cands = q3 + 1.5*iqr\n",
    "print(q1, q3)\n",
    "print(min_cands, max_cands)\n",
    "\n",
    "for p in corpus_arxiv.papers:\n",
    "    nb_cands = len(p.content[p.content[\"candidate\"] == True])\n",
    "\n",
    "    if nb_cands < 20:\n",
    "        p.init_error = \"Not enough candidate sentences (less than 20)\"\n",
    "        corpus_arxiv.papers_with_errors.append(p)\n",
    "\n",
    "    elif nb_cands > max_cands:\n",
    "        p.init_error = f\"Too many candidate sentences (more than Q3 + 1.5*IQR = {max_cands})\"\n",
    "        corpus_arxiv.papers_with_errors.append(p)  \n",
    "\n",
    "no_errors = [p for p in corpus_arxiv.papers if p.init_error is None]\n",
    "corpus_arxiv.papers = no_errors\n",
    "\n",
    "corpus_arxiv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv/corpus_arxiv_IRC.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_arxiv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect IRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the ACL and arXiv corpora\n",
    "# with open(\"../data/acl/corpus_ACL_IRC.pkl\", \"rb\") as f:\n",
    "#     corpus_ACL = pickle.load(f)\n",
    "\n",
    "# with open(\"../data/arxiv/corpus_arxiv_IRC.pkl\", \"rb\") as f:\n",
    "#     corpus_arxiv = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40966/40966 [00:48<00:00, 840.49it/s]\n",
      "100%|██████████| 22826/22826 [00:34<00:00, 658.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# for corpus in [corpus_ACL, corpus_arxiv]:\n",
    "#     for p in tqdm(corpus.papers):\n",
    "#         p.content[\"candidate\"] = p.content.apply(lambda x: True if x[\"section\"] == \"abstract\" else x[\"candidate\"], axis = 1)\n",
    "\n",
    "# cdb = ClaimDB(corpora = [corpus_ACL, corpus_arxiv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cdb_IRC.pkl\", \"rb\") as f:\n",
    "    cdb = pickle.load(f)\n",
    "\n",
    "corpus_ACL, corpus_arxiv = cdb.corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Usually, measures of similarity between two wo...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The taxonomy approaches are more or less seman...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>However, in real applications, both semantic a...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Word similarity based on context vectors is a ...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>c) Syntactic and semantic similarity is balanc...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>The performance of our method might have been ...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>Further more, Cilin was published a long time ...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>However, our experimental results are encourag...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>They supports the theory that using context ve...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           sentence      section  \\\n",
       "0      0  There is a need to measure word similarity whe...     abstract   \n",
       "1      1  Usually, measures of similarity between two wo...     abstract   \n",
       "2      2  The taxonomy approaches are more or less seman...     abstract   \n",
       "3      3  However, in real applications, both semantic a...     abstract   \n",
       "4      4  Word similarity based on context vectors is a ...     abstract   \n",
       "..   ...                                                ...          ...   \n",
       "184  184  c) Syntactic and semantic similarity is balanc...  Conclusions   \n",
       "185  185  The performance of our method might have been ...  Conclusions   \n",
       "186  186  Further more, Cilin was published a long time ...  Conclusions   \n",
       "187  187  However, our experimental results are encourag...  Conclusions   \n",
       "188  188  They supports the theory that using context ve...  Conclusions   \n",
       "\n",
       "     candidate  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  \n",
       "..         ...  \n",
       "184       True  \n",
       "185       True  \n",
       "186       True  \n",
       "187       True  \n",
       "188       True  \n",
       "\n",
       "[189 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ACL.papers[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "1971\n",
      "11285\n",
      "27429\n",
      "total: 40966\n",
      "0\n",
      "101\n",
      "200\n",
      "22525\n",
      "total: 22826\n"
     ]
    }
   ],
   "source": [
    "corpus_ACL_0 = [p for p in corpus_ACL.papers if p.year < 1994]\n",
    "corpus_ACL_1 = [p for p in corpus_ACL.papers if p.year >= 1994 and p.year < 2004]\n",
    "corpus_ACL_2 = [p for p in corpus_ACL.papers if p.year >= 2004 and p.year < 2014]\n",
    "corpus_ACL_3 = [p for p in corpus_ACL.papers if p.year >= 2014]\n",
    "\n",
    "corpus_ACL_by_year_slices = [corpus_ACL_0, corpus_ACL_1, corpus_ACL_2, corpus_ACL_3]\n",
    "\n",
    "corpus_arxiv_0 = [p for p in corpus_arxiv.papers if p.year < 1994]\n",
    "corpus_arxiv_1 = [p for p in corpus_arxiv.papers if p.year >= 1994 and p.year < 2004]\n",
    "corpus_arxiv_2 = [p for p in corpus_arxiv.papers if p.year >= 2004 and p.year < 2014]\n",
    "corpus_arxiv_3 = [p for p in corpus_arxiv.papers if p.year >= 2014]\n",
    "\n",
    "corpus_arxiv_by_year_slices = [corpus_arxiv_0, corpus_arxiv_1, corpus_arxiv_2, corpus_arxiv_3]\n",
    "\n",
    "acl_total = 0\n",
    "arxiv_total = 0\n",
    "\n",
    "for c_acl in corpus_ACL_by_year_slices:\n",
    "    n = len(c_acl)\n",
    "    acl_total += n\n",
    "    print(n)\n",
    "\n",
    "print(\"total:\", acl_total)\n",
    "\n",
    "for c_arxiv in corpus_arxiv_by_year_slices:\n",
    "    n = len(c_arxiv)\n",
    "    arxiv_total += n\n",
    "    print(n)\n",
    "\n",
    "print(\"total:\", arxiv_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020.signlang-1.20', 'W17-4709', 'N19-1358', 'Y15-1047', 'P18-1048', 'W17-5513', '2022.naacl-main.19', '2022.in2writing-1.4', 'W18-3406', 'H89-1053', 'D15-1013', 'P97-1030', 'Y11-1012', 'W01-1826', 'D19-1210', 'W98-1507', 'W01-0812', 'W94-0305', 'P90-1033', 'A92-1020', 'P11-1048', 'P11-4022', '2020.emnlp-main.554', 'P13-1099', 'E87-1013', 'W03-0425', '2020.acl-main.413', 'W18-0527', 'C92-1030', 'W16-5818', 'W99-0402', 'W13-4420', '2001.mtsummit-papers.31', '1993.tmi-1.6', 'W00-1415', 'C98-2177', '2007.mtsummit-papers.40', 'N18-1107', 'Y03-1032', 'H89-2041', 'P19-1654', 'P93-1023', 'W08-1302', 'P11-1029', 'C90-1007', 'C92-2087', 'H89-2017', 'P91-1024', '2020.acl-demos.20', 'W00-0505', 'P07-2029', 'Q16-1037', 'I13-1029', 'J92-4001', 'P07-1088', 'J97-2004', 'W09-0809', 'E93-1027', 'E09-1027', 'W99-0609', 'J98-3005', '2020.lrec-1.826', 'W12-4402', 'C14-1028', 'Y09-2035', 'H93-1064', '2020.loresmt-1.4', 'D18-1087']\n",
      "68\n",
      "['2103.14302', '1708.01009', '1611.08765', '1605.05172', '2012.04584', '2108.06957', '2003.06634', 'cs/0412024', '2008.05282', 'cmp-lg/9612002', 'cmp-lg/9712002', 'cmp-lg/9610002', 'cs/9910011', '1312.0482', 'cmp-lg/9608017', '2312.06522', '1101.5494', '1010.2384', '2202.09509', '1210.5965', '2207.06403', 'cs/0703049', '2303.13375', 'cmp-lg/9412006', '1309.1125', '2302.07856', '1008.3667', '2401.04515', '2207.08988', '1303.2826', '1211.0498', '2001.10822', '1305.5918', '1310.4546', '1301.5686', '1312.0482', 'cmp-lg/9805006', '1910.13291', '1006.3271', '2311.09443', 'cs/0001020', '2108.02524', 'cmp-lg/9406037', 'cmp-lg/9506020', '2402.16654', '1309.4628', 'cs/0205067', 'cmp-lg/9611003', '1205.3316', '1508.03386', 'cmp-lg/9504004', '1207.5409', 'cmp-lg/9606024', 'cmp-lg/9605029', '1309.5174', '2311.16509', 'cmp-lg/9607024', 'cs/9904018', 'cs/0310018', '1512.01587', '2008.08901', 'cs/9906014', 'cs/9906014', '2307.08689', '1311.0833', '2307.08487']\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/annotated_articles.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "ACL_annotated = []\n",
    "arxiv_annotated = []\n",
    "\n",
    "for v in d.keys():\n",
    "    if v != \"v5\":\n",
    "        for paper in d[v]:\n",
    "            if paper[0] == \"ACL\":\n",
    "                ACL_annotated.append(paper[1])\n",
    "            else:\n",
    "                arxiv_annotated.append(paper[1])\n",
    "\n",
    "print(ACL_annotated)\n",
    "print(len(ACL_annotated))\n",
    "print(arxiv_annotated)\n",
    "print(len(arxiv_annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACL_ids = [p.id for p in corpus_ACL.papers]\n",
    "\n",
    "\"P18-1048\" in ACL_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O02-2002'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACL_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "1971\n",
      "11285\n",
      "27429\n",
      "total: 122898\n",
      "0\n",
      "101\n",
      "200\n",
      "22525\n",
      "total: 68478\n"
     ]
    }
   ],
   "source": [
    "for c_acl in corpus_ACL_by_year_slices:\n",
    "    c_acl = [p for p in c_acl if p.id not in ACL_annotated]\n",
    "\n",
    "for c_arx in corpus_arxiv_by_year_slices:\n",
    "    c_arx = [p for p in c_arx if p.id not in arxiv_annotated]\n",
    "\n",
    "for c_acl in corpus_ACL_by_year_slices:\n",
    "    n = len(c_acl)\n",
    "    acl_total += n\n",
    "    print(n)\n",
    "\n",
    "print(\"total:\", acl_total)\n",
    "\n",
    "for c_arxiv in corpus_arxiv_by_year_slices:\n",
    "    n = len(c_arxiv)\n",
    "    arxiv_total += n\n",
    "    print(n)\n",
    "\n",
    "print(\"total:\", arxiv_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random_papers = []\n",
    "random.seed(0)\n",
    "\n",
    "for c_acl in corpus_ACL_by_year_slices:\n",
    "    random_papers.extend(np.random.choice(c_acl, 15))\n",
    "\n",
    "for c_arx in corpus_arxiv_by_year_slices[1:]:\n",
    "    random_papers.extend(np.random.choice(c_arx, 20))\n",
    "\n",
    "random.shuffle(random_papers)\n",
    "print(len(random_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ACL', 'W19-8612'], ['arXiv', '2311.15402'], ['arXiv', '1302.1572'], ['ACL', 'E87-1037'], ['arXiv', '2205.10593'], ['ACL', 'W19-2609'], ['arXiv', 'cmp-lg/9608003'], ['ACL', 'W01-0506'], ['ACL', 'N10-1040'], ['ACL', 'P02-1043'], ['ACL', 'W16-2379'], ['arXiv', 'cs/0105005'], ['ACL', 'J01-2001'], ['arXiv', 'cmp-lg/9711004'], ['arXiv', 'cs/0307055'], ['ACL', 'J03-1001'], ['arXiv', '1302.2131'], ['ACL', 'J00-4001'], ['ACL', 'C90-2054'], ['arXiv', 'cmp-lg/9505021'], ['ACL', 'C86-1067'], ['arXiv', '2209.10505'], ['ACL', 'R13-1019'], ['arXiv', '1307.0261'], ['arXiv', 'cs/0405044'], ['ACL', 'C10-2093'], ['ACL', 'P19-1570'], ['arXiv', '2204.10181'], ['arXiv', '1212.2477'], ['ACL', 'W11-4614'], ['ACL', 'P91-1038'], ['ACL', 'N03-3005'], ['ACL', 'P15-1021'], ['ACL', 'W16-2373'], ['arXiv', '1807.04978'], ['arXiv', '1301.3614'], ['ACL', 'A88-1027'], ['ACL', 'D14-1169'], ['ACL', 'Y03-1009'], ['arXiv', '2012.08695'], ['arXiv', 'cmp-lg/9506024'], ['arXiv', '1310.1975'], ['ACL', '2006.amta-papers.16'], ['ACL', 'J96-1001'], ['ACL', 'H92-1069'], ['arXiv', '2307.14142'], ['arXiv', '1203.3511'], ['ACL', 'W02-1607'], ['arXiv', '2105.01331'], ['arXiv', '2104.07123'], ['ACL', 'A94-1026'], ['ACL', 'P07-1120'], ['ACL', 'S16-1042'], ['arXiv', '1206.6481'], ['arXiv', '1203.4238'], ['ACL', 'P03-2004'], ['ACL', 'P90-1013'], ['arXiv', '2007.09554'], ['ACL', '2021.nodalida-main.35'], ['arXiv', '1311.2978'], ['ACL', 'H93-1053'], ['arXiv', '1312.4092'], ['arXiv', '1302.4874'], ['ACL', 'W13-5717'], ['ACL', 'P06-2124'], ['arXiv', '1310.1975'], ['arXiv', 'cmp-lg/9505005'], ['ACL', 'H93-1074'], ['arXiv', '2102.00769'], ['ACL', 'J93-2004'], ['ACL', 'C86-1085'], ['arXiv', '1109.4531'], ['ACL', 'P93-1004'], ['arXiv', '2402.11955'], ['arXiv', 'cs/0307055'], ['ACL', 'D19-6216'], ['arXiv', '2311.00204'], ['arXiv', 'cs/0307055'], ['ACL', 'N03-2013'], ['arXiv', 'cmp-lg/9505043'], ['ACL', 'D13-1022'], ['ACL', 'Q16-1032'], ['arXiv', '2403.04454'], ['arXiv', '1302.2131'], ['arXiv', 'cs/0109013'], ['arXiv', 'cs/0110015'], ['ACL', '2005.iwslt-1.26'], ['arXiv', '1305.3107'], ['ACL', 'C88-2157'], ['arXiv', '2402.03627'], ['ACL', 'R13-1008'], ['ACL', 'H94-1054'], ['arXiv', 'cs/9902002'], ['arXiv', '1302.1572'], ['arXiv', 'cmp-lg/9505005'], ['ACL', 'D09-1052'], ['arXiv', 'cs/0007016'], ['ACL', '1991.mtsummit-papers.8'], ['arXiv', '1202.3752'], ['arXiv', '2311.04199'], ['ACL', 'W10-4334'], ['ACL', 'W03-1315'], ['arXiv', 'cs/0302014'], ['ACL', 'W97-1011'], ['arXiv', 'cmp-lg/9711009'], ['ACL', 'E17-2017'], ['arXiv', 'cmp-lg/9805011'], ['ACL', 'W10-3601'], ['arXiv', '2203.02385'], ['arXiv', '2105.08261'], ['ACL', 'W15-1505'], ['arXiv', 'cmp-lg/9605029'], ['arXiv', 'cs/9811016'], ['ACL', 'I13-1099'], ['ACL', 'A92-1021'], ['ACL', 'D15-1089'], ['arXiv', '2212.03721'], ['arXiv', '1010.2384'], ['ACL', '2020.iwslt-1.2'], ['arXiv', '2008.07189']]\n",
      "{'v1': [['ACL', '2020.signlang-1.20'], ['ACL', 'W17-4709'], ['ACL', 'N19-1358'], ['arXiv', '2103.14302'], ['ACL', 'Y15-1047'], ['ACL', 'P18-1048'], ['arXiv', '1708.01009'], ['arXiv', '1611.08765'], ['arXiv', '1605.05172'], ['arXiv', '2012.04584']], 'v2': [['ACL', 'W17-5513'], ['arXiv', '2108.06957']], 'v3': [['ACL', '2022.naacl-main.19'], ['ACL', '2022.in2writing-1.4']], 'v4': [['ACL', 'W18-3406'], ['arXiv', '2003.06634'], ['arXiv', 'cs/0412024'], ['ACL', 'H89-1053'], ['arXiv', '2008.05282'], ['ACL', 'D15-1013'], ['arXiv', 'cmp-lg/9612002'], ['ACL', 'P97-1030'], ['ACL', 'Y11-1012'], ['ACL', 'W01-1826'], ['ACL', 'D19-1210'], ['arXiv', 'cmp-lg/9712002'], ['ACL', 'W98-1507'], ['arXiv', 'cmp-lg/9610002'], ['arXiv', 'cs/9910011'], ['ACL', 'W01-0812'], ['arXiv', '1312.0482'], ['ACL', 'W94-0305'], ['ACL', 'P90-1033'], ['arXiv', 'cmp-lg/9608017'], ['ACL', 'A92-1020'], ['arXiv', '2312.06522'], ['ACL', 'P11-1048'], ['arXiv', '1101.5494'], ['arXiv', '1010.2384'], ['ACL', 'P11-4022'], ['ACL', '2020.emnlp-main.554'], ['arXiv', '2202.09509'], ['arXiv', '1210.5965'], ['ACL', 'P13-1099'], ['ACL', 'E87-1013'], ['ACL', 'W03-0425'], ['ACL', '2020.acl-main.413'], ['ACL', 'W18-0527'], ['arXiv', '2207.06403'], ['arXiv', 'cs/0703049'], ['ACL', 'C92-1030'], ['ACL', 'W16-5818'], ['ACL', 'W99-0402'], ['arXiv', '2303.13375'], ['arXiv', 'cmp-lg/9412006'], ['arXiv', '1309.1125'], ['ACL', 'W13-4420'], ['ACL', '2001.mtsummit-papers.31'], ['ACL', '1993.tmi-1.6'], ['arXiv', '2302.07856'], ['arXiv', '1008.3667'], ['ACL', 'W00-1415'], ['arXiv', '2401.04515'], ['arXiv', '2207.08988'], ['ACL', 'C98-2177'], ['ACL', '2007.mtsummit-papers.40'], ['ACL', 'N18-1107'], ['arXiv', '1303.2826'], ['arXiv', '1211.0498'], ['ACL', 'Y03-1032'], ['ACL', 'H89-2041'], ['arXiv', '2001.10822'], ['ACL', 'P19-1654'], ['arXiv', '1305.5918'], ['ACL', 'P93-1023'], ['arXiv', '1310.4546'], ['arXiv', '1301.5686'], ['ACL', 'W08-1302'], ['ACL', 'P11-1029'], ['arXiv', '1312.0482'], ['arXiv', 'cmp-lg/9805006'], ['ACL', 'C90-1007'], ['arXiv', '1910.13291'], ['ACL', 'C92-2087'], ['ACL', 'H89-2017'], ['arXiv', '1006.3271'], ['ACL', 'P91-1024'], ['arXiv', '2311.09443'], ['arXiv', 'cs/0001020'], ['ACL', '2020.acl-demos.20'], ['arXiv', '2108.02524'], ['arXiv', 'cmp-lg/9406037'], ['ACL', 'W00-0505'], ['arXiv', 'cmp-lg/9506020'], ['ACL', 'P07-2029'], ['ACL', 'Q16-1037'], ['arXiv', '2402.16654'], ['arXiv', '1309.4628'], ['arXiv', 'cs/0205067'], ['arXiv', 'cmp-lg/9611003'], ['ACL', 'I13-1029'], ['arXiv', '1205.3316'], ['ACL', 'J92-4001'], ['arXiv', '1508.03386'], ['ACL', 'P07-1088'], ['ACL', 'J97-2004'], ['arXiv', 'cmp-lg/9504004'], ['arXiv', '1207.5409'], ['arXiv', 'cmp-lg/9606024'], ['ACL', 'W09-0809'], ['arXiv', 'cmp-lg/9605029'], ['ACL', 'E93-1027'], ['arXiv', '1309.5174'], ['arXiv', '2311.16509'], ['ACL', 'E09-1027'], ['ACL', 'W99-0609'], ['arXiv', 'cmp-lg/9607024'], ['ACL', 'J98-3005'], ['arXiv', 'cs/9904018'], ['ACL', '2020.lrec-1.826'], ['arXiv', 'cs/0310018'], ['ACL', 'W12-4402'], ['arXiv', '1512.01587'], ['arXiv', '2008.08901'], ['ACL', 'C14-1028'], ['arXiv', 'cs/9906014'], ['arXiv', 'cs/9906014'], ['ACL', 'Y09-2035'], ['ACL', 'H93-1064'], ['ACL', '2020.loresmt-1.4'], ['arXiv', '2307.08689'], ['arXiv', '1311.0833'], ['ACL', 'D18-1087'], ['arXiv', '2307.08487']], 'v5': [['ACL', 'W19-8612'], ['arXiv', '2311.15402'], ['arXiv', '1302.1572'], ['ACL', 'E87-1037'], ['arXiv', '2205.10593'], ['ACL', 'W19-2609'], ['arXiv', 'cmp-lg/9608003'], ['ACL', 'W01-0506'], ['ACL', 'N10-1040'], ['ACL', 'P02-1043'], ['ACL', 'W16-2379'], ['arXiv', 'cs/0105005'], ['ACL', 'J01-2001'], ['arXiv', 'cmp-lg/9711004'], ['arXiv', 'cs/0307055'], ['ACL', 'J03-1001'], ['arXiv', '1302.2131'], ['ACL', 'J00-4001'], ['ACL', 'C90-2054'], ['arXiv', 'cmp-lg/9505021'], ['ACL', 'C86-1067'], ['arXiv', '2209.10505'], ['ACL', 'R13-1019'], ['arXiv', '1307.0261'], ['arXiv', 'cs/0405044'], ['ACL', 'C10-2093'], ['ACL', 'P19-1570'], ['arXiv', '2204.10181'], ['arXiv', '1212.2477'], ['ACL', 'W11-4614'], ['ACL', 'P91-1038'], ['ACL', 'N03-3005'], ['ACL', 'P15-1021'], ['ACL', 'W16-2373'], ['arXiv', '1807.04978'], ['arXiv', '1301.3614'], ['ACL', 'A88-1027'], ['ACL', 'D14-1169'], ['ACL', 'Y03-1009'], ['arXiv', '2012.08695'], ['arXiv', 'cmp-lg/9506024'], ['arXiv', '1310.1975'], ['ACL', '2006.amta-papers.16'], ['ACL', 'J96-1001'], ['ACL', 'H92-1069'], ['arXiv', '2307.14142'], ['arXiv', '1203.3511'], ['ACL', 'W02-1607'], ['arXiv', '2105.01331'], ['arXiv', '2104.07123'], ['ACL', 'A94-1026'], ['ACL', 'P07-1120'], ['ACL', 'S16-1042'], ['arXiv', '1206.6481'], ['arXiv', '1203.4238'], ['ACL', 'P03-2004'], ['ACL', 'P90-1013'], ['arXiv', '2007.09554'], ['ACL', '2021.nodalida-main.35'], ['arXiv', '1311.2978'], ['ACL', 'H93-1053'], ['arXiv', '1312.4092'], ['arXiv', '1302.4874'], ['ACL', 'W13-5717'], ['ACL', 'P06-2124'], ['arXiv', '1310.1975'], ['arXiv', 'cmp-lg/9505005'], ['ACL', 'H93-1074'], ['arXiv', '2102.00769'], ['ACL', 'J93-2004'], ['ACL', 'C86-1085'], ['arXiv', '1109.4531'], ['ACL', 'P93-1004'], ['arXiv', '2402.11955'], ['arXiv', 'cs/0307055'], ['ACL', 'D19-6216'], ['arXiv', '2311.00204'], ['arXiv', 'cs/0307055'], ['ACL', 'N03-2013'], ['arXiv', 'cmp-lg/9505043'], ['ACL', 'D13-1022'], ['ACL', 'Q16-1032'], ['arXiv', '2403.04454'], ['arXiv', '1302.2131'], ['arXiv', 'cs/0109013'], ['arXiv', 'cs/0110015'], ['ACL', '2005.iwslt-1.26'], ['arXiv', '1305.3107'], ['ACL', 'C88-2157'], ['arXiv', '2402.03627'], ['ACL', 'R13-1008'], ['ACL', 'H94-1054'], ['arXiv', 'cs/9902002'], ['arXiv', '1302.1572'], ['arXiv', 'cmp-lg/9505005'], ['ACL', 'D09-1052'], ['arXiv', 'cs/0007016'], ['ACL', '1991.mtsummit-papers.8'], ['arXiv', '1202.3752'], ['arXiv', '2311.04199'], ['ACL', 'W10-4334'], ['ACL', 'W03-1315'], ['arXiv', 'cs/0302014'], ['ACL', 'W97-1011'], ['arXiv', 'cmp-lg/9711009'], ['ACL', 'E17-2017'], ['arXiv', 'cmp-lg/9805011'], ['ACL', 'W10-3601'], ['arXiv', '2203.02385'], ['arXiv', '2105.08261'], ['ACL', 'W15-1505'], ['arXiv', 'cmp-lg/9605029'], ['arXiv', 'cs/9811016'], ['ACL', 'I13-1099'], ['ACL', 'A92-1021'], ['ACL', 'D15-1089'], ['arXiv', '2212.03721'], ['arXiv', '1010.2384'], ['ACL', '2020.iwslt-1.2'], ['arXiv', '2008.07189']]}\n"
     ]
    }
   ],
   "source": [
    "random_papers_ids = [[p.corpus.name, p.id] for p in random_papers]\n",
    "print(random_papers_ids)\n",
    "\n",
    "d[\"v5\"] = random_papers_ids\n",
    "print(d)\n",
    "\n",
    "with open(\"../data/annotated_articles.json\", \"w\") as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12603, 7)\n"
     ]
    }
   ],
   "source": [
    "random_sentences_ids = []\n",
    "coord2idx = {v:k for k,v in cdb.idx_map.items()}\n",
    "\n",
    "for rp in random_papers:\n",
    "\n",
    "    sentences_ids = rp.content[rp.content[\"candidate\"] == True][\"id\"].tolist()\n",
    "\n",
    "    random_sentences_ids.extend([coord2idx[(rp.corpus.name, rp.id, i)] for i in sentences_ids])\n",
    "\n",
    "df = cdb.candidates.loc[random_sentences_ids]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arXiv', '1010.2384']\n",
      "['arXiv', 'cmp-lg/9605029']\n"
     ]
    }
   ],
   "source": [
    "for clem in d[\"v4\"]:\n",
    "    if clem in d[\"v5\"]:\n",
    "        print(clem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_section_hierarchy(sections):\n",
    "\n",
    "    # initialize section hierarchy with abstract\n",
    "    d = {0: {\"header\" : \"abstract\", \"subsections\" : {}}}\n",
    "    i = 1\n",
    "\n",
    "    # add the other sections\n",
    "    for _, s in sections.items():\n",
    "        \n",
    "        if s[\"head_n\"] == None:\n",
    "\n",
    "            if s[\"n\"] != None or s[\"header\"] != \"unidentified-section\":\n",
    "                d[i] = {\"header\" : s[\"header\"],\n",
    "                        \"subsections\": {}}\n",
    "                i += 1\n",
    "            \n",
    "            else:\n",
    "                j = len(d[i-1][\"subsections\"])\n",
    "\n",
    "                if j > 0:\n",
    "                    k = len(d[i-1][\"subsections\"][j-1][\"subsections\"])\n",
    "                    if k > 0:\n",
    "                        d[i-1][\"subsections\"][j-1][\"subsections\"][k] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "                    else:\n",
    "                        d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "                else:\n",
    "                    d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "        \n",
    "        else:\n",
    "            j = len(d[i-1][\"subsections\"])\n",
    "            d[i-1][\"subsections\"][j] = {\"header\" : s[\"header\"], \"subsections\" : {}}\n",
    "\n",
    "    return d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def get_random_candidates_subset(cdb, ACL_count, arxiv_count, seed = 0, random_papers = []):\n",
    "    \n",
    "    coord2idx = {v:k for k,v in cdb.idx_map.items()}\n",
    "\n",
    "    if random_papers == []:\n",
    "        ## TODO: check which papers have already been annotated\n",
    "        random.seed(seed)\n",
    "        random_ACL = np.random.choice(corpus_ACL.papers, ACL_count, replace = False)\n",
    "        random_arxiv = np.random.choice(corpus_arxiv.papers, arxiv_count, replace = False)\n",
    "        \n",
    "        random_papers = list(random_ACL)\n",
    "        random_papers.extend(list(random_arxiv))\n",
    "\n",
    "        random.shuffle(random_papers)\n",
    "\n",
    "    random_idx = []\n",
    "\n",
    "    for rp in random_papers:\n",
    "\n",
    "        cand_ids = rp.content[rp.content[\"candidate\"] == True][\"id\"].tolist()\n",
    "\n",
    "        random_idx.extend([coord2idx[(rp.corpus.name, rp.id, i)] for i in cand_ids])\n",
    "\n",
    "    return cdb.candidates.loc[random_idx]\n",
    "\n",
    "def get_feedback_on_article(p, n=90):\n",
    "    s = \"=\"*n + \"\\n\" + \"Annotator feedback\" + \"\\n\" + \"=\"*n + \"\\n\\n\"\n",
    "    s += f\"You just finished annotating the article entitled <<{p.title}>>. Please answer following questions: \\n\\n\"\n",
    "\n",
    "    s1 = \"1. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its technicity / because it handles subjects you are unfamiliar with ?\\n\\n\"\n",
    "    s1 += \"Please add any label of your choice if your answer is yes.\"\n",
    "\n",
    "    s2 = \"2. Do you think that this article was difficult to understand, in a way that may have affected the quality of your annotations, because of its writing style / structure / parsing errors ?\\n\\n\"\n",
    "    s2 += \"Please add any label of your choice if your answer is yes.\"\n",
    "\n",
    "    s3 = \"3. Did you know / read the article before this annotation task, or do you think you have identified its authors ?\\n\\n\"\n",
    "    s3 += \"Please add any label of your choice if your answer is yes.\"\n",
    "\n",
    "    return [s + s1, s + s2, s + s3]\n",
    "\n",
    "def find_main_head(d, section):\n",
    "    \n",
    "    section_n = 0\n",
    "    head = section\n",
    "    head_n = 0\n",
    "\n",
    "    for i, sec in d.items():\n",
    "\n",
    "        if section == sec[\"header\"]:\n",
    "            head_n = i\n",
    "            section_n = i\n",
    "            break\n",
    "\n",
    "        subsections = sec[\"subsections\"]\n",
    "        subsections_names = [v[\"header\"] for k, v in subsections.items()]\n",
    "\n",
    "        if section in subsections_names:\n",
    "           head = sec[\"header\"]\n",
    "           head_n = i\n",
    "           section_n = subsections_names.index(section)\n",
    "           break\n",
    "\n",
    "        else:\n",
    "            # check in the subsections\n",
    "            to_explore = list(subsections.items())\n",
    "            ids = [[k] for k, v in subsections.items()]\n",
    "            while len(to_explore) > 0:\n",
    "                k, v = to_explore.pop(0)\n",
    "                id_ = ids.pop(0)\n",
    "\n",
    "                subsections_names = [v[\"header\"] for k, v in v[\"subsections\"].items()]\n",
    "\n",
    "                if section in subsections_names:\n",
    "                    head = sec[\"header\"]\n",
    "                    head_n = i\n",
    "                    section_n = id_\n",
    "                else:\n",
    "                    to_explore.extend(v[\"subsections\"].items())\n",
    "                    ids.extend([[k] + [k_] for k_, v in v[\"subsections\"].items()])\n",
    "\n",
    "\n",
    "    return section, section_n, head, head_n\n",
    "\n",
    "def find_all_children(d, section):\n",
    "    \n",
    "    children = [section]\n",
    "\n",
    "    for i, sec in d.items():\n",
    "\n",
    "        if sec[\"header\"] == section:\n",
    "\n",
    "            while \"subsections\" in sec.keys():\n",
    "                ss = sec[\"subsections\"]\n",
    "                children.extend([v[\"header\"] for k, v in ss.items()])\n",
    "                sec = ss\n",
    "\n",
    "    return children\n",
    "\n",
    "\n",
    "def prepare_for_doccano_format(cdb, df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"A function to prepare a dataframe of sentences for Doccano format\n",
    "    - df : a pandas DataFrame with columns {corpus, paper_id, sentence_id, sentence, section}\"\"\"\n",
    "\n",
    "    data = []\n",
    "    coord2idx = {v:k for k,v in cdb.idx_map.items()}\n",
    "\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        c = cdb.get_corpus_by_name(row[\"corpus\"])\n",
    "        p = c.get_paper_by_id(row[\"paper_id\"])\n",
    "\n",
    "        sections_d = reorder_section_hierarchy(p.sections)\n",
    "        main_sections_str = \"\"\n",
    "        for i, sec in enumerate(sections_d.values()):\n",
    "            main_sections_str += str(i) + \". \" + sec[\"header\"] + \"\\n\"\n",
    "\n",
    "        sec, sec_n, head, head_n = find_main_head(sections_d, row[\"section\"])\n",
    "        \n",
    "\n",
    "        idx = coord2idx[(c.name, p.id, row[\"sentence_id\"])]\n",
    "        text = row[\"sentence\"]\n",
    "\n",
    "        \n",
    "        prev_sent_id = int(row[\"sentence_id\"]) - 1\n",
    "        next_sent_id = int(row[\"sentence_id\"]) + 1\n",
    "\n",
    "        # get previous sentence\n",
    "        if prev_sent_id in p.content[\"id\"].values:\n",
    "            prev_doc = p.content.loc[prev_sent_id]\n",
    "            prev_text = prev_doc[\"sentence\"]\n",
    "            prev_sec = prev_doc[\"section\"]\n",
    "\n",
    "        else:\n",
    "            prev_text = \"\"\n",
    "            prev_sec = \"\"\n",
    "\n",
    "        # get next sentence\n",
    "        if next_sent_id in p.content[\"id\"].values:\n",
    "            next_doc = p.content.loc[next_sent_id]\n",
    "            next_text = next_doc[\"sentence\"]\n",
    "            next_sec = next_doc[\"section\"]\n",
    "        \n",
    "        else:\n",
    "            next_text = \"\"\n",
    "            next_sec = \"\"\n",
    "\n",
    "\n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"doc_id\": idx,\n",
    "            \"corpus\": p.corpus.name,\n",
    "            \"paper_title\" : p.title,\n",
    "            \"paper_id\" : p.id,\n",
    "            \"paper_structure\": main_sections_str,\n",
    "            \"year\": p.year,\n",
    "            \"section\": sec,\n",
    "            \"section_n\" : sec_n,\n",
    "            \"main_head\" : head,\n",
    "            \"main_head_n\" : head_n,\n",
    "            \"prev_text\": prev_text,\n",
    "            \"prev_section\": prev_sec,\n",
    "            \"next_text\": next_text,\n",
    "            \"next_section\": next_sec,\n",
    "            \"label\": \"\"\n",
    "        })\n",
    "\n",
    "    df_doccano = pd.DataFrame(data)\n",
    "\n",
    "    return df_doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>paper_structure</th>\n",
       "      <th>year</th>\n",
       "      <th>section</th>\n",
       "      <th>section_n</th>\n",
       "      <th>main_head</th>\n",
       "      <th>main_head_n</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>prev_section</th>\n",
       "      <th>next_text</th>\n",
       "      <th>next_section</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiple headlines of a newspaper article have...</td>\n",
       "      <td>2242712</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>W19-8612</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>A headline depends on the content and intent o...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A headline depends on the content and intent o...</td>\n",
       "      <td>2242713</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>W19-8612</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multiple headlines of a newspaper article have...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>While a single headline expresses the whole co...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While a single headline expresses the whole co...</td>\n",
       "      <td>2242714</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>W19-8612</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>A headline depends on the content and intent o...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>We suggest an automatic generation method of s...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We suggest an automatic generation method of s...</td>\n",
       "      <td>2242715</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>W19-8612</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>While a single headline expresses the whole co...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>Our generation method is based on the Pointer-...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our generation method is based on the Pointer-...</td>\n",
       "      <td>2242716</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>W19-8612</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We suggest an automatic generation method of s...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>We conducted automatic evaluations for generat...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   doc_id corpus  \\\n",
       "0  Multiple headlines of a newspaper article have...  2242712    ACL   \n",
       "1  A headline depends on the content and intent o...  2242713    ACL   \n",
       "2  While a single headline expresses the whole co...  2242714    ACL   \n",
       "3  We suggest an automatic generation method of s...  2242715    ACL   \n",
       "4  Our generation method is based on the Pointer-...  2242716    ACL   \n",
       "\n",
       "                                         paper_title  paper_id  \\\n",
       "0  Multiple News Headlines Generation using Page ...  W19-8612   \n",
       "1  Multiple News Headlines Generation using Page ...  W19-8612   \n",
       "2  Multiple News Headlines Generation using Page ...  W19-8612   \n",
       "3  Multiple News Headlines Generation using Page ...  W19-8612   \n",
       "4  Multiple News Headlines Generation using Page ...  W19-8612   \n",
       "\n",
       "                                     paper_structure  year   section  \\\n",
       "0  0. abstract\\n1. Introduction\\n2. Multiple News...  2019  abstract   \n",
       "1  0. abstract\\n1. Introduction\\n2. Multiple News...  2019  abstract   \n",
       "2  0. abstract\\n1. Introduction\\n2. Multiple News...  2019  abstract   \n",
       "3  0. abstract\\n1. Introduction\\n2. Multiple News...  2019  abstract   \n",
       "4  0. abstract\\n1. Introduction\\n2. Multiple News...  2019  abstract   \n",
       "\n",
       "   section_n main_head  main_head_n  \\\n",
       "0          0  abstract            0   \n",
       "1          0  abstract            0   \n",
       "2          0  abstract            0   \n",
       "3          0  abstract            0   \n",
       "4          0  abstract            0   \n",
       "\n",
       "                                           prev_text prev_section  \\\n",
       "0                                                                   \n",
       "1  Multiple headlines of a newspaper article have...     abstract   \n",
       "2  A headline depends on the content and intent o...     abstract   \n",
       "3  While a single headline expresses the whole co...     abstract   \n",
       "4  We suggest an automatic generation method of s...     abstract   \n",
       "\n",
       "                                           next_text next_section label  \n",
       "0  A headline depends on the content and intent o...     abstract        \n",
       "1  While a single headline expresses the whole co...     abstract        \n",
       "2  We suggest an automatic generation method of s...     abstract        \n",
       "3  Our generation method is based on the Pointer-...     abstract        \n",
       "4  We conducted automatic evaluations for generat...     abstract        "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doccano = prepare_for_doccano_format(cdb, df)\n",
    "df_doccano.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = []\n",
    "import re\n",
    "\n",
    "current_head = \"\"\n",
    "current_section = \"\"\n",
    "h = 0\n",
    "s = 0\n",
    "total_head_len = 0\n",
    "total_section_len = 0\n",
    "\n",
    "for i, row in df_doccano.iterrows():\n",
    "    n = 90\n",
    "\n",
    "    sec = row[\"section\"] \n",
    "    sec_n = row[\"section_n\"]\n",
    "    head = row[\"main_head\"]\n",
    "    head_n = row[\"main_head_n\"]\n",
    "\n",
    "    if row[\"corpus\"] == \"ACL\":\n",
    "            p = corpus_ACL.get_paper_by_id(row[\"paper_id\"])\n",
    "    else:\n",
    "        p = corpus_arxiv.get_paper_by_id(row[\"paper_id\"])\n",
    "\n",
    "    if head != current_head:\n",
    "        current_head = head\n",
    "\n",
    "        cands = p.content[p.content[\"candidate\"] == True]\n",
    "        children = find_all_children(reorder_section_hierarchy(p.sections), head)\n",
    "        total_head_len = len(cands[cands[\"section\"].isin(children)])\n",
    "\n",
    "        h = 1\n",
    "\n",
    "    else:\n",
    "        h += 1\n",
    "\n",
    "    if sec != current_section:\n",
    "        current_section = sec\n",
    "        \n",
    "        if row[\"corpus\"] == \"ACL\":\n",
    "            p = corpus_ACL.get_paper_by_id(row[\"paper_id\"])\n",
    "        else:\n",
    "            p = corpus_arxiv.get_paper_by_id(row[\"paper_id\"])\n",
    "\n",
    "        cands = p.content[p.content[\"candidate\"] == True]\n",
    "        children = find_all_children(reorder_section_hierarchy(p.sections), sec)\n",
    "        total_section_len = len(cands[cands[\"section\"].isin(children)])\n",
    "\n",
    "        s = 1\n",
    "\n",
    "    else:\n",
    "        s += 1\n",
    "\n",
    "\n",
    "    text = p.title.replace(\"\\n\", \"\").replace(\"\\t\", \"\") + \"\\n\"\n",
    "    text += \"=\" * n + \"\\n\" + str(head_n) + \". \" + head + \" -- \" + str(h) + \"/\" + str(total_head_len) + \"\\n\" + \"=\" * n + \"\\n\"\n",
    "\n",
    "    if sec != head:\n",
    "        text += str(sec_n) + \". \" + sec + \" -- \" + str(s) + \"/\" + str(total_section_len) +\"\\n\" + \"-\" * n + \"\\n\" \n",
    "\n",
    "    text+= row[\"text\"]\n",
    "\n",
    "    text2.append(text)\n",
    "\n",
    "df_doccano[\"text\"] = text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert feedback questions in the dataset\n",
    "for rp in random_papers:\n",
    "    last_row= df_doccano[df_doccano[\"paper_title\"] == rp.title].iloc[-1]\n",
    "    last_index = float(last_row.name)\n",
    "    values = list(last_row.values)\n",
    "\n",
    "    fb = get_feedback_on_article(rp)\n",
    "    for fb_q, h in zip(fb, [0.25, 0.5, 0.75]):\n",
    "        values[0] = fb_q\n",
    "        for i in range(7, len(values)):\n",
    "            values[i] = \"\"\n",
    "        df_doccano.loc[last_index + h] = values\n",
    "\n",
    "df_doccano = df_doccano.sort_index().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>paper_structure</th>\n",
       "      <th>year</th>\n",
       "      <th>section</th>\n",
       "      <th>section_n</th>\n",
       "      <th>main_head</th>\n",
       "      <th>main_head_n</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>prev_section</th>\n",
       "      <th>next_text</th>\n",
       "      <th>next_section</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, doc_id, corpus, paper_title, paper_id, paper_structure, year, section, section_n, main_head, main_head_n, prev_text, prev_section, next_text, next_section, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doccano[df_doccano[\"paper_title\"].str.startswith(\"Latent\")]\n",
    "# df_doccano.at[0, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_structure</th>\n",
       "      <th>year</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>prev_section</th>\n",
       "      <th>next_text</th>\n",
       "      <th>next_section</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>2242712</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>A headline depends on the content and intent o...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>2242713</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Multiple headlines of a newspaper article have...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>While a single headline expresses the whole co...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>2242714</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>A headline depends on the content and intent o...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>We suggest an automatic generation method of s...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>2242715</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>While a single headline expresses the whole co...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>Our generation method is based on the Pointer-...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>2242716</td>\n",
       "      <td>Multiple News Headlines Generation using Page ...</td>\n",
       "      <td>0. abstract\\n1. Introduction\\n2. Multiple News...</td>\n",
       "      <td>2019</td>\n",
       "      <td>We suggest an automatic generation method of s...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>We conducted automatic evaluations for generat...</td>\n",
       "      <td>abstract</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12952</th>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>4678499</td>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>0. abstract\\n1. Introduction and related work\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td>In this light, SNN is attractive as it shows t...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>Our experiment with self-training Google parse...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12953</th>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>4678500</td>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>0. abstract\\n1. Introduction and related work\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Mate is the one with the lowest time required ...</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12954</th>\n",
       "      <td>==============================================...</td>\n",
       "      <td>4678500</td>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>0. abstract\\n1. Introduction and related work\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12955</th>\n",
       "      <td>==============================================...</td>\n",
       "      <td>4678500</td>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>0. abstract\\n1. Introduction and related work\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>==============================================...</td>\n",
       "      <td>4678500</td>\n",
       "      <td>Comparison of Syntactic Parsers on Biomedical ...</td>\n",
       "      <td>0. abstract\\n1. Introduction and related work\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12957 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   doc_id  \\\n",
       "0      Multiple News Headlines Generation using Page ...  2242712   \n",
       "1      Multiple News Headlines Generation using Page ...  2242713   \n",
       "2      Multiple News Headlines Generation using Page ...  2242714   \n",
       "3      Multiple News Headlines Generation using Page ...  2242715   \n",
       "4      Multiple News Headlines Generation using Page ...  2242716   \n",
       "...                                                  ...      ...   \n",
       "12952  Comparison of Syntactic Parsers on Biomedical ...  4678499   \n",
       "12953  Comparison of Syntactic Parsers on Biomedical ...  4678500   \n",
       "12954  ==============================================...  4678500   \n",
       "12955  ==============================================...  4678500   \n",
       "12956  ==============================================...  4678500   \n",
       "\n",
       "                                             paper_title  \\\n",
       "0      Multiple News Headlines Generation using Page ...   \n",
       "1      Multiple News Headlines Generation using Page ...   \n",
       "2      Multiple News Headlines Generation using Page ...   \n",
       "3      Multiple News Headlines Generation using Page ...   \n",
       "4      Multiple News Headlines Generation using Page ...   \n",
       "...                                                  ...   \n",
       "12952  Comparison of Syntactic Parsers on Biomedical ...   \n",
       "12953  Comparison of Syntactic Parsers on Biomedical ...   \n",
       "12954  Comparison of Syntactic Parsers on Biomedical ...   \n",
       "12955  Comparison of Syntactic Parsers on Biomedical ...   \n",
       "12956  Comparison of Syntactic Parsers on Biomedical ...   \n",
       "\n",
       "                                         paper_structure  year  \\\n",
       "0      0. abstract\\n1. Introduction\\n2. Multiple News...  2019   \n",
       "1      0. abstract\\n1. Introduction\\n2. Multiple News...  2019   \n",
       "2      0. abstract\\n1. Introduction\\n2. Multiple News...  2019   \n",
       "3      0. abstract\\n1. Introduction\\n2. Multiple News...  2019   \n",
       "4      0. abstract\\n1. Introduction\\n2. Multiple News...  2019   \n",
       "...                                                  ...   ...   \n",
       "12952  0. abstract\\n1. Introduction and related work\\...  2020   \n",
       "12953  0. abstract\\n1. Introduction and related work\\...  2020   \n",
       "12954  0. abstract\\n1. Introduction and related work\\...  2020   \n",
       "12955  0. abstract\\n1. Introduction and related work\\...  2020   \n",
       "12956  0. abstract\\n1. Introduction and related work\\...  2020   \n",
       "\n",
       "                                               prev_text prev_section  \\\n",
       "0                                                                       \n",
       "1      Multiple headlines of a newspaper article have...     abstract   \n",
       "2      A headline depends on the content and intent o...     abstract   \n",
       "3      While a single headline expresses the whole co...     abstract   \n",
       "4      We suggest an automatic generation method of s...     abstract   \n",
       "...                                                  ...          ...   \n",
       "12952  In this light, SNN is attractive as it shows t...  Conclusions   \n",
       "12953  Mate is the one with the lowest time required ...  Conclusions   \n",
       "12954                                                                   \n",
       "12955                                                                   \n",
       "12956                                                                   \n",
       "\n",
       "                                               next_text next_section label  \n",
       "0      A headline depends on the content and intent o...     abstract        \n",
       "1      While a single headline expresses the whole co...     abstract        \n",
       "2      We suggest an automatic generation method of s...     abstract        \n",
       "3      Our generation method is based on the Pointer-...     abstract        \n",
       "4      We conducted automatic evaluations for generat...     abstract        \n",
       "...                                                  ...          ...   ...  \n",
       "12952  Our experiment with self-training Google parse...  Conclusions        \n",
       "12953                                                                        \n",
       "12954                                                                        \n",
       "12955                                                                        \n",
       "12956                                                                        \n",
       "\n",
       "[12957 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doccano = df_doccano.drop(columns = [\"corpus\", \"paper_id\", \"section\",  \"section_n\", \"main_head\", \"main_head_n\"])\n",
    "df_doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doccano.to_csv(\"to-annotate-Fanny-120.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
