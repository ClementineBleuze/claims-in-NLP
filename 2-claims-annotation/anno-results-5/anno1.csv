id,text,doc_id,paper_title,paper_structure,year,prev_text,prev_section,next_text,next_section,label,Comments
9859,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 1/7
==========================================================================================
There has been a growing interest in interpreting the underlying dynamics of Transformers.",3170925,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,,,"While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations.",abstract,background-AIC,
9860,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 2/7
==========================================================================================
While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations.",3170926,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,There has been a growing interest in interpreting the underlying dynamics of Transformers.,abstract,This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers.,abstract,background-AIC#rw,
9861,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 3/7
==========================================================================================
This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers.",3170927,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations.",abstract,"Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions.",abstract,contribution,
9862,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 4/7
==========================================================================================
Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions.",3170928,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers.,abstract,Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings.,abstract,result,ou contribution ?
9863,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 5/7
==========================================================================================
Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings.",3170929,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions.",abstract,Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores.,abstract,result,ou contribution ?
9864,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 6/7
==========================================================================================
Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores.",3170930,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings.,abstract,Our code is freely available at https://github.com/mohsenfayyaz/GlobEnc.,abstract,result,
9865,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
0. abstract -- 7/7
==========================================================================================
Our code is freely available at https://github.com/mohsenfayyaz/GlobEnc.",3170931,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores.,abstract,"The stellar performance of Transformers (Vaswani et al., 2017) has garnered a lot of attention to analyzing the reasons behind their effectiveness.",Introduction,contribution,
9866,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
1. Introduction -- 1/5
==========================================================================================
The stellar performance of Transformers (Vaswani et al., 2017) has garnered a lot of attention to analyzing the reasons behind their effectiveness.",3170932,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Our code is freely available at https://github.com/mohsenfayyaz/GlobEnc.,abstract,"The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019;Kovaleva et al., 2019;Reif et al., 2019;Htut et al., 2019).",Introduction,background-AIC#rw,
9867,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
1. Introduction -- 2/5
==========================================================================================
The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019;Kovaleva et al., 2019;Reif et al., 2019;Htut et al., 2019).",3170933,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"The stellar performance of Transformers (Vaswani et al., 2017) has garnered a lot of attention to analyzing the reasons behind their effectiveness.",Introduction,"However, there have been debates on whether raw attention weights are reliable anchors for explaining model's behavior or not (Wiegreffe and Pinter, 2019;Serrano and Smith, 2019;Jain and Wallace, 2019).",Introduction,background-AIC#rw,
9868,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
1. Introduction -- 3/5
==========================================================================================
However, there have been debates on whether raw attention weights are reliable anchors for explaining model's behavior or not (Wiegreffe and Pinter, 2019;Serrano and Smith, 2019;Jain and Wallace, 2019).",3170934,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"The self-attention mechanism has been one of the main areas of focus (Clark et al., 2019;Kovaleva et al., 2019;Reif et al., 2019;Htut et al., 2019).",Introduction,"Recently, it was shown that incorporating vector norms should be an indispensable part of any attention-based analysis 1 (Kobayashi et al., 2020,   ⋆ Equal contribution.",Introduction,background-AIC#rw,
9869,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
1. Introduction -- 4/5
==========================================================================================
Recently, it was shown that incorporating vector norms should be an indispensable part of any attention-based analysis 1 (Kobayashi et al., 2020,   ⋆ Equal contribution.",3170935,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"However, there have been debates on whether raw attention weights are reliable anchors for explaining model's behavior or not (Wiegreffe and Pinter, 2019;Serrano and Smith, 2019;Jain and Wallace, 2019).",Introduction,"We also have shown the unreliability of weights due to norm disparities in probing studies (Fayyaz et al., 2021).",Introduction,background-AIC#error#rw,
9870,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
1. Introduction -- 5/5
==========================================================================================
We also have shown the unreliability of weights due to norm disparities in probing studies (Fayyaz et al., 2021).",3170936,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Recently, it was shown that incorporating vector norms should be an indispensable part of any attention-based analysis 1 (Kobayashi et al., 2020,   ⋆ Equal contribution.",Introduction,"However, these norm-based studies incorporate only the attention block into their analysis, whereas Transformer encoder layer is composed of more components. ",2021,background-AIC#rw,
9871,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 1/98
==========================================================================================
In this section, we introduce the datasets and the token attribution analysis methods used in our evaluations, followed by the experimental setup and results.",3170937,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In what follows we report our experiments, comparing GlobEnc with several other settings.",Methodology,All analysis methods are evaluated on three different classification tasks.,Datasets,,
9872,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 2/98
==========================================================================================
0. Datasets -- 1/2
------------------------------------------------------------------------------------------
All analysis methods are evaluated on three different classification tasks.",3170938,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In this section, we introduce the datasets and the token attribution analysis methods used in our evaluations, followed by the experimental setup and results.",Experiments,"To cover sentiment detection tasks we use SST2 (Socher et al., 2013), MNLI (Williams et al., 2018) for Natural Language Inference and Hatexplain (Mathew et al., 2021) in hate speech detection.",Datasets,,
9873,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 3/98
==========================================================================================
0. Datasets -- 2/2
------------------------------------------------------------------------------------------
To cover sentiment detection tasks we use SST2 (Socher et al., 2013), MNLI (Williams et al., 2018) for Natural Language Inference and Hatexplain (Mathew et al., 2021) in hate speech detection.",3170939,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,All analysis methods are evaluated on three different classification tasks.,Datasets,We use two categories of explainability approaches in our work: Weight-based and Norm-based.,Analysis Methods,,
9874,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 4/98
==========================================================================================
1. Analysis Methods -- 1/10
------------------------------------------------------------------------------------------
We use two categories of explainability approaches in our work: Weight-based and Norm-based.",3170940,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To cover sentiment detection tasks we use SST2 (Socher et al., 2013), MNLI (Williams et al., 2018) for Natural Language Inference and Hatexplain (Mathew et al., 2021) in hate speech detection.",Datasets,The Weight-based approaches employed in our experiments are as follows: • W : The raw attention maps averaged across all heads (See A ℓ in §2). ,Analysis Methods,,
9875,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 5/98
==========================================================================================
1. Analysis Methods -- 2/10
------------------------------------------------------------------------------------------
The Weight-based approaches employed in our experiments are as follows: • W : The raw attention maps averaged across all heads (See A ℓ in §2). ",3170941,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,We use two categories of explainability approaches in our work: Weight-based and Norm-based.,Analysis Methods,• W FIXEDRES :,Analysis Methods,,
9876,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 6/98
==========================================================================================
1. Analysis Methods -- 3/10
------------------------------------------------------------------------------------------
• W FIXEDRES :",3170942,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The Weight-based approaches employed in our experiments are as follows: • W : The raw attention maps averaged across all heads (See A ℓ in §2). ,Analysis Methods,Abnar and Zuidema's assumption; add an identity matrix as a fixed residual to A ℓ (see Âℓ in Eq. 13). ,Analysis Methods,,
9877,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 7/98
==========================================================================================
1. Analysis Methods -- 4/10
------------------------------------------------------------------------------------------
Abnar and Zuidema's assumption; add an identity matrix as a fixed residual to A ℓ (see Âℓ in Eq. 13). ",3170943,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,• W FIXEDRES :,Analysis Methods,"• W RES : The corrected version of W in which accurate residuals are added based on the context-mixing ratios of N ENC : ri = n j=1,j̸ =i xi←j n j=1,j̸ =i xi←j +",Analysis Methods,,
9878,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 8/98
==========================================================================================
1. Analysis Methods -- 5/10
------------------------------------------------------------------------------------------
• W RES : The corrected version of W in which accurate residuals are added based on the context-mixing ratios of N ENC : ri = n j=1,j̸ =i xi←j n j=1,j̸ =i xi←j +",3170944,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Abnar and Zuidema's assumption; add an identity matrix as a fixed residual to A ℓ (see Âℓ in Eq. 13). ,Analysis Methods,"In order to enforce W RES to have a contextmixing ratio equal to ri , it is essential to zero-out the diagonal elements (the tokens' attentions to themselves) of Āℓ and renormalize it: A ′ ℓ = (I − diag Āℓ ) −1 ( Āℓ − diag Āℓ )",Analysis Methods,,
9879,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 9/98
==========================================================================================
1. Analysis Methods -- 6/10
------------------------------------------------------------------------------------------
In order to enforce W RES to have a contextmixing ratio equal to ri , it is essential to zero-out the diagonal elements (the tokens' attentions to themselves) of Āℓ and renormalize it: A ′ ℓ = (I − diag Āℓ ) −1 ( Āℓ − diag Āℓ )",3170945,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"• W RES : The corrected version of W in which accurate residuals are added based on the context-mixing ratios of N ENC : ri = n j=1,j̸ =i xi←j n j=1,j̸ =i xi←j +",Analysis Methods,"I The Norm-based analysis methods, namely N , N RES and N RESLN were discussed in detail in §2. ",Analysis Methods,,
9880,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 10/98
==========================================================================================
1. Analysis Methods -- 7/10
------------------------------------------------------------------------------------------
I The Norm-based analysis methods, namely N , N RES and N RESLN were discussed in detail in §2. ",3170946,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In order to enforce W RES to have a contextmixing ratio equal to ri , it is essential to zero-out the diagonal elements (the tokens' attentions to themselves) of Āℓ and renormalize it: A ′ ℓ = (I − diag Āℓ ) −1 ( Āℓ − diag Āℓ )",Analysis Methods,Our proposed norm-based method N ENC was explained in §3.,Analysis Methods,,
9881,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 11/98
==========================================================================================
1. Analysis Methods -- 8/10
------------------------------------------------------------------------------------------
Our proposed norm-based method N ENC was explained in §3.",3170947,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"I The Norm-based analysis methods, namely N , N RES and N RESLN were discussed in detail in §2. ",Analysis Methods,"For an ablation study, we introduce N FIXEDRES which is N , corrected with a fixed residual similar to W FIXEDRES 5 . ",Analysis Methods,,
9882,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 12/98
==========================================================================================
1. Analysis Methods -- 9/10
------------------------------------------------------------------------------------------
For an ablation study, we introduce N FIXEDRES which is N , corrected with a fixed residual similar to W FIXEDRES 5 . ",3170948,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Our proposed norm-based method N ENC was explained in §3.,Analysis Methods,"I In §4.5, we will demonstrate our comparative studies between the aforementioned methods and GlobEnc.",Analysis Methods,,
9883,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 13/98
==========================================================================================
1. Analysis Methods -- 10/10
------------------------------------------------------------------------------------------
I In §4.5, we will demonstrate our comparative studies between the aforementioned methods and GlobEnc.",3170949,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"For an ablation study, we introduce N FIXEDRES which is N , corrected with a fixed residual similar to W FIXEDRES 5 . ",Analysis Methods,"Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016;Atanasova et al., 2020).",Gradient-based Methods for Faithfulness Analysis,,
9884,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 14/98
==========================================================================================
2. Gradient-based Methods for Faithfulness Analysis -- 1/2
------------------------------------------------------------------------------------------
Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016;Atanasova et al., 2020).",3170950,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"I In §4.5, we will demonstrate our comparative studies between the aforementioned methods and GlobEnc.",Analysis Methods,"In this section we discuss the specific gradient-based methods we use, namely saliency, HTA, and our adjusted HTA.",Gradient-based Methods for Faithfulness Analysis,,
9885,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 15/98
==========================================================================================
2. Gradient-based Methods for Faithfulness Analysis -- 2/2
------------------------------------------------------------------------------------------
In this section we discuss the specific gradient-based methods we use, namely saliency, HTA, and our adjusted HTA.",3170951,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Gradient-based methods are widely used as alternatives for attention-based counterparts for quantifying the importance of a specific input feature in making the right prediction (Li et al., 2016;Atanasova et al., 2020).",Gradient-based Methods for Faithfulness Analysis,Gradient-based saliency is based on the gradient of the output (y c ) w.r.t.,Saliency,,
9886,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 16/98
==========================================================================================
3. Saliency -- 1/7
------------------------------------------------------------------------------------------
Gradient-based saliency is based on the gradient of the output (y c ) w.r.t.",3170952,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In this section we discuss the specific gradient-based methods we use, namely saliency, HTA, and our adjusted HTA.",Gradient-based Methods for Faithfulness Analysis,the input embeddings (e 0 i ).,Saliency,,
9887,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 17/98
==========================================================================================
3. Saliency -- 2/7
------------------------------------------------------------------------------------------
the input embeddings (e 0 i ).",3170953,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Gradient-based saliency is based on the gradient of the output (y c ) w.r.t.,Saliency,"One of the most accurate variations of the saliency family is the gradient×input method (Kindermans et al., 2016) where the input embeddings is multiplied by the gradients.",Saliency,,
9888,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 18/98
==========================================================================================
3. Saliency -- 3/7
------------------------------------------------------------------------------------------
One of the most accurate variations of the saliency family is the gradient×input method (Kindermans et al., 2016) where the input embeddings is multiplied by the gradients.",3170954,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,the input embeddings (e 0 i ).,Saliency,"Thus, the contribution score of input token i is determined by first computing the element-wise product of the input embeddings (e 0 i ) and the gradients of the true class output score (y c ) w.r.t.",Saliency,,
9889,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 19/98
==========================================================================================
3. Saliency -- 4/7
------------------------------------------------------------------------------------------
Thus, the contribution score of input token i is determined by first computing the element-wise product of the input embeddings (e 0 i ) and the gradients of the true class output score (y c ) w.r.t.",3170955,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"One of the most accurate variations of the saliency family is the gradient×input method (Kindermans et al., 2016) where the input embeddings is multiplied by the gradients.",Saliency,the input embeddings.,Saliency,,
9890,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 20/98
==========================================================================================
3. Saliency -- 5/7
------------------------------------------------------------------------------------------
the input embeddings.",3170956,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Thus, the contribution score of input token i is determined by first computing the element-wise product of the input embeddings (e 0 i ) and the gradients of the true class output score (y c ) w.r.t.",Saliency,"Then, the L2 norm of the scaled gradients is computed to derive the final score: ",Saliency,,
9891,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 21/98
==========================================================================================
3. Saliency -- 6/7
------------------------------------------------------------------------------------------
Then, the L2 norm of the scaled gradients is computed to derive the final score: ",3170957,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,the input embeddings.,Saliency,Saliency i = ∂y c ∂e 0,Saliency,,
9892,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 22/98
==========================================================================================
3. Saliency -- 7/7
------------------------------------------------------------------------------------------
Saliency i = ∂y c ∂e 0",3170958,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Then, the L2 norm of the scaled gradients is computed to derive the final score: ",Saliency,"To determine an upper bound on the information mixing within each layer, we use a modified version of Hidden Token Attribution (Brunner et al., 2020, HTA).",HTA x Inputs,,
9893,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 23/98
==========================================================================================
4. HTA x Inputs -- 1/6
------------------------------------------------------------------------------------------
To determine an upper bound on the information mixing within each layer, we use a modified version of Hidden Token Attribution (Brunner et al., 2020, HTA).",3170959,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Saliency i = ∂y c ∂e 0,Saliency,"In the original version, HTA is the sensitivity between any two vectors in the model's computational graph.",HTA x Inputs,,
9894,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 24/98
==========================================================================================
4. HTA x Inputs -- 2/6
------------------------------------------------------------------------------------------
In the original version, HTA is the sensitivity between any two vectors in the model's computational graph.",3170960,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To determine an upper bound on the information mixing within each layer, we use a modified version of Hidden Token Attribution (Brunner et al., 2020, HTA).",HTA x Inputs,"However, inspired by the gradient×input method (Kindermans et al., 2016), which has shown more faithful results (Atanasova et al., 2020;Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm.",HTA x Inputs,,
9895,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 25/98
==========================================================================================
4. HTA x Inputs -- 3/6
------------------------------------------------------------------------------------------
However, inspired by the gradient×input method (Kindermans et al., 2016), which has shown more faithful results (Atanasova et al., 2020;Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm.",3170961,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In the original version, HTA is the sensitivity between any two vectors in the model's computational graph.",HTA x Inputs,We compute the attribution from hidden embedding j (e ℓ−1 j ) to hidden embedding i (e ℓ i ) in layer ℓ as: c ℓ i←j = ∂e ℓ i ∂e ℓ−1 j ⊙ e ℓ−1 j F Computing HTA-based attribution matrices is an extremely computation-intensive task (especially for long texts) due to the high dimensionality of hidden embeddings.,HTA x Inputs,,
9896,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 26/98
==========================================================================================
4. HTA x Inputs -- 4/6
------------------------------------------------------------------------------------------
We compute the attribution from hidden embedding j (e ℓ−1 j ) to hidden embedding i (e ℓ i ) in layer ℓ as: c ℓ i←j = ∂e ℓ i ∂e ℓ−1 j ⊙ e ℓ−1 j F Computing HTA-based attribution matrices is an extremely computation-intensive task (especially for long texts) due to the high dimensionality of hidden embeddings.",3170962,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"However, inspired by the gradient×input method (Kindermans et al., 2016), which has shown more faithful results (Atanasova et al., 2020;Wu and Ong, 2021), we multiply the input vectors by the gradients and then apply a Frobenius norm.",HTA x Inputs,"Hence, we only use this method for 256 examples from the SST-2 task's validation set.",HTA x Inputs,,
9897,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 27/98
==========================================================================================
4. HTA x Inputs -- 5/6
------------------------------------------------------------------------------------------
Hence, we only use this method for 256 examples from the SST-2 task's validation set.",3170963,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,We compute the attribution from hidden embedding j (e ℓ−1 j ) to hidden embedding i (e ℓ i ) in layer ℓ as: c ℓ i←j = ∂e ℓ i ∂e ℓ−1 j ⊙ e ℓ−1 j F Computing HTA-based attribution matrices is an extremely computation-intensive task (especially for long texts) due to the high dimensionality of hidden embeddings.,HTA x Inputs,"It is worth noting that extracting the HTAbased contribution maps for the aforementioned data took approximately 2 hours, whereas computing the maps for the entire analysis methods stated in §4.2 took only 5 seconds.",HTA x Inputs,,
9898,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 28/98
==========================================================================================
4. HTA x Inputs -- 6/6
------------------------------------------------------------------------------------------
It is worth noting that extracting the HTAbased contribution maps for the aforementioned data took approximately 2 hours, whereas computing the maps for the entire analysis methods stated in §4.2 took only 5 seconds.",3170964,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Hence, we only use this method for 256 examples from the SST-2 task's validation set.",HTA x Inputs,"We employ HuggingFace's Transformers library 7 (Wolf et al., 2020) and the BERT-base-uncased model.",Setup,,
9899,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 29/98
==========================================================================================
5. Setup -- 1/7
------------------------------------------------------------------------------------------
We employ HuggingFace's Transformers library 7 (Wolf et al., 2020) and the BERT-base-uncased model.",3170965,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"It is worth noting that extracting the HTAbased contribution maps for the aforementioned data took approximately 2 hours, whereas computing the maps for the entire analysis methods stated in §4.2 took only 5 seconds.",HTA x Inputs,"For fine-tuning BERT, epochs vary from 3 to 5, and the batch size and learning rate are 32 and 3e-5, respectively.",Setup,,
9900,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 30/98
==========================================================================================
5. Setup -- 2/7
------------------------------------------------------------------------------------------
For fine-tuning BERT, epochs vary from 3 to 5, and the batch size and learning rate are 32 and 3e-5, respectively.",3170966,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"We employ HuggingFace's Transformers library 7 (Wolf et al., 2020) and the BERT-base-uncased model.",Setup,"8 We also carried out the main experiment on BERT-large and ELECTRA (Devlin et al., 2019;Clark et al., 2020) where the results are reported at §A.2.",Setup,,
9901,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 31/98
==========================================================================================
5. Setup -- 3/7
------------------------------------------------------------------------------------------
8 We also carried out the main experiment on BERT-large and ELECTRA (Devlin et al., 2019;Clark et al., 2020) where the results are reported at §A.2.",3170967,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"For fine-tuning BERT, epochs vary from 3 to 5, and the batch size and learning rate are 32 and 3e-5, respectively.",Setup,"After rollout aggregation of each analysis method, we obtain an accumulated attribution matrix for every layer (ℓ) of BERT.",Setup,,
9902,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 32/98
==========================================================================================
5. Setup -- 4/7
------------------------------------------------------------------------------------------
After rollout aggregation of each analysis method, we obtain an accumulated attribution matrix for every layer (ℓ) of BERT.",3170968,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"8 We also carried out the main experiment on BERT-large and ELECTRA (Devlin et al., 2019;Clark et al., 2020) where the results are reported at §A.2.",Setup,"These matrices indicate the overall contribution of each input token to all token representations in layer ℓ. Since the classifier in a fine-tuned model is attached to the final layer representation of the [CLS] token, we consider the first row (corresponding to [CLS] attributions) of the last layer attribution matrix.",Setup,,
9903,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 33/98
==========================================================================================
5. Setup -- 5/7
------------------------------------------------------------------------------------------
These matrices indicate the overall contribution of each input token to all token representations in layer ℓ. Since the classifier in a fine-tuned model is attached to the final layer representation of the [CLS] token, we consider the first row (corresponding to [CLS] attributions) of the last layer attribution matrix.",3170969,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"After rollout aggregation of each analysis method, we obtain an accumulated attribution matrix for every layer (ℓ) of BERT.",Setup,This vector represents the contribution of each input token to the model's final decision.,Setup,,
9904,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 34/98
==========================================================================================
5. Setup -- 6/7
------------------------------------------------------------------------------------------
This vector represents the contribution of each input token to the model's final decision.",3170970,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"These matrices indicate the overall contribution of each input token to all token representations in layer ℓ. Since the classifier in a fine-tuned model is attached to the final layer representation of the [CLS] token, we consider the first row (corresponding to [CLS] attributions) of the last layer attribution matrix.",Setup,"As a measure of faithfulness of the resulting vector with the saliency scores, we report the Spearman's rank correlation between the two vectors.",Setup,,
9905,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 35/98
==========================================================================================
5. Setup -- 7/7
------------------------------------------------------------------------------------------
As a measure of faithfulness of the resulting vector with the saliency scores, we report the Spearman's rank correlation between the two vectors.",3170971,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This vector represents the contribution of each input token to the model's final decision.,Setup,Table 1 shows the Spearman correlation of saliency scores with the aggregated attribution scores from [CLS] to input tokens at the final layer.,Results,,
9906,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 36/98
==========================================================================================
6. Results -- 1/4
------------------------------------------------------------------------------------------
Table 1 shows the Spearman correlation of saliency scores with the aggregated attribution scores from [CLS] to input tokens at the final layer.",3170972,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"As a measure of faithfulness of the resulting vector with the saliency scores, we report the Spearman's rank correlation between the two vectors.",Setup,"In order to determine the contribution of each component of encoder layer to the overall performance, we report the results for attribution analysis methods discussed in §4.2.",Results,,
9907,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 37/98
==========================================================================================
6. Results -- 2/4
------------------------------------------------------------------------------------------
In order to determine the contribution of each component of encoder layer to the overall performance, we report the results for attribution analysis methods discussed in §4.2.",3170973,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Table 1 shows the Spearman correlation of saliency scores with the aggregated attribution scores from [CLS] to input tokens at the final layer.,Results,"Our results demonstrate that incorporating the vector norms, residual connection, and both layer normalizations yields the highest correlation (N ENC ).",Results,,
9908,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 38/98
==========================================================================================
6. Results -- 3/4
------------------------------------------------------------------------------------------
Our results demonstrate that incorporating the vector norms, residual connection, and both layer normalizations yields the highest correlation (N ENC ).",3170974,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In order to determine the contribution of each component of encoder layer to the overall performance, we report the results for attribution analysis methods discussed in §4.2.",Results,"In what follows, we discuss the impact of incorporating various parts in the analysis.",Results,result,
9909,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 39/98
==========================================================================================
6. Results -- 4/4
------------------------------------------------------------------------------------------
In what follows, we discuss the impact of incorporating various parts in the analysis.",3170975,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Our results demonstrate that incorporating the vector norms, residual connection, and both layer normalizations yields the highest correlation (N ENC ).",Results,As also suggested by Kobayashi et al.,On the role of vector norms,,
9910,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 40/98
==========================================================================================
7. On the role of vector norms -- 1/16
------------------------------------------------------------------------------------------
As also suggested by Kobayashi et al.",3170976,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In what follows, we discuss the impact of incorporating various parts in the analysis.",Results,(2020)  attention outputs.,On the role of vector norms,,
9911,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 41/98
==========================================================================================
7. On the role of vector norms -- 2/16
------------------------------------------------------------------------------------------
(2020)  attention outputs.",3170977,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,As also suggested by Kobayashi et al.,On the role of vector norms,This is highlighted by the significant gap between weight-based and norm-based settings across all datasets in Table 1. ,On the role of vector norms,error,
9912,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 42/98
==========================================================================================
7. On the role of vector norms -- 3/16
------------------------------------------------------------------------------------------
This is highlighted by the significant gap between weight-based and norm-based settings across all datasets in Table 1. ",3170978,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,(2020)  attention outputs.,On the role of vector norms,We also show the correlation of the aggregated attention for all layers in Figure 3.,On the role of vector norms,result,sans l'accès au tableau et à la phrase précédente c'est difficile de savoir
9913,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 43/98
==========================================================================================
7. On the role of vector norms -- 4/16
------------------------------------------------------------------------------------------
We also show the correlation of the aggregated attention for all layers in Figure 3.",3170979,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This is highlighted by the significant gap between weight-based and norm-based settings across all datasets in Table 1. ,On the role of vector norms,"The norm-based settings (N and N RES ) attain higher correlation than the weight-based counterparts (W and W RES ) almost in all layers, confirming the importance of incorporating vector norms.",On the role of vector norms,,
9914,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 44/98
==========================================================================================
7. On the role of vector norms -- 5/16
------------------------------------------------------------------------------------------
The norm-based settings (N and N RES ) attain higher correlation than the weight-based counterparts (W and W RES ) almost in all layers, confirming the importance of incorporating vector norms.",3170980,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,We also show the correlation of the aggregated attention for all layers in Figure 3.,On the role of vector norms,"9 As mentioned in §4.2, this analysis method is based on the original experiment by Abnar and Zuidema (2020).",On the role of vector norms,result,
9915,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 45/98
==========================================================================================
7. On the role of vector norms -- 6/16
------------------------------------------------------------------------------------------
9 As mentioned in §4.2, this analysis method is based on the original experiment by Abnar and Zuidema (2020).",3170981,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"The norm-based settings (N and N RES ) attain higher correlation than the weight-based counterparts (W and W RES ) almost in all layers, confirming the importance of incorporating vector norms.",On the role of vector norms,"Our experiments on SST2 differ from theirs in two aspects: (i) we opted for gradient×input saliencies, while they used the sum of gradients (sensitivity) (ii) instead of BERT, they used a DistillBERT fine-tuned model (Sanh et al., 2019).",On the role of vector norms,,
9916,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 46/98
==========================================================================================
7. On the role of vector norms -- 7/16
------------------------------------------------------------------------------------------
Our experiments on SST2 differ from theirs in two aspects: (i) we opted for gradient×input saliencies, while they used the sum of gradients (sensitivity) (ii) instead of BERT, they used a DistillBERT fine-tuned model (Sanh et al., 2019).",3170982,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"9 As mentioned in §4.2, this analysis method is based on the original experiment by Abnar and Zuidema (2020).",On the role of vector norms,"However, their reported results in their sepcific setup (Spearman Corr.",On the role of vector norms,,
9917,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 47/98
==========================================================================================
7. On the role of vector norms -- 8/16
------------------------------------------------------------------------------------------
However, their reported results in their sepcific setup (Spearman Corr.",3170983,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Our experiments on SST2 differ from theirs in two aspects: (i) we opted for gradient×input saliencies, while they used the sum of gradients (sensitivity) (ii) instead of BERT, they used a DistillBERT fine-tuned model (Sanh et al., 2019).",On the role of vector norms,= 0.14) still yields significantly lower results than GlobEnc.,On the role of vector norms,result#rw,
9918,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 48/98
==========================================================================================
7. On the role of vector norms -- 9/16
------------------------------------------------------------------------------------------
= 0.14) still yields significantly lower results than GlobEnc.",3170984,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"However, their reported results in their sepcific setup (Spearman Corr.",On the role of vector norms,"Kobayashi et al. (2021) showed that in the encoder layer, the output representations of each token is mainly determined by its own representation, and the contextualization from other tokens' plays a marginal role.",On the role of vector norms,error#result#rw,
9919,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 49/98
==========================================================================================
7. On the role of vector norms -- 10/16
------------------------------------------------------------------------------------------
Kobayashi et al. (2021) showed that in the encoder layer, the output representations of each token is mainly determined by its own representation, and the contextualization from other tokens' plays a marginal role.",3170985,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,= 0.14) still yields significantly lower results than GlobEnc.,On the role of vector norms,This is in contrary to the simplifying assumption made by Abnar and Zuidema (2020) who used a fixed context-mixing ratio of 0.5 (assuming that BERT equally preserves and mixes the representations).,On the role of vector norms,,
9920,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 50/98
==========================================================================================
7. On the role of vector norms -- 11/16
------------------------------------------------------------------------------------------
This is in contrary to the simplifying assumption made by Abnar and Zuidema (2020) who used a fixed context-mixing ratio of 0.5 (assuming that BERT equally preserves and mixes the representations).",3170986,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Kobayashi et al. (2021) showed that in the encoder layer, the output representations of each token is mainly determined by its own representation, and the contextualization from other tokens' plays a marginal role.",On the role of vector norms,This setting is shown as weightbased with fixed residual (W FIXEDRES ) in Table 1.,On the role of vector norms,,
9921,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 51/98
==========================================================================================
7. On the role of vector norms -- 12/16
------------------------------------------------------------------------------------------
This setting is shown as weightbased with fixed residual (W FIXEDRES ) in Table 1.",3170987,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This is in contrary to the simplifying assumption made by Abnar and Zuidema (2020) who used a fixed context-mixing ratio of 0.5 (assuming that BERT equally preserves and mixes the representations).,On the role of vector norms,We compare this setting against W RES (see §4.2).,On the role of vector norms,,
9922,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 52/98
==========================================================================================
7. On the role of vector norms -- 13/16
------------------------------------------------------------------------------------------
We compare this setting against W RES (see §4.2).",3170988,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This setting is shown as weightbased with fixed residual (W FIXEDRES ) in Table 1.,On the role of vector norms,W RES is similar to W FIXEDRES (in that it does not take into account vector norms) but differs in that it considers a dynamic mixing ratio (the one from N ENC ).,On the role of vector norms,,
9923,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 53/98
==========================================================================================
7. On the role of vector norms -- 14/16
------------------------------------------------------------------------------------------
W RES is similar to W FIXEDRES (in that it does not take into account vector norms) but differs in that it considers a dynamic mixing ratio (the one from N ENC ).",3170989,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,We compare this setting against W RES (see §4.2).,On the role of vector norms,The huge performance gap between the two settings in Table 1 clearly highlights the importance of considering accurate context-mixing ratios.,On the role of vector norms,,
9924,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 54/98
==========================================================================================
7. On the role of vector norms -- 15/16
------------------------------------------------------------------------------------------
The huge performance gap between the two settings in Table 1 clearly highlights the importance of considering accurate context-mixing ratios.",3170990,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,W RES is similar to W FIXEDRES (in that it does not take into account vector norms) but differs in that it considers a dynamic mixing ratio (the one from N ENC ).,On the role of vector norms,"Therefore, it is crucial to consider the residual connection in the attention block for input token attribution analysis.",On the role of vector norms,result,
9925,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 55/98
==========================================================================================
7. On the role of vector norms -- 16/16
------------------------------------------------------------------------------------------
Therefore, it is crucial to consider the residual connection in the attention block for input token attribution analysis.",3170991,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The huge performance gap between the two settings in Table 1 clearly highlights the importance of considering accurate context-mixing ratios.,On the role of vector norms,"To further demonstrate the role of residual connections, we utilize the introduced method in §4.2, where we modified the norm-based attentions with fixed residual (r ≈ 0.5).",On the role of residual connections,result,
9926,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 56/98
==========================================================================================
8. On the role of residual connections -- 1/4
------------------------------------------------------------------------------------------
To further demonstrate the role of residual connections, we utilize the introduced method in §4.2, where we modified the norm-based attentions with fixed residual (r ≈ 0.5).",3170992,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Therefore, it is crucial to consider the residual connection in the attention block for input token attribution analysis.",On the role of vector norms,The comparison of normbased without any residual (N ) and with a fixed residual (N FIXEDRES ) shows a consistent improvement for the latter across all the datasets.,On the role of residual connections,,
9927,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 57/98
==========================================================================================
8. On the role of residual connections -- 2/4
------------------------------------------------------------------------------------------
The comparison of normbased without any residual (N ) and with a fixed residual (N FIXEDRES ) shows a consistent improvement for the latter across all the datasets.",3170993,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To further demonstrate the role of residual connections, we utilize the introduced method in §4.2, where we modified the norm-based attentions with fixed residual (r ≈ 0.5).",On the role of residual connections,This provides evidence on that having a fixed uniform context-mixing ratio is better than neglecting the residual connection altogether. ,On the role of residual connections,result,
9928,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 58/98
==========================================================================================
8. On the role of residual connections -- 3/4
------------------------------------------------------------------------------------------
This provides evidence on that having a fixed uniform context-mixing ratio is better than neglecting the residual connection altogether. ",3170994,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The comparison of normbased without any residual (N ) and with a fixed residual (N FIXEDRES ) shows a consistent improvement for the latter across all the datasets.,On the role of residual connections,"Finally, when we aggregate the norm-based analysis with an accurate dynamic context-mixing ratio (N RES ), we observe the highest correlation up to this point, without layer normalization.",On the role of residual connections,result,
9929,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 59/98
==========================================================================================
8. On the role of residual connections -- 4/4
------------------------------------------------------------------------------------------
Finally, when we aggregate the norm-based analysis with an accurate dynamic context-mixing ratio (N RES ), we observe the highest correlation up to this point, without layer normalization.",3170995,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This provides evidence on that having a fixed uniform context-mixing ratio is better than neglecting the residual connection altogether. ,On the role of residual connections,In Table 1 we see a sudden drop in correlations for N RESLN .,On the role of layer normalization,result,
9930,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 60/98
==========================================================================================
9. On the role of layer normalization -- 1/18
------------------------------------------------------------------------------------------
In Table 1 we see a sudden drop in correlations for N RESLN .",3170996,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Finally, when we aggregate the norm-based analysis with an accurate dynamic context-mixing ratio (N RES ), we observe the highest correlation up to this point, without layer normalization.",On the role of residual connections,"Although this method considers vector norms and residuals, incorporating LN#1 in the encoder seems to have deteriorated the accuracy for token attribution analysis.",On the role of layer normalization,result,
9931,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 61/98
==========================================================================================
9. On the role of layer normalization -- 2/18
------------------------------------------------------------------------------------------
Although this method considers vector norms and residuals, incorporating LN#1 in the encoder seems to have deteriorated the accuracy for token attribution analysis.",3170997,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,In Table 1 we see a sudden drop in correlations for N RESLN .,On the role of layer normalization,"To determine whether this deterioration of correlation in aggregated attributions is also present in individual single layers, we compare the HTA maps as a baseline with the attribution matrices extracted from different analysis methods.",On the role of layer normalization,result,
9932,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 62/98
==========================================================================================
9. On the role of layer normalization -- 3/18
------------------------------------------------------------------------------------------
To determine whether this deterioration of correlation in aggregated attributions is also present in individual single layers, we compare the HTA maps as a baseline with the attribution matrices extracted from different analysis methods.",3170998,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Although this method considers vector norms and residuals, incorporating LN#1 in the encoder seems to have deteriorated the accuracy for token attribution analysis.",On the role of layer normalization,"Figure 4 shows the correlation of HTA attribution maps with the maps obtained by N RES , N RESLN , and N ENC methods.",On the role of layer normalization,,
9933,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 63/98
==========================================================================================
9. On the role of layer normalization -- 4/18
------------------------------------------------------------------------------------------
Figure 4 shows the correlation of HTA attribution maps with the maps obtained by N RES , N RESLN , and N ENC methods.",3170999,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To determine whether this deterioration of correlation in aggregated attributions is also present in individual single layers, we compare the HTA maps as a baseline with the attribution matrices extracted from different analysis methods.",On the role of layer normalization,The results indicate that N RESLN exhibits a significantly lower association. ,On the role of layer normalization,,
9934,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 64/98
==========================================================================================
9. On the role of layer normalization -- 5/18
------------------------------------------------------------------------------------------
The results indicate that N RESLN exhibits a significantly lower association. ",3171000,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Figure 4 shows the correlation of HTA attribution maps with the maps obtained by N RES , N RESLN , and N ENC methods.",On the role of layer normalization,The question that arises here is that how incorporating an additional component of the encoder (LN#1) in N RESLN degrades the results (compared to N RES ).,On the role of layer normalization,result,
9935,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 65/98
==========================================================================================
9. On the role of layer normalization -- 6/18
------------------------------------------------------------------------------------------
The question that arises here is that how incorporating an additional component of the encoder (LN#1) in N RESLN degrades the results (compared to N RES ).",3171001,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The results indicate that N RESLN exhibits a significantly lower association. ,On the role of layer normalization,"To answer this question, we investigated the learned weights of LN#1 and LN#2.",On the role of layer normalization,,
9936,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 66/98
==========================================================================================
9. On the role of layer normalization -- 7/18
------------------------------------------------------------------------------------------
To answer this question, we investigated the learned weights of LN#1 and LN#2.",3171002,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The question that arises here is that how incorporating an additional component of the encoder (LN#1) in N RESLN degrades the results (compared to N RES ).,On the role of layer normalization,"The outlier weights 10 in specific dimensions of LNs are shown to be significantly influential on the model's performance (Kovaleva et al., 2021;Luo et al., 2021).",On the role of layer normalization,,
9937,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 67/98
==========================================================================================
9. On the role of layer normalization -- 8/18
------------------------------------------------------------------------------------------
The outlier weights 10 in specific dimensions of LNs are shown to be significantly influential on the model's performance (Kovaleva et al., 2021;Luo et al., 2021).",3171003,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To answer this question, we investigated the learned weights of LN#1 and LN#2.",On the role of layer normalization,"It is interesting to note that based on our observations, the outlier weights of the two layer norms seem to be the opposite of each other.",On the role of layer normalization,result,
9938,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 68/98
==========================================================================================
9. On the role of layer normalization -- 9/18
------------------------------------------------------------------------------------------
It is interesting to note that based on our observations, the outlier weights of the two layer norms seem to be the opposite of each other.",3171004,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"The outlier weights 10 in specific dimensions of LNs are shown to be significantly influential on the model's performance (Kovaleva et al., 2021;Luo et al., 2021).",On the role of layer normalization,Figure 5 demonstrates the weight values in layer 11 and also the correlation of the outlier weights across layers.,On the role of layer normalization,discussion,
9939,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 69/98
==========================================================================================
9. On the role of layer normalization -- 10/18
------------------------------------------------------------------------------------------
Figure 5 demonstrates the weight values in layer 11 and also the correlation of the outlier weights across layers.",3171005,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"It is interesting to note that based on our observations, the outlier weights of the two layer norms seem to be the opposite of each other.",On the role of layer normalization,The large negative correlations confirm that the outlier weights work contrary to each other.,On the role of layer normalization,,
9940,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 70/98
==========================================================================================
9. On the role of layer normalization -- 11/18
------------------------------------------------------------------------------------------
The large negative correlations confirm that the outlier weights work contrary to each other.",3171006,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Figure 5 demonstrates the weight values in layer 11 and also the correlation of the outlier weights across layers.,On the role of layer normalization,"We speculate that the effect of outliers in the two layer norms is 10 We identify the dimensions where the weights are at least 3σ from the mean as outliers (Kovaleva et al., 2021).  ",On the role of layer normalization,result,
9941,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 71/98
==========================================================================================
9. On the role of layer normalization -- 12/18
------------------------------------------------------------------------------------------
We speculate that the effect of outliers in the two layer norms is 10 We identify the dimensions where the weights are at least 3σ from the mean as outliers (Kovaleva et al., 2021).  ",3171007,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The large negative correlations confirm that the outlier weights work contrary to each other.,On the role of layer normalization,partly cancelled out when both are considered. ,On the role of layer normalization,discussion#error,
9942,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 72/98
==========================================================================================
9. On the role of layer normalization -- 13/18
------------------------------------------------------------------------------------------
partly cancelled out when both are considered. ",3171008,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"We speculate that the effect of outliers in the two layer norms is 10 We identify the dimensions where the weights are at least 3σ from the mean as outliers (Kovaleva et al., 2021).  ",On the role of layer normalization,"As shown in Figure 2, the FFN and the second layer normalization are on top of the attention block.",On the role of layer normalization,discussion#error,
9943,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 73/98
==========================================================================================
9. On the role of layer normalization -- 14/18
------------------------------------------------------------------------------------------
As shown in Figure 2, the FFN and the second layer normalization are on top of the attention block.",3171009,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,partly cancelled out when both are considered. ,On the role of layer normalization,"However, N RESLN does not incorporate the components outside of the attention block.",On the role of layer normalization,,
9944,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 74/98
==========================================================================================
9. On the role of layer normalization -- 15/18
------------------------------------------------------------------------------------------
However, N RESLN does not incorporate the components outside of the attention block.",3171010,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"As shown in Figure 2, the FFN and the second layer normalization are on top of the attention block.",On the role of layer normalization,"As described in §3, in our local analysis method N ENC we incorporate the second layer normalization in the transformer's encoder (Figure 2), thus considering the whole encoder block (except FFN).",On the role of layer normalization,,
9945,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 75/98
==========================================================================================
9. On the role of layer normalization -- 16/18
------------------------------------------------------------------------------------------
As described in §3, in our local analysis method N ENC we incorporate the second layer normalization in the transformer's encoder (Figure 2), thus considering the whole encoder block (except FFN).",3171011,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"However, N RESLN does not incorporate the components outside of the attention block.",On the role of layer normalization,"Overall, our global method, GlobEnc, yields the best results among all the methods evaluated in our experiments.",On the role of layer normalization,,
9946,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 76/98
==========================================================================================
9. On the role of layer normalization -- 17/18
------------------------------------------------------------------------------------------
Overall, our global method, GlobEnc, yields the best results among all the methods evaluated in our experiments.",3171012,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"As described in §3, in our local analysis method N ENC we incorporate the second layer normalization in the transformer's encoder (Figure 2), thus considering the whole encoder block (except FFN).",On the role of layer normalization,"In general, Table 1 suggests that incorporating each component of the encoder will increase the correlation; however, the two layer normalizations should be considered together.",On the role of layer normalization,result,
9947,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 77/98
==========================================================================================
9. On the role of layer normalization -- 18/18
------------------------------------------------------------------------------------------
In general, Table 1 suggests that incorporating each component of the encoder will increase the correlation; however, the two layer normalizations should be considered together.",3171013,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Overall, our global method, GlobEnc, yields the best results among all the methods evaluated in our experiments.",On the role of layer normalization,"We carried out an additional analysis to verify if incorporating vector norms, residual connection and layer normalizations in individual layers is adequate for achieving high correlations, or if it is also necessary to aggregate them via rollout.",On the role of aggregation,result,
9948,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 78/98
==========================================================================================
10. On the role of aggregation -- 1/8
------------------------------------------------------------------------------------------
We carried out an additional analysis to verify if incorporating vector norms, residual connection and layer normalizations in individual layers is adequate for achieving high correlations, or if it is also necessary to aggregate them via rollout.",3171014,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In general, Table 1 suggests that incorporating each component of the encoder will increase the correlation; however, the two layer normalizations should be considered together.",On the role of layer normalization,Table 2 shows the correlation results in different layers for raw attributions (without aggregation) and for the aggregated attributions using the rollout method.,On the role of aggregation,,
9949,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 79/98
==========================================================================================
10. On the role of aggregation -- 2/8
------------------------------------------------------------------------------------------
Table 2 shows the correlation results in different layers for raw attributions (without aggregation) and for the aggregated attributions using the rollout method.",3171015,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"We carried out an additional analysis to verify if incorporating vector norms, residual connection and layer normalizations in individual layers is adequate for achieving high correlations, or if it is also necessary to aggregate them via rollout.",On the role of aggregation,"Applying rollout method on attribution maps up to each layer results in higher correlations with the saliency scores than the raw single layer attribution maps, especially in deeper layers.",On the role of aggregation,,
9950,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 80/98
==========================================================================================
10. On the role of aggregation -- 3/8
------------------------------------------------------------------------------------------
Applying rollout method on attribution maps up to each layer results in higher correlations with the saliency scores than the raw single layer attribution maps, especially in deeper layers.",3171016,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Table 2 shows the correlation results in different layers for raw attributions (without aggregation) and for the aggregated attributions using the rollout method.,On the role of aggregation,"Therefore, attention aggregation is essential for global input token attribution analysis. ",On the role of aggregation,result,
9951,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 81/98
==========================================================================================
10. On the role of aggregation -- 4/8
------------------------------------------------------------------------------------------
Therefore, attention aggregation is essential for global input token attribution analysis. ",3171017,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Applying rollout method on attribution maps up to each layer results in higher correlations with the saliency scores than the raw single layer attribution maps, especially in deeper layers.",On the role of aggregation,"An interesting point in Figure 3, which shows the correlation of the aggregated methods throughout the layers, is that the correlation curves flatten out after only a few layers.",On the role of aggregation,result,
9952,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 82/98
==========================================================================================
10. On the role of aggregation -- 5/8
------------------------------------------------------------------------------------------
An interesting point in Figure 3, which shows the correlation of the aggregated methods throughout the layers, is that the correlation curves flatten out after only a few layers.",3171018,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Therefore, attention aggregation is essential for global input token attribution analysis. ",On the role of aggregation,This indicates that BERT identifies decisive tokens only after the first few layers.,On the role of aggregation,result,
9953,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 83/98
==========================================================================================
10. On the role of aggregation -- 6/8
------------------------------------------------------------------------------------------
This indicates that BERT identifies decisive tokens only after the first few layers.",3171019,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"An interesting point in Figure 3, which shows the correlation of the aggregated methods throughout the layers, is that the correlation curves flatten out after only a few layers.",On the role of aggregation,The final layers only make minor adjustments to this order.,On the role of aggregation,result,
9954,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 84/98
==========================================================================================
10. On the role of aggregation -- 7/8
------------------------------------------------------------------------------------------
The final layers only make minor adjustments to this order.",3171020,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This indicates that BERT identifies decisive tokens only after the first few layers.,On the role of aggregation,"Nevertheless, it is worth noting that the order of attribution does not necessarily imply the model's final decision and the final result may still change for the better or worse (Zhou et al., 2020).",On the role of aggregation,result,
9955,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 85/98
==========================================================================================
10. On the role of aggregation -- 8/8
------------------------------------------------------------------------------------------
Nevertheless, it is worth noting that the order of attribution does not necessarily imply the model's final decision and the final result may still change for the better or worse (Zhou et al., 2020).",3171021,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,The final layers only make minor adjustments to this order.,On the role of aggregation,"To qualitatively answer if the aggregated attribution maps provide plausible and meaningful interpretations, we take a closer look at the attribution maps generated by GlobEnc.",Qualitative analysis,discussion,
9956,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 86/98
==========================================================================================
11. Qualitative analysis -- 1/13
------------------------------------------------------------------------------------------
To qualitatively answer if the aggregated attribution maps provide plausible and meaningful interpretations, we take a closer look at the attribution maps generated by GlobEnc.",3171022,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Nevertheless, it is worth noting that the order of attribution does not necessarily imply the model's final decision and the final result may still change for the better or worse (Zhou et al., 2020).",On the role of aggregation,Figure 1 shows the GlobEnc attribution of the model trained on SST-2.,Qualitative analysis,,
9957,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 87/98
==========================================================================================
11. Qualitative analysis -- 2/13
------------------------------------------------------------------------------------------
Figure 1 shows the GlobEnc attribution of the model trained on SST-2.",3171023,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"To qualitatively answer if the aggregated attribution maps provide plausible and meaningful interpretations, we take a closer look at the attribution maps generated by GlobEnc.",Qualitative analysis,Each layer demonstrates the [CLS] token's aggregated attribution to input tokens up to the corresponding layer.,Qualitative analysis,,
9958,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 88/98
==========================================================================================
11. Qualitative analysis -- 3/13
------------------------------------------------------------------------------------------
Each layer demonstrates the [CLS] token's aggregated attribution to input tokens up to the corresponding layer.",3171024,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Figure 1 shows the GlobEnc attribution of the model trained on SST-2.,Qualitative analysis,"The example inputs are ""a deep and meaningful film.""",Qualitative analysis,,
9959,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 89/98
==========================================================================================
11. Qualitative analysis -- 4/13
------------------------------------------------------------------------------------------
The example inputs are ""a deep and meaningful film.""",3171025,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Each layer demonstrates the [CLS] token's aggregated attribution to input tokens up to the corresponding layer.,Qualitative analysis,"and ""big fat waste of time.",Qualitative analysis,,
9960,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 90/98
==========================================================================================
11. Qualitative analysis -- 5/13
------------------------------------------------------------------------------------------
and ""big fat waste of time.",3171026,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"The example inputs are ""a deep and meaningful film.""",Qualitative analysis,""", both correctly classified by the model.",Qualitative analysis,,
9961,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 91/98
==========================================================================================
11. Qualitative analysis -- 6/13
------------------------------------------------------------------------------------------
"", both correctly classified by the model.",3171027,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"and ""big fat waste of time.",Qualitative analysis,"In both cases, GlobEnc focuses on the relevant words for sentiment classification, i.e., ""meaningful"" and ""waste"".",Qualitative analysis,,
9962,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 92/98
==========================================================================================
11. Qualitative analysis -- 7/13
------------------------------------------------------------------------------------------
In both cases, GlobEnc focuses on the relevant words for sentiment classification, i.e., ""meaningful"" and ""waste"".",3171028,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,""", both correctly classified by the model.",Qualitative analysis,"An interesting observation in Figure 1 is that in the first few layers, the [CLS] token mostly attends to itself while other tokens have marginal impact.",Qualitative analysis,,
9963,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 93/98
==========================================================================================
11. Qualitative analysis -- 8/13
------------------------------------------------------------------------------------------
An interesting observation in Figure 1 is that in the first few layers, the [CLS] token mostly attends to itself while other tokens have marginal impact.",3171029,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In both cases, GlobEnc focuses on the relevant words for sentiment classification, i.e., ""meaningful"" and ""waste"".",Qualitative analysis,"As the representations get more contextualized in deeper layers, the attribution correctly shifts to the words which indicate the sentiment of the sentence.",Qualitative analysis,,res?
9964,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 94/98
==========================================================================================
11. Qualitative analysis -- 9/13
------------------------------------------------------------------------------------------
As the representations get more contextualized in deeper layers, the attribution correctly shifts to the words which indicate the sentiment of the sentence.",3171030,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"An interesting observation in Figure 1 is that in the first few layers, the [CLS] token mostly attends to itself while other tokens have marginal impact.",Qualitative analysis,"12 More examples from MNLI and SST2 datasets, including misclassified examples are available at §A.3.",Qualitative analysis,,
9965,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 95/98
==========================================================================================
11. Qualitative analysis -- 10/13
------------------------------------------------------------------------------------------
12 More examples from MNLI and SST2 datasets, including misclassified examples are available at §A.3.",3171031,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"As the representations get more contextualized in deeper layers, the attribution correctly shifts to the words which indicate the sentiment of the sentence.",Qualitative analysis,"Our qualitative analysis suggests that GlobEnc can be useful for a reasonable interpretation of attention mechanism in BERT, ELECTRA, and possibly any other transformer-based model. ",Qualitative analysis,,
9966,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 96/98
==========================================================================================
11. Qualitative analysis -- 11/13
------------------------------------------------------------------------------------------
Our qualitative analysis suggests that GlobEnc can be useful for a reasonable interpretation of attention mechanism in BERT, ELECTRA, and possibly any other transformer-based model. ",3171032,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"12 More examples from MNLI and SST2 datasets, including misclassified examples are available at §A.3.",Qualitative analysis,11 WRES is the only exception with a constant increase; this method is gradually and artificially corrected by NENC context mixing ratios. ,Qualitative analysis,result,
9967,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 97/98
==========================================================================================
11. Qualitative analysis -- 12/13
------------------------------------------------------------------------------------------
11 WRES is the only exception with a constant increase; this method is gradually and artificially corrected by NENC context mixing ratios. ",3171033,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Our qualitative analysis suggests that GlobEnc can be useful for a reasonable interpretation of attention mechanism in BERT, ELECTRA, and possibly any other transformer-based model. ",Qualitative analysis,"12 Complete attention maps in Figure A.3 show that, similarly to [CLS], other tokens also focus on sentiment tokens.",Qualitative analysis,result,
9968,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
5. Experiments -- 98/98
==========================================================================================
11. Qualitative analysis -- 13/13
------------------------------------------------------------------------------------------
12 Complete attention maps in Figure A.3 show that, similarly to [CLS], other tokens also focus on sentiment tokens.",3171034,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,11 WRES is the only exception with a constant increase; this method is gradually and artificially corrected by NENC context mixing ratios. ,Qualitative analysis,"While numerous studies have used attention weights to analyze and interpret the self-attention mechanism (Clark et al., 2019;Kovaleva et al., 2019;Reif et al., 2019;Htut et al., 2019), the use of mere attention weights to explain a model's inner workings has been an active topic of debate (Serrano and Smith, 2019;Jain and Wallace, 2019;Wiegreffe and Pinter, 2019).",Related Work,,
9969,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 1/8
==========================================================================================
In this work, we proposed a novel method for single layer token attribution analysis which incorporates the whole encoder layer, i.e., the attention block and the output layer normalization.",3171035,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,We showed that attention rollout does not perform well on the raw attention maps of language models fine-tuned on downstream tasks and that this problem can be resolved by utilizing attribution norms.,Related Work,"When aggregated across layers using the rollout method, our technique achieves quantitatively and qualitatively plausible results.",Conclusions,contribution,
9970,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 2/8
==========================================================================================
When aggregated across layers using the rollout method, our technique achieves quantitatively and qualitatively plausible results.",3171036,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"In this work, we proposed a novel method for single layer token attribution analysis which incorporates the whole encoder layer, i.e., the attention block and the output layer normalization.",Conclusions,"Our evaluation of different analysis methods provided evidence on roles played by individual components of the encoder layer, i.e., the vector norms, the residual connections, and the layer normalizations.",Conclusions,result,
9971,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 3/8
==========================================================================================
Our evaluation of different analysis methods provided evidence on roles played by individual components of the encoder layer, i.e., the vector norms, the residual connections, and the layer normalizations.",3171037,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"When aggregated across layers using the rollout method, our technique achieves quantitatively and qualitatively plausible results.",Conclusions,"Furthermore, our in-depth analysis suggested that the two layer normalizations in the encoder layer counteract each other; hence, it is important to couple them for an accurate analysis. ",Conclusions,result,
9972,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 4/8
==========================================================================================
Furthermore, our in-depth analysis suggested that the two layer normalizations in the encoder layer counteract each other; hence, it is important to couple them for an accurate analysis. ",3171038,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Our evaluation of different analysis methods provided evidence on roles played by individual components of the encoder layer, i.e., the vector norms, the residual connections, and the layer normalizations.",Conclusions,"Additionally, using a newly proposed and improved version of Hidden Token Attribution, we demonstrated that encoder-based attribution analysis is more accurate when compared to other partial solutions in a single layer (local-level).",Conclusions,result,
9973,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 5/8
==========================================================================================
Additionally, using a newly proposed and improved version of Hidden Token Attribution, we demonstrated that encoder-based attribution analysis is more accurate when compared to other partial solutions in a single layer (local-level).",3171039,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Furthermore, our in-depth analysis suggested that the two layer normalizations in the encoder layer counteract each other; hence, it is important to couple them for an accurate analysis. ",Conclusions,This is consistent with our global observations.,Conclusions,contribution#result,"""newly proposed and improved version of HTA"" = contribution ? alors que le ""we demonstrated..."" result?"
9974,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 6/8
==========================================================================================
This is consistent with our global observations.",3171040,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,"Additionally, using a newly proposed and improved version of Hidden Token Attribution, we demonstrated that encoder-based attribution analysis is more accurate when compared to other partial solutions in a single layer (local-level).",Conclusions,Quantifying global input token attribution based on our work can provide a meaningful explanation of the whole model's behavior.,Conclusions,,
9975,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 7/8
==========================================================================================
Quantifying global input token attribution based on our work can provide a meaningful explanation of the whole model's behavior.",3171041,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,This is consistent with our global observations.,Conclusions,"In future work, we plan to apply our global analysis method on various datasets and models, to provide valuable insights into model decisions and interpretability.     ",Conclusions,impact,
9976,"{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers
==========================================================================================
7. Conclusions -- 8/8
==========================================================================================
In future work, we plan to apply our global analysis method on various datasets and models, to provide valuable insights into model decisions and interpretability.     ",3171042,{G}lob{E}nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"0. abstract
1. Introduction
2. 2021
3. Background
4. Methodology
5. Experiments
6. Related Work
7. Conclusions
",2022,Quantifying global input token attribution based on our work can provide a meaningful explanation of the whole model's behavior.,Conclusions,,,directions,
9977,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 1/9
==========================================================================================
Low-literate users with intellectual or developmental disabilities (IDD) and/or complex communication needs (CCN) require specific writing support.",546265,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,,,"We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache (LS; German term for easy-toread German), slightly extended within and beyond the inner-sentential syntactic level.",abstract,background-AIC,
9978,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 2/9
==========================================================================================
We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache (LS; German term for easy-toread German), slightly extended within and beyond the inner-sentential syntactic level.",546266,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Low-literate users with intellectual or developmental disabilities (IDD) and/or complex communication needs (CCN) require specific writing support.,abstract,The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator.,abstract,contribution,
9979,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 3/9
==========================================================================================
The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator.",546267,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache (LS; German term for easy-toread German), slightly extended within and beyond the inner-sentential syntactic level.",abstract,"Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence.",abstract,contribution,
9980,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 4/9
==========================================================================================
Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence.",546268,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator.,abstract,"In earlier development phases, the system was evaluated with different groups of substitute users.",abstract,contribution,
9981,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 5/9
==========================================================================================
In earlier development phases, the system was evaluated with different groups of substitute users.",546269,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence.",abstract,"Here, we report a case study with seven low-literate users with IDD.",abstract,,
9982,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 6/9
==========================================================================================
Here, we report a case study with seven low-literate users with IDD.",546270,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In earlier development phases, the system was evaluated with different groups of substitute users.",abstract,1,abstract,outline-AIC,
9983,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 7/9
==========================================================================================
1",546271,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Here, we report a case study with seven low-literate users with IDD.",abstract,"They may be supported by rule-based validation tools (for LS, see, e.g., languagetool.org/de/leichtesprache/) or automatic text-simplification (cf.",abstract,error,
9984,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 8/9
==========================================================================================
They may be supported by rule-based validation tools (for LS, see, e.g., languagetool.org/de/leichtesprache/) or automatic text-simplification (cf.",546272,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,1,abstract,"Ebling et al., 2022; for English, see, e.g., paperswithcode.com/task/text-simplification)",abstract,contribution#error,
9985,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
0. abstract -- 9/9
==========================================================================================
Ebling et al., 2022; for English, see, e.g., paperswithcode.com/task/text-simplification)",546273,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"They may be supported by rule-based validation tools (for LS, see, e.g., languagetool.org/de/leichtesprache/) or automatic text-simplification (cf.",abstract,Recent studies report that more than 10 percent of German-speaking adults have low literacy skills (cf.,Introduction,contribution#error,
9986,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 1/26
==========================================================================================
Recent studies report that more than 10 percent of German-speaking adults have low literacy skills (cf.",546274,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Ebling et al., 2022; for English, see, e.g., paperswithcode.com/task/text-simplification)",abstract,"Anke Grotlüschen et al., 2020).",Introduction,background-AIC#error#rw,
9987,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 2/26
==========================================================================================
Anke Grotlüschen et al., 2020).",546275,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Recent studies report that more than 10 percent of German-speaking adults have low literacy skills (cf.,Introduction,"People with intellectual and developmental disabilities (IDD) and/or complex communication needs (CCN) often belong to this group (Light et al., 2019;Grotlüschen and Buddeberg, 2020; hereafter referred to as the target group, or simply the users). ",Introduction,background-AIC#error#rw,
9988,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 3/26
==========================================================================================
People with intellectual and developmental disabilities (IDD) and/or complex communication needs (CCN) often belong to this group (Light et al., 2019;Grotlüschen and Buddeberg, 2020; hereafter referred to as the target group, or simply the users). ",546276,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Anke Grotlüschen et al., 2020).",Introduction,"Leichte Sprache (LS; easy-to-read German), a simplified variety of German, was developed for the target group as part of the plain language movement of the 2000s (cf.",Introduction,background-AIC#rw,
9989,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 4/26
==========================================================================================
Leichte Sprache (LS; easy-to-read German), a simplified variety of German, was developed for the target group as part of the plain language movement of the 2000s (cf.",546277,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"People with intellectual and developmental disabilities (IDD) and/or complex communication needs (CCN) often belong to this group (Light et al., 2019;Grotlüschen and Buddeberg, 2020; hereafter referred to as the target group, or simply the users). ",Introduction,"Inclusion Europe, 2009;BITV2.0, 2011, Netzwerk Leichte Sprache, 2013, or Bredel and Maaß, 2016). ",Introduction,background-AIC#error#rw,
9990,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 5/26
==========================================================================================
Inclusion Europe, 2009;BITV2.0, 2011, Netzwerk Leichte Sprache, 2013, or Bredel and Maaß, 2016). ",546278,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Leichte Sprache (LS; easy-to-read German), a simplified variety of German, was developed for the target group as part of the plain language movement of the 2000s (cf.",Introduction,"Inclusion necessitates technical assistance to barrier-free participation in all social spheres (Hirschberg and Lindmeier, 2013).",Introduction,background-AIC#error#rw,
9991,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 6/26
==========================================================================================
Inclusion necessitates technical assistance to barrier-free participation in all social spheres (Hirschberg and Lindmeier, 2013).",546279,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Inclusion Europe, 2009;BITV2.0, 2011, Netzwerk Leichte Sprache, 2013, or Bredel and Maaß, 2016). ",Introduction,"In the following, we investigate the extent to which natural language processing (NLP) can support the users while writing.",Introduction,background-AIC#rw,
9992,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 7/26
==========================================================================================
In the following, we investigate the extent to which natural language processing (NLP) can support the users while writing.",546280,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Inclusion necessitates technical assistance to barrier-free participation in all social spheres (Hirschberg and Lindmeier, 2013).",Introduction,"An increasing variety of writingsupport systems based on natural language generation (NLG) attract attention (for their prospects, see, e.g., Dale and Viethen, 2021; for approaches based on deep learning, see Otter et al., 2021).",Introduction,outline-AIC,
9993,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 8/26
==========================================================================================
An increasing variety of writingsupport systems based on natural language generation (NLG) attract attention (for their prospects, see, e.g., Dale and Viethen, 2021; for approaches based on deep learning, see Otter et al., 2021).",546281,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In the following, we investigate the extent to which natural language processing (NLP) can support the users while writing.",Introduction,Adaptive behavior like automatically modifying the written text incurs the risk that usersdue to low-literacy-do not carefully check whether or not the changes express the intended meaning.,Introduction,background-AIC#rw,
9994,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 9/26
==========================================================================================
Adaptive behavior like automatically modifying the written text incurs the risk that usersdue to low-literacy-do not carefully check whether or not the changes express the intended meaning.",546282,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"An increasing variety of writingsupport systems based on natural language generation (NLG) attract attention (for their prospects, see, e.g., Dale and Viethen, 2021; for approaches based on deep learning, see Otter et al., 2021).",Introduction,Missing is a text base produced by the target group.,Introduction,background-AIC,
9995,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 10/26
==========================================================================================
Missing is a text base produced by the target group.",546283,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Adaptive behavior like automatically modifying the written text incurs the risk that usersdue to low-literacy-do not carefully check whether or not the changes express the intended meaning.,Introduction,"In general, text in LS is produced by authors proficient in standard German 1 .",Introduction,background-AIC,
9996,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 11/26
==========================================================================================
In general, text in LS is produced by authors proficient in standard German 1 .",546284,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Missing is a text base produced by the target group.,Introduction,"Thus, suggestions by the system that are automatically extracted from given LS text might not be perceived as helpful but irritating, let alone unintentionally patronizing.",Introduction,background-AIC,
9997,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 12/26
==========================================================================================
Thus, suggestions by the system that are automatically extracted from given LS text might not be perceived as helpful but irritating, let alone unintentionally patronizing.",546285,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In general, text in LS is produced by authors proficient in standard German 1 .",Introduction,"In addition, interactions with the user pose additional challenges, such as designing an accessible interface (cf.",Introduction,background-AIC,
9998,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 13/26
==========================================================================================
In addition, interactions with the user pose additional challenges, such as designing an accessible interface (cf.",546286,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Thus, suggestions by the system that are automatically extracted from given LS text might not be perceived as helpful but irritating, let alone unintentionally patronizing.",Introduction,"Nganji and Nggada, 2011).",Introduction,background-AIC#error#rw,
9999,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 14/26
==========================================================================================
Nganji and Nggada, 2011).",546287,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In addition, interactions with the user pose additional challenges, such as designing an accessible interface (cf.",Introduction,"In essence, supportive interaction patterns should not overtax the user. ",Introduction,background-AIC#error#rw,
10000,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 15/26
==========================================================================================
In essence, supportive interaction patterns should not overtax the user. ",546288,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Nganji and Nggada, 2011).",Introduction,"In the present paper, we describe EasyTalk for fast, correct and reader-centered writing in Extended Leichte Sprache (ELS; Harbusch and Steinmetz, 2022; ELS extends LS in several respects, for instance, with high frequent constructions from spoken German that incorporate the target group's ways of articulating their thoughts; for previous prototypes of EasyTalk, see Steinmetz and Harbusch, 2020;2021a/b).",Introduction,background-AIC,
10001,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 16/26
==========================================================================================
In the present paper, we describe EasyTalk for fast, correct and reader-centered writing in Extended Leichte Sprache (ELS; Harbusch and Steinmetz, 2022; ELS extends LS in several respects, for instance, with high frequent constructions from spoken German that incorporate the target group's ways of articulating their thoughts; for previous prototypes of EasyTalk, see Steinmetz and Harbusch, 2020;2021a/b).",546289,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In essence, supportive interaction patterns should not overtax the user. ",Introduction,"On the sentential level, a natural-language paraphrase generator suggests correctly inflected word forms.",Introduction,contribution#outline-AIC#rw,
10002,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 17/26
==========================================================================================
On the sentential level, a natural-language paraphrase generator suggests correctly inflected word forms.",546290,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In the present paper, we describe EasyTalk for fast, correct and reader-centered writing in Extended Leichte Sprache (ELS; Harbusch and Steinmetz, 2022; ELS extends LS in several respects, for instance, with high frequent constructions from spoken German that incorporate the target group's ways of articulating their thoughts; for previous prototypes of EasyTalk, see Steinmetz and Harbusch, 2020;2021a/b).",Introduction,It pursues the overall correctness and completeness of the sentence and provides the correct German word ordering. ,Introduction,background-AIC,
10003,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 18/26
==========================================================================================
It pursues the overall correctness and completeness of the sentence and provides the correct German word ordering. ",546291,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"On the sentential level, a natural-language paraphrase generator suggests correctly inflected word forms.",Introduction,"In order to improve textunderstandability and text-coherence over the entire text, EasyTalk reminds the user to add audi-ence-design features within a clause (Bell, 1984).",Introduction,background-AIC,
10004,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 19/26
==========================================================================================
In order to improve textunderstandability and text-coherence over the entire text, EasyTalk reminds the user to add audi-ence-design features within a clause (Bell, 1984).",546292,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,It pursues the overall correctness and completeness of the sentence and provides the correct German word ordering. ,Introduction,"The user is invited to clarify the discourse structure by adding connectors (inspired by Rhetorical-Structure Theory (RST); see Hovy, 1988 andMann andThompson, 1988), thus explicitly marking the relationship between the simple clauses.",Introduction,contribution#rw,
10005,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 20/26
==========================================================================================
The user is invited to clarify the discourse structure by adding connectors (inspired by Rhetorical-Structure Theory (RST); see Hovy, 1988 andMann andThompson, 1988), thus explicitly marking the relationship between the simple clauses.",546293,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In order to improve textunderstandability and text-coherence over the entire text, EasyTalk reminds the user to add audi-ence-design features within a clause (Bell, 1984).",Introduction,(SVO order is mandatory in declarative main clauses of (E)LS). ,Introduction,contribution#rw,
10006,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 21/26
==========================================================================================
(SVO order is mandatory in declarative main clauses of (E)LS). ",546294,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"The user is invited to clarify the discourse structure by adding connectors (inspired by Rhetorical-Structure Theory (RST); see Hovy, 1988 andMann andThompson, 1988), thus explicitly marking the relationship between the simple clauses.",Introduction,"In the following, we first summarize the state of the art in writing-support systems.",Introduction,background-AIC,
10007,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 22/26
==========================================================================================
In the following, we first summarize the state of the art in writing-support systems.",546295,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,(SVO order is mandatory in declarative main clauses of (E)LS). ,Introduction,"Then, we outline EasyTalk's mechanisms for supporting textproduction both within and between sentences.",Introduction,outline-AIC,
10008,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 23/26
==========================================================================================
Then, we outline EasyTalk's mechanisms for supporting textproduction both within and between sentences.",546296,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In the following, we first summarize the state of the art in writing-support systems.",Introduction,"In Section 4, we report the results of a case study we recently conducted with seven users from the target group.",Introduction,outline-AIC,
10009,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 24/26
==========================================================================================
In Section 4, we report the results of a case study we recently conducted with seven users from the target group.",546297,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Then, we outline EasyTalk's mechanisms for supporting textproduction both within and between sentences.",Introduction,"The results are compared with observations from earlier evaluations with other user groups, in particular with L2 learners of German.",Introduction,background-AIC,
10010,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 25/26
==========================================================================================
The results are compared with observations from earlier evaluations with other user groups, in particular with L2 learners of German.",546298,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In Section 4, we report the results of a case study we recently conducted with seven users from the target group.",Introduction,The paper ends with a discussion of open issues and desirable future work.,Introduction,outline-AIC,
10011,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
1. Introduction -- 26/26
==========================================================================================
The paper ends with a discussion of open issues and desirable future work.",546299,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"The results are compared with observations from earlier evaluations with other user groups, in particular with L2 learners of German.",Introduction,This section summarizes the state of the art in writing systems focusing on German where particular problems arise from rich morphology and free word ordering.,Writing support systems for users with IDD and/or CCN,outline-AIC,
10012,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 1/74
==========================================================================================
In general, it is best practice to identify and correct usability flaws in software before it is made available to the user (see, e.g., Holzinger, 2005).",546300,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Now, we report the recent evaluation study.",Sentence-combining support,"For the target group, the first impression is particularly crucial for the acceptance of a system.",Evaluation,rw,
10013,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 2/74
==========================================================================================
For the target group, the first impression is particularly crucial for the acceptance of a system.",546301,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In general, it is best practice to identify and correct usability flaws in software before it is made available to the user (see, e.g., Holzinger, 2005).",Evaluation,"AAC software is often abandoned after a short period of use (see, e.g., Dawe, 2006;Fager et al., 2006;Waller, 2019). ",Evaluation,background-AIC,
10014,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 3/74
==========================================================================================
AAC software is often abandoned after a short period of use (see, e.g., Dawe, 2006;Fager et al., 2006;Waller, 2019). ",546302,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"For the target group, the first impression is particularly crucial for the acceptance of a system.",Evaluation,"Maturing versions of EasyTalk were previously evaluated in several tests with substitute user groups (see, e.g., Steinmetz andHarbusch, 2020, 2021a) such as experts in the field of accessible communication and L2 learners (CEFR-level A1-B1 and differing computer skills).",Evaluation,,
10015,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 4/74
==========================================================================================
Maturing versions of EasyTalk were previously evaluated in several tests with substitute user groups (see, e.g., Steinmetz andHarbusch, 2020, 2021a) such as experts in the field of accessible communication and L2 learners (CEFR-level A1-B1 and differing computer skills).",546303,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"AAC software is often abandoned after a short period of use (see, e.g., Dawe, 2006;Fager et al., 2006;Waller, 2019). ",Evaluation,"Nevertheless, it is essential to test the system with the actual target group (cf.",Evaluation,,
10016,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 5/74
==========================================================================================
Nevertheless, it is essential to test the system with the actual target group (cf.",546304,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Maturing versions of EasyTalk were previously evaluated in several tests with substitute user groups (see, e.g., Steinmetz andHarbusch, 2020, 2021a) such as experts in the field of accessible communication and L2 learners (CEFR-level A1-B1 and differing computer skills).",Evaluation,"Newell and Gregor, 2000;Henry, 2007;Nganji and Nggada, 2011, for user sensitive, inclusive design of accessible, disability-aware software).",Evaluation,,
10017,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 6/74
==========================================================================================
Newell and Gregor, 2000;Henry, 2007;Nganji and Nggada, 2011, for user sensitive, inclusive design of accessible, disability-aware software).",546305,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Nevertheless, it is essential to test the system with the actual target group (cf.",Evaluation,"Here, we compare the previous findings with observations from the recent study.",Evaluation,,
10018,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 7/74
==========================================================================================
Here, we compare the previous findings with observations from the recent study.",546306,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Newell and Gregor, 2000;Henry, 2007;Nganji and Nggada, 2011, for user sensitive, inclusive design of accessible, disability-aware software).",Evaluation,"Testing with people with disabilities presents unique challenges and increased organizational effort (cf. Lazar, 2017:",Test setup and participants,,
10019,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 8/74
==========================================================================================
0. Test setup and participants -- 1/8
------------------------------------------------------------------------------------------
Testing with people with disabilities presents unique challenges and increased organizational effort (cf. Lazar, 2017:",546307,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Here, we compare the previous findings with observations from the recent study.",Evaluation,"Chapter 16, for an overview)-for example, special precautions currently need to be taken in direct contact with the target group which is particularly vulnerable to COVID-19 (cf.",Test setup and participants,,
10020,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 9/74
==========================================================================================
0. Test setup and participants -- 2/8
------------------------------------------------------------------------------------------
Chapter 16, for an overview)-for example, special precautions currently need to be taken in direct contact with the target group which is particularly vulnerable to COVID-19 (cf.",546308,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Testing with people with disabilities presents unique challenges and increased organizational effort (cf. Lazar, 2017:",Test setup and participants,"Rödler, 2020;Portal et al., 2021).",Test setup and participants,,
10021,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 10/74
==========================================================================================
0. Test setup and participants -- 3/8
------------------------------------------------------------------------------------------
Rödler, 2020;Portal et al., 2021).",546309,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Chapter 16, for an overview)-for example, special precautions currently need to be taken in direct contact with the target group which is particularly vulnerable to COVID-19 (cf.",Test setup and participants,"There-fore, we conducted a qualitative case study aiming to uncover the biggest usability flaws in our software with only a handful of participants (cf. discount testing; Nielson, 1989). ",Test setup and participants,,
10022,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 11/74
==========================================================================================
0. Test setup and participants -- 4/8
------------------------------------------------------------------------------------------
There-fore, we conducted a qualitative case study aiming to uncover the biggest usability flaws in our software with only a handful of participants (cf. discount testing; Nielson, 1989). ",546310,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Rödler, 2020;Portal et al., 2021).",Test setup and participants,"For this purpose, we asked eight Germanspeaking participants, aged 18-25, with different conditions, writing and computer skills (cf.",Test setup and participants,,
10023,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 12/74
==========================================================================================
0. Test setup and participants -- 5/8
------------------------------------------------------------------------------------------
For this purpose, we asked eight Germanspeaking participants, aged 18-25, with different conditions, writing and computer skills (cf.",546311,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"There-fore, we conducted a qualitative case study aiming to uncover the biggest usability flaws in our software with only a handful of participants (cf. discount testing; Nielson, 1989). ",Test setup and participants,"Table 1), to exploratively test the system in sessions from 25 to 40 minutes.",Test setup and participants,,
10024,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 13/74
==========================================================================================
0. Test setup and participants -- 6/8
------------------------------------------------------------------------------------------
Table 1), to exploratively test the system in sessions from 25 to 40 minutes.",546312,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"For this purpose, we asked eight Germanspeaking participants, aged 18-25, with different conditions, writing and computer skills (cf.",Test setup and participants,"The tests were performed under normal room lighting on a laptop with 15"" display screen resolution of 1920x1080.",Test setup and participants,,
10025,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 14/74
==========================================================================================
0. Test setup and participants -- 7/8
------------------------------------------------------------------------------------------
The tests were performed under normal room lighting on a laptop with 15"" display screen resolution of 1920x1080.",546313,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Table 1), to exploratively test the system in sessions from 25 to 40 minutes.",Test setup and participants,"EasyTalk had to be operated in the same setup (e.g., displaying the ARASAAC symbols) by all participants using the provided laptop keyboard and an external mouse.",Test setup and participants,,
10026,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 15/74
==========================================================================================
0. Test setup and participants -- 8/8
------------------------------------------------------------------------------------------
EasyTalk had to be operated in the same setup (e.g., displaying the ARASAAC symbols) by all participants using the provided laptop keyboard and an external mouse.",546314,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"The tests were performed under normal room lighting on a laptop with 15"" display screen resolution of 1920x1080.",Test setup and participants,Since predefined tasks-like in a usability study-might exert unnecessary pressure and frustration on the target group which could distract from evaluating the specific communication features in question we aimed to create casual situations in our experimental set-up that avoids unintentionally scrutinizing our participant's personal skills.,Test procedure,,
10027,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 16/74
==========================================================================================
1. Test procedure -- 1/18
------------------------------------------------------------------------------------------
Since predefined tasks-like in a usability study-might exert unnecessary pressure and frustration on the target group which could distract from evaluating the specific communication features in question we aimed to create casual situations in our experimental set-up that avoids unintentionally scrutinizing our participant's personal skills.",546315,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"EasyTalk had to be operated in the same setup (e.g., displaying the ARASAAC symbols) by all participants using the provided laptop keyboard and an external mouse.",Test setup and participants,"To provide a feeling of security, the individual caregiver (or the writing workshop leader) and only one person from the evaluation team (the interviewer) were present during the sessions.",Test procedure,,
10028,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 17/74
==========================================================================================
1. Test procedure -- 2/18
------------------------------------------------------------------------------------------
To provide a feeling of security, the individual caregiver (or the writing workshop leader) and only one person from the evaluation team (the interviewer) were present during the sessions.",546316,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Since predefined tasks-like in a usability study-might exert unnecessary pressure and frustration on the target group which could distract from evaluating the specific communication features in question we aimed to create casual situations in our experimental set-up that avoids unintentionally scrutinizing our participant's personal skills.,Test procedure,Each session started with a brief warmup to break the ice. ,Test procedure,,
10029,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 18/74
==========================================================================================
1. Test procedure -- 3/18
------------------------------------------------------------------------------------------
Each session started with a brief warmup to break the ice. ",546317,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"To provide a feeling of security, the individual caregiver (or the writing workshop leader) and only one person from the evaluation team (the interviewer) were present during the sessions.",Test procedure,Standard evaluation techniques like thinking aloud or UX questionnaires 15 would overtax the target group.,Test procedure,,
10030,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 19/74
==========================================================================================
1. Test procedure -- 4/18
------------------------------------------------------------------------------------------
Standard evaluation techniques like thinking aloud or UX questionnaires 15 would overtax the target group.",546318,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Each session started with a brief warmup to break the ice. ,Test procedure,"Complex, open-end questions are particularly difficult for participants with CCN or severe ASD.",Test procedure,,
10031,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 20/74
==========================================================================================
1. Test procedure -- 5/18
------------------------------------------------------------------------------------------
Complex, open-end questions are particularly difficult for participants with CCN or severe ASD.",546319,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Standard evaluation techniques like thinking aloud or UX questionnaires 15 would overtax the target group.,Test procedure,"Thus, we abstained from systematically switching between typing and judging this process in a structured interview with post-task question as another potential source of irritation due to test subjects feeling pressured to make a statement.",Test procedure,,
10032,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 21/74
==========================================================================================
1. Test procedure -- 6/18
------------------------------------------------------------------------------------------
Thus, we abstained from systematically switching between typing and judging this process in a structured interview with post-task question as another potential source of irritation due to test subjects feeling pressured to make a statement.",546320,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Complex, open-end questions are particularly difficult for participants with CCN or severe ASD.",Test procedure,"Nevertheless, we encouraged the participants to give comments.",Test procedure,,
10033,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 22/74
==========================================================================================
1. Test procedure -- 7/18
------------------------------------------------------------------------------------------
Nevertheless, we encouraged the participants to give comments.",546321,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Thus, we abstained from systematically switching between typing and judging this process in a structured interview with post-task question as another potential source of irritation due to test subjects feeling pressured to make a statement.",Test procedure,"As far as the participants complied, we elaborated on raised topics.",Test procedure,,
10034,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 23/74
==========================================================================================
1. Test procedure -- 8/18
------------------------------------------------------------------------------------------
As far as the participants complied, we elaborated on raised topics.",546322,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Nevertheless, we encouraged the participants to give comments.",Test procedure,"Besides observing the participants as they typed their conceptual message and logging the users' actions, we decided to employ eye tracking as far as the participants gave their permission and conditions allowed for recording eye movements with a Tobii Pro Nano 16 to obtain objective information (cf.",Test procedure,,
10035,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 24/74
==========================================================================================
1. Test procedure -- 9/18
------------------------------------------------------------------------------------------
Besides observing the participants as they typed their conceptual message and logging the users' actions, we decided to employ eye tracking as far as the participants gave their permission and conditions allowed for recording eye movements with a Tobii Pro Nano 16 to obtain objective information (cf.",546323,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"As far as the participants complied, we elaborated on raised topics.",Test procedure,"Bojko, 2005). ",Test procedure,,
10036,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 25/74
==========================================================================================
1. Test procedure -- 10/18
------------------------------------------------------------------------------------------
Bojko, 2005). ",546324,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Besides observing the participants as they typed their conceptual message and logging the users' actions, we decided to employ eye tracking as far as the participants gave their permission and conditions allowed for recording eye movements with a Tobii Pro Nano 16 to obtain objective information (cf.",Test procedure,"To explain how the system works, the interviewer wrote one sample sentence in EasyTalk: Die Sonne scheint heute.",Test procedure,,
10037,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 26/74
==========================================================================================
1. Test procedure -- 11/18
------------------------------------------------------------------------------------------
To explain how the system works, the interviewer wrote one sample sentence in EasyTalk: Die Sonne scheint heute.",546325,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Bojko, 2005). ",Test procedure,'The sun shines today.'.,Test procedure,,
10038,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 27/74
==========================================================================================
1. Test procedure -- 12/18
------------------------------------------------------------------------------------------
'The sun shines today.'.",546326,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"To explain how the system works, the interviewer wrote one sample sentence in EasyTalk: Die Sonne scheint heute.",Test procedure,The participants could opt for rehearsing the example interactively with the interviewer.,Test procedure,,
10039,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 28/74
==========================================================================================
1. Test procedure -- 13/18
------------------------------------------------------------------------------------------
The participants could opt for rehearsing the example interactively with the interviewer.",546327,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,'The sun shines today.'.,Test procedure,"Afterwards, all participants were invited to explore the system freely.",Test procedure,,
10040,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 29/74
==========================================================================================
1. Test procedure -- 14/18
------------------------------------------------------------------------------------------
Afterwards, all participants were invited to explore the system freely.",546328,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The participants could opt for rehearsing the example interactively with the interviewer.,Test procedure,"(Before the experiment, the leader of the Schreibwerkstatt had advised participants with spontaneous decision-making problems to think up in advance the sentences they wanted to write during the experiment.)",Test procedure,,
10041,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 30/74
==========================================================================================
1. Test procedure -- 15/18
------------------------------------------------------------------------------------------
(Before the experiment, the leader of the Schreibwerkstatt had advised participants with spontaneous decision-making problems to think up in advance the sentences they wanted to write during the experiment.)",546329,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Afterwards, all participants were invited to explore the system freely.",Test procedure,"If needed, the participant received help with spelling or interacting with the computer either from the interviewer or the caretaker.",Test procedure,,
10042,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 31/74
==========================================================================================
1. Test procedure -- 16/18
------------------------------------------------------------------------------------------
If needed, the participant received help with spelling or interacting with the computer either from the interviewer or the caretaker.",546330,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"(Before the experiment, the leader of the Schreibwerkstatt had advised participants with spontaneous decision-making problems to think up in advance the sentences they wanted to write during the experiment.)",Test procedure,"At the end of the typing session, the interviewer exported the text from EasyTalk with or without symbols according to the participants preference to hand it to them as receipt for participating in the experiment.",Test procedure,,
10043,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 32/74
==========================================================================================
1. Test procedure -- 17/18
------------------------------------------------------------------------------------------
At the end of the typing session, the interviewer exported the text from EasyTalk with or without symbols according to the participants preference to hand it to them as receipt for participating in the experiment.",546331,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"If needed, the participant received help with spelling or interacting with the computer either from the interviewer or the caretaker.",Test procedure,One final yes/noquestion was asked to all participants: Would you like to use EasyTalk in the writing workshop in the future?,Test procedure,,
10044,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 33/74
==========================================================================================
1. Test procedure -- 18/18
------------------------------------------------------------------------------------------
One final yes/noquestion was asked to all participants: Would you like to use EasyTalk in the writing workshop in the future?",546332,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"At the end of the typing session, the interviewer exported the text from EasyTalk with or without symbols according to the participants preference to hand it to them as receipt for participating in the experiment.",Test procedure,"In general, the evaluation corroborates the easy and intuitive interface design of EasyTalk.",Results,,
10045,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 34/74
==========================================================================================
2. Results -- 1/41
------------------------------------------------------------------------------------------
In general, the evaluation corroborates the easy and intuitive interface design of EasyTalk.",546333,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,One final yes/noquestion was asked to all participants: Would you like to use EasyTalk in the writing workshop in the future?,Test procedure,"All participants successfully typed at least three sentences, with each sentence being an average of four words long with EasyTalk (see Figure 6 for the text typed in two sessions).",Results,result,
10046,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 35/74
==========================================================================================
2. Results -- 2/41
------------------------------------------------------------------------------------------
All participants successfully typed at least three sentences, with each sentence being an average of four words long with EasyTalk (see Figure 6 for the text typed in two sessions).",546334,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In general, the evaluation corroborates the easy and intuitive interface design of EasyTalk.",Results,Four participants spontaneously skipped the interactive example rehearsal and typed their own sentences without problems.,Results,result,
10047,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 36/74
==========================================================================================
2. Results -- 3/41
------------------------------------------------------------------------------------------
Four participants spontaneously skipped the interactive example rehearsal and typed their own sentences without problems.",546335,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"All participants successfully typed at least three sentences, with each sentence being an average of four words long with EasyTalk (see Figure 6 for the text typed in two sessions).",Results,"Participant P8, who can write texts beyond the scope of LS in MS Word, stated that EasyTalk did not benefit her and opted out of the test after writing a four-word sentence.",Results,result,
10048,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 37/74
==========================================================================================
2. Results -- 4/41
------------------------------------------------------------------------------------------
Participant P8, who can write texts beyond the scope of LS in MS Word, stated that EasyTalk did not benefit her and opted out of the test after writing a four-word sentence.",546336,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Four participants spontaneously skipped the interactive example rehearsal and typed their own sentences without problems.,Results,We exclude P8 in the following.,Results,result,
10049,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 38/74
==========================================================================================
2. Results -- 5/41
------------------------------------------------------------------------------------------
We exclude P8 in the following.",546337,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Participant P8, who can write texts beyond the scope of LS in MS Word, stated that EasyTalk did not benefit her and opted out of the test after writing a four-word sentence.",Results,"Spontaneously, P5 judged: ""The headers help with concentration"" and ""The connectors between sentences are important.",Results,,
10050,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 39/74
==========================================================================================
2. Results -- 6/41
------------------------------------------------------------------------------------------
Spontaneously, P5 judged: ""The headers help with concentration"" and ""The connectors between sentences are important.",546338,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,We exclude P8 in the following.,Results,Sometimes there are longer sentences. ,Results,result,
10051,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 40/74
==========================================================================================
2. Results -- 7/41
------------------------------------------------------------------------------------------
Sometimes there are longer sentences. ",546339,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Spontaneously, P5 judged: ""The headers help with concentration"" and ""The connectors between sentences are important.",Results,"16 www.tobiipro.com/product-listing/nano/ You can do them piece by piece in this manner."".",Results,result,
10052,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 41/74
==========================================================================================
2. Results -- 8/41
------------------------------------------------------------------------------------------
16 www.tobiipro.com/product-listing/nano/ You can do them piece by piece in this manner."".",546340,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Sometimes there are longer sentences. ,Results,"P2 stated: ""It works great",Results,error#result,
10053,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 42/74
==========================================================================================
2. Results -- 9/41
------------------------------------------------------------------------------------------
P2 stated: ""It works great",546341,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"16 www.tobiipro.com/product-listing/nano/ You can do them piece by piece in this manner."".",Results,but I have to concentrate a bit here.,Results,result,
10054,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 43/74
==========================================================================================
2. Results -- 10/41
------------------------------------------------------------------------------------------
but I have to concentrate a bit here.",546342,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"P2 stated: ""It works great",Results,We attribute the overall positive result to improvements of the overall interface that were based on several evaluation rounds with substitute users.,Results,result,
10055,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 44/74
==========================================================================================
2. Results -- 11/41
------------------------------------------------------------------------------------------
We attribute the overall positive result to improvements of the overall interface that were based on several evaluation rounds with substitute users.",546343,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,but I have to concentrate a bit here.,Results,The current test confirms that the communication with the system is easy to learn due to intuitive dialogues all over the system. ,Results,discussion,
10056,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 45/74
==========================================================================================
2. Results -- 12/41
------------------------------------------------------------------------------------------
The current test confirms that the communication with the system is easy to learn due to intuitive dialogues all over the system. ",546344,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,We attribute the overall positive result to improvements of the overall interface that were based on several evaluation rounds with substitute users.,Results,The eye-tracking data supports this claim.,Results,result,
10057,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 46/74
==========================================================================================
2. Results -- 13/41
------------------------------------------------------------------------------------------
The eye-tracking data supports this claim.",546345,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The current test confirms that the communication with the system is easy to learn due to intuitive dialogues all over the system. ,Results,We defined areas of interest (AOIs) in the interface to be able to track task-accomplishment paths.,Results,result,
10058,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 47/74
==========================================================================================
2. Results -- 14/41
------------------------------------------------------------------------------------------
We defined areas of interest (AOIs) in the interface to be able to track task-accomplishment paths.",546346,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The eye-tracking data supports this claim.,Results,All users focused on the dialogue elements in the intended manner.,Results,,
10059,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 48/74
==========================================================================================
2. Results -- 15/41
------------------------------------------------------------------------------------------
All users focused on the dialogue elements in the intended manner.",546347,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,We defined areas of interest (AOIs) in the interface to be able to track task-accomplishment paths.,Results,"With respect to effectiveness, we did not find traces of searching around for items.",Results,result,
10060,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 49/74
==========================================================================================
2. Results -- 16/41
------------------------------------------------------------------------------------------
With respect to effectiveness, we did not find traces of searching around for items.",546348,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,All users focused on the dialogue elements in the intended manner.,Results,The eye-tracking data documents the inspection of the Text Panel after a sentence was finished. ,Results,result,
10061,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 50/74
==========================================================================================
2. Results -- 17/41
------------------------------------------------------------------------------------------
The eye-tracking data documents the inspection of the Text Panel after a sentence was finished. ",546349,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"With respect to effectiveness, we did not find traces of searching around for items.",Results,One person spontaneously wrote a question.,Results,result,
10062,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 51/74
==========================================================================================
2. Results -- 18/41
------------------------------------------------------------------------------------------
One person spontaneously wrote a question.",546350,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The eye-tracking data documents the inspection of the Text Panel after a sentence was finished. ,Results,"Participants P1-P7 supplemented their sentences with modifiers (e.g., when? or how?",Results,result,
10063,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 52/74
==========================================================================================
2. Results -- 19/41
------------------------------------------------------------------------------------------
Participants P1-P7 supplemented their sentences with modifiers (e.g., when? or how?",546351,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,One person spontaneously wrote a question.,Results,cues were spontaneously selected in the Next-Word Panel).,Results,result,
10064,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 53/74
==========================================================================================
2. Results -- 20/41
------------------------------------------------------------------------------------------
cues were spontaneously selected in the Next-Word Panel).",546352,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Participants P1-P7 supplemented their sentences with modifiers (e.g., when? or how?",Results,"Six participants completed the decision dialogue for complex verb constructions (Steinmetz and Harbusch, 2020).",Results,result,
10065,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 54/74
==========================================================================================
2. Results -- 21/41
------------------------------------------------------------------------------------------
Six participants completed the decision dialogue for complex verb constructions (Steinmetz and Harbusch, 2020).",546353,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,cues were spontaneously selected in the Next-Word Panel).,Results,"Although we had not demonstrated this decision dialog in the introduction, four participants typed verbs in present perfect tense, and two users selected a modal as finite verb followed by an infinitive (cf.",Results,result,
10066,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 55/74
==========================================================================================
2. Results -- 22/41
------------------------------------------------------------------------------------------
Although we had not demonstrated this decision dialog in the introduction, four participants typed verbs in present perfect tense, and two users selected a modal as finite verb followed by an infinitive (cf.",546354,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Six participants completed the decision dialogue for complex verb constructions (Steinmetz and Harbusch, 2020).",Results,the example sentence in Figure 4).,Results,result,
10067,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 56/74
==========================================================================================
2. Results -- 23/41
------------------------------------------------------------------------------------------
the example sentence in Figure 4).",546355,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Although we had not demonstrated this decision dialog in the introduction, four participants typed verbs in present perfect tense, and two users selected a modal as finite verb followed by an infinitive (cf.",Results,Two participants spontaneously erased words in the Sentence Panel using the red X-button-also not shown in the introduction.,Results,error#result,
10068,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 57/74
==========================================================================================
2. Results -- 24/41
------------------------------------------------------------------------------------------
Two participants spontaneously erased words in the Sentence Panel using the red X-button-also not shown in the introduction.",546356,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,the example sentence in Figure 4).,Results,"Clicking the green ✓-button in the Sentence Panel was shown, and completing a sentence was successfully performed by all participants.",Results,result,
10069,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 58/74
==========================================================================================
2. Results -- 25/41
------------------------------------------------------------------------------------------
Clicking the green ✓-button in the Sentence Panel was shown, and completing a sentence was successfully performed by all participants.",546357,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,Two participants spontaneously erased words in the Sentence Panel using the red X-button-also not shown in the introduction.,Results,These observations also reflect that EasyTalk is easy and intuitive to use for the target group beyond explicitly demonstrated features. ,Results,result,
10070,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 59/74
==========================================================================================
2. Results -- 26/41
------------------------------------------------------------------------------------------
These observations also reflect that EasyTalk is easy and intuitive to use for the target group beyond explicitly demonstrated features. ",546358,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Clicking the green ✓-button in the Sentence Panel was shown, and completing a sentence was successfully performed by all participants.",Results,"With respect to efficiency, P4 systematically selected the words as soon as they appeared in the completion list in favor of writing the words to the end.",Results,result,
10071,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 60/74
==========================================================================================
2. Results -- 27/41
------------------------------------------------------------------------------------------
With respect to efficiency, P4 systematically selected the words as soon as they appeared in the completion list in favor of writing the words to the end.",546359,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,These observations also reflect that EasyTalk is easy and intuitive to use for the target group beyond explicitly demonstrated features. ,Results,"In contrast, P6 initially typed every word from start to finish.",Results,result,
10072,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 61/74
==========================================================================================
2. Results -- 28/41
------------------------------------------------------------------------------------------
In contrast, P6 initially typed every word from start to finish.",546360,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"With respect to efficiency, P4 systematically selected the words as soon as they appeared in the completion list in favor of writing the words to the end.",Results,"Later on, P6 selected the words from the completion list as soon as possible.",Results,result,
10073,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 62/74
==========================================================================================
2. Results -- 29/41
------------------------------------------------------------------------------------------
Later on, P6 selected the words from the completion list as soon as possible.",546361,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In contrast, P6 initially typed every word from start to finish.",Results,"P2 commented: ""Writing to the end is better.""",Results,result,
10074,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 63/74
==========================================================================================
2. Results -- 30/41
------------------------------------------------------------------------------------------
P2 commented: ""Writing to the end is better.""",546362,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Later on, P6 selected the words from the completion list as soon as possible.",Results,and judged the completion list as helpful to prevent spelling mistakes. ,Results,error#result,
10075,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 64/74
==========================================================================================
2. Results -- 31/41
------------------------------------------------------------------------------------------
and judged the completion list as helpful to prevent spelling mistakes. ",546363,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"P2 commented: ""Writing to the end is better.""",Results,"According to the eye-tracking data, the participants' focus while writing the current sentence was mainly on the Next-Word Panel.",Results,error#result,
10076,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 65/74
==========================================================================================
2. Results -- 32/41
------------------------------------------------------------------------------------------
According to the eye-tracking data, the participants' focus while writing the current sentence was mainly on the Next-Word Panel.",546364,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,and judged the completion list as helpful to prevent spelling mistakes. ,Results,The Text Panel and the Sentence Panel were used to back up the flow of thoughts.,Results,result,
10077,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 66/74
==========================================================================================
2. Results -- 33/41
------------------------------------------------------------------------------------------
The Text Panel and the Sentence Panel were used to back up the flow of thoughts.",546365,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"According to the eye-tracking data, the participants' focus while writing the current sentence was mainly on the Next-Word Panel.",Results,"In detail, the participants exhibited different interaction strategies (Figure 8, e.g., illustrates P1's word selection strategy of focusing the wh-cues).",Results,result,
10078,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 67/74
==========================================================================================
2. Results -- 34/41
------------------------------------------------------------------------------------------
In detail, the participants exhibited different interaction strategies (Figure 8, e.g., illustrates P1's word selection strategy of focusing the wh-cues).",546366,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The Text Panel and the Sentence Panel were used to back up the flow of thoughts.,Results,"To connect a sentence, all participants looked at the previous text in the Text Panel and read through the Connector Panel (see Figure 7 for an example gaze plot).",Results,result,
10079,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 68/74
==========================================================================================
2. Results -- 35/41
------------------------------------------------------------------------------------------
To connect a sentence, all participants looked at the previous text in the Text Panel and read through the Connector Panel (see Figure 7 for an example gaze plot).",546367,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"In detail, the participants exhibited different interaction strategies (Figure 8, e.g., illustrates P1's word selection strategy of focusing the wh-cues).",Results,"However, the eye-tracking data unveiled shortcomings of the Connector Panel's layout.",Results,result,
10080,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 69/74
==========================================================================================
2. Results -- 36/41
------------------------------------------------------------------------------------------
However, the eye-tracking data unveiled shortcomings of the Connector Panel's layout.",546368,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"To connect a sentence, all participants looked at the previous text in the Text Panel and read through the Connector Panel (see Figure 7 for an example gaze plot).",Results,"Often, the second row of connector options was considerably less likely inspected.",Results,limitation,
10081,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 70/74
==========================================================================================
2. Results -- 37/41
------------------------------------------------------------------------------------------
Often, the second row of connector options was considerably less likely inspected.",546369,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"However, the eye-tracking data unveiled shortcomings of the Connector Panel's layout.",Results,"Unfortunately, nobody felt inclined to add a connector systematically after reading through all/some options.",Results,limitation,
10082,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 71/74
==========================================================================================
2. Results -- 38/41
------------------------------------------------------------------------------------------
Unfortunately, nobody felt inclined to add a connector systematically after reading through all/some options.",546370,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Often, the second row of connector options was considerably less likely inspected.",Results,"Accordingly, we plan to shorten the list of mentioned options.",Results,limitation,
10083,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 72/74
==========================================================================================
2. Results -- 39/41
------------------------------------------------------------------------------------------
Accordingly, we plan to shorten the list of mentioned options.",546371,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Unfortunately, nobody felt inclined to add a connector systematically after reading through all/some options.",Results,"Moreover, we intend to set up an active training mode in EasyTalk that teaches when and how to use text connectors (Reid et al., 2013). ",Results,directions,
10084,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 73/74
==========================================================================================
2. Results -- 40/41
------------------------------------------------------------------------------------------
Moreover, we intend to set up an active training mode in EasyTalk that teaches when and how to use text connectors (Reid et al., 2013). ",546372,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Accordingly, we plan to shorten the list of mentioned options.",Results,"Because of the participants' overall positive response to the question of whether they wanted to use the system, the leader of the writing workshop asked for a copy of EasyTalk for using it in future.",Results,directions#rw,
10085,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
4. Evaluation -- 74/74
==========================================================================================
2. Results -- 41/41
------------------------------------------------------------------------------------------
Because of the participants' overall positive response to the question of whether they wanted to use the system, the leader of the writing workshop asked for a copy of EasyTalk for using it in future.",546373,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Moreover, we intend to set up an active training mode in EasyTalk that teaches when and how to use text connectors (Reid et al., 2013). ",Results,"We presented EasyTalk, an intuitive-to-use writing assistant for fast and correct text writing in ELS for low-literate users with IDD and/or CCN.",Conclusions,directions,
10086,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 1/9
==========================================================================================
We presented EasyTalk, an intuitive-to-use writing assistant for fast and correct text writing in ELS for low-literate users with IDD and/or CCN.",546374,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Because of the participants' overall positive response to the question of whether they wanted to use the system, the leader of the writing workshop asked for a copy of EasyTalk for using it in future.",Results,The evaluation verified the claim that users can instantaneously type complete and correct sentences with EasyTalk.,Conclusions,contribution,
10087,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 2/9
==========================================================================================
The evaluation verified the claim that users can instantaneously type complete and correct sentences with EasyTalk.",546375,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"We presented EasyTalk, an intuitive-to-use writing assistant for fast and correct text writing in ELS for low-literate users with IDD and/or CCN.",Conclusions,"However, the offer of connectors should be improved.",Conclusions,result,
10088,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 3/9
==========================================================================================
However, the offer of connectors should be improved.",546376,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,The evaluation verified the claim that users can instantaneously type complete and correct sentences with EasyTalk.,Conclusions,"As mentioned above, we plan a make-over of the Connector Panel combined with an active teaching unit.",Conclusions,directions,
10089,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 4/9
==========================================================================================
As mentioned above, we plan a make-over of the Connector Panel combined with an active teaching unit.",546377,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"However, the offer of connectors should be improved.",Conclusions,It is an open question to which extent automatic storytelling concepts (cf. Section 2.3) can be incorporated into the active training mode of our system (cf.,Conclusions,directions,
10090,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 5/9
==========================================================================================
It is an open question to which extent automatic storytelling concepts (cf. Section 2.3) can be incorporated into the active training mode of our system (cf.",546378,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"As mentioned above, we plan a make-over of the Connector Panel combined with an active teaching unit.",Conclusions,"Steinmetz and Harbusch, 2021a).",Conclusions,discussion#error#rw,ou impact ?
10091,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 6/9
==========================================================================================
Steinmetz and Harbusch, 2021a).",546379,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,It is an open question to which extent automatic storytelling concepts (cf. Section 2.3) can be incorporated into the active training mode of our system (cf.,Conclusions,We intend to evaluate this new feature in longitudinal studies with the target user group. ,Conclusions,discussion#error#rw,
10092,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 7/9
==========================================================================================
We intend to evaluate this new feature in longitudinal studies with the target user group. ",546380,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Steinmetz and Harbusch, 2021a).",Conclusions,"Besides further above-mentioned future work, personalized features for specific user groups will be realized.",Conclusions,directions,
10093,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 8/9
==========================================================================================
Besides further above-mentioned future work, personalized features for specific user groups will be realized.",546381,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,We intend to evaluate this new feature in longitudinal studies with the target user group. ,Conclusions,"Moreover, a native smartphone version is under development.  ",Conclusions,directions,
10094,"A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment
==========================================================================================
5. Conclusions -- 9/9
==========================================================================================
Moreover, a native smartphone version is under development.  ",546382,A text-writing system for Easy-to-Read {G}erman evaluated with low-literate users with cognitive impairment,"0. abstract
1. Introduction
2. Writing support systems for users with IDD and/or CCN
3. Text-writing assistance by EasyTalk
4. Evaluation
5. Conclusions
",2022,"Besides further above-mentioned future work, personalized features for specific user groups will be realized.",Conclusions,,,directions,
