text,doc_id,paper_title,paper_structure,year,prev_text,prev_section,next_text,next_section,label
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 1/11
==========================================================================================
  Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning?",5706655,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,,,"Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 2/11
==========================================================================================
Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets.",5706656,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"  Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning?",abstract,"To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 3/11
==========================================================================================
To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.",5706657,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets.",abstract,"Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 4/11
==========================================================================================
Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer.",5706658,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.",abstract,"To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 5/11
==========================================================================================
To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates.",5706659,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer.",abstract,"We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 6/11
==========================================================================================
We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis.",5706660,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates.",abstract,It is also substantially faster than existing top-performing diffusion models.,abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 7/11
==========================================================================================
It is also substantially faster than existing top-performing diffusion models.",5706661,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis.",abstract,We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training.,abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 8/11
==========================================================================================
We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training.",5706662,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,It is also substantially faster than existing top-performing diffusion models.,abstract,"Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 9/11
==========================================================================================
Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification.",5706663,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training.,abstract,"Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 10/11
==========================================================================================
Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.",5706664,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification.",abstract,"Code, models, samples: https://github.com/RF5/simple-asgan/ ",abstract,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
0. abstract -- 11/11
==========================================================================================
Code, models, samples: https://github.com/RF5/simple-asgan/ ",5706665,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.",abstract,U NCONDITIONAL speech synthesis systems aim to produce coherent speech without conditioning inputs such as text or speaker labels [1].,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 1/37
==========================================================================================
U NCONDITIONAL speech synthesis systems aim to produce coherent speech without conditioning inputs such as text or speaker labels [1].",5706666,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Code, models, samples: https://github.com/RF5/simple-asgan/ ",abstract,In this work we are specifically interested in learning to map noise from a known continuous distribution into spoken utterances [2].,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 2/37
==========================================================================================
In this work we are specifically interested in learning to map noise from a known continuous distribution into spoken utterances [2].",5706667,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,U NCONDITIONAL speech synthesis systems aim to produce coherent speech without conditioning inputs such as text or speaker labels [1].,I. INTRODUCTION,"A model that could do this would have several useful downstream applications: from latent interpolations between utterances and fine-grained tuning of properties of the generated speech, to audio compression and better probability density estimation of speech.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 3/37
==========================================================================================
A model that could do this would have several useful downstream applications: from latent interpolations between utterances and fine-grained tuning of properties of the generated speech, to audio compression and better probability density estimation of speech.",5706668,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,In this work we are specifically interested in learning to map noise from a known continuous distribution into spoken utterances [2].,I. INTRODUCTION,"Some of these advances from latent generative modelling have already been realized in the image modality [3], [4]; our goal is to bring these developments to the speech domain. ",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 4/37
==========================================================================================
Some of these advances from latent generative modelling have already been realized in the image modality [3], [4]; our goal is to bring these developments to the speech domain. ",5706669,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"A model that could do this would have several useful downstream applications: from latent interpolations between utterances and fine-grained tuning of properties of the generated speech, to audio compression and better probability density estimation of speech.",I. INTRODUCTION,"Direct speech synthesis from a latent space is a very challenging problem, with prior efforts only able to model Matthew Baas and Herman Kamper are with the Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa (email: 20786379@sun.ac.za; kamperh@sun.ac.za). ",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 5/37
==========================================================================================
Direct speech synthesis from a latent space is a very challenging problem, with prior efforts only able to model Matthew Baas and Herman Kamper are with the Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa (email: 20786379@sun.ac.za; kamperh@sun.ac.za). ",5706670,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Some of these advances from latent generative modelling have already been realized in the image modality [3], [4]; our goal is to bring these developments to the speech domain. ",I. INTRODUCTION,All experiments were performed on Stellenbosch University's High Performance Computing (HPC) GPU cluster. ,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 6/37
==========================================================================================
All experiments were performed on Stellenbosch University's High Performance Computing (HPC) GPU cluster. ",5706671,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Direct speech synthesis from a latent space is a very challenging problem, with prior efforts only able to model Matthew Baas and Herman Kamper are with the Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa (email: 20786379@sun.ac.za; kamperh@sun.ac.za). ",I. INTRODUCTION,"Manuscript accepted to IEEE TASLP, 2024. speech in a restricted setting [1], [5], [6].",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 7/37
==========================================================================================
Manuscript accepted to IEEE TASLP, 2024. speech in a restricted setting [1], [5], [6].",5706672,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,All experiments were performed on Stellenbosch University's High Performance Computing (HPC) GPU cluster. ,I. INTRODUCTION,We therefore also restrict ourselves to the small-vocabulary case here. ,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 8/37
==========================================================================================
We therefore also restrict ourselves to the small-vocabulary case here. ",5706673,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Manuscript accepted to IEEE TASLP, 2024. speech in a restricted setting [1], [5], [6].",I. INTRODUCTION,"In this problem setting, recent studies on diffusion models [7] for images [8]- [10] has led to major improvements in unconditional speech synthesis.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 9/37
==========================================================================================
In this problem setting, recent studies on diffusion models [7] for images [8]- [10] has led to major improvements in unconditional speech synthesis.",5706674,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We therefore also restrict ourselves to the small-vocabulary case here. ,I. INTRODUCTION,"The current best-performing approaches are all based on diffusion modelling [5], [6], which iteratively de-noise a sampled signal into a waveform through a Markov chain [7].",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 10/37
==========================================================================================
The current best-performing approaches are all based on diffusion modelling [5], [6], which iteratively de-noise a sampled signal into a waveform through a Markov chain [7].",5706675,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"In this problem setting, recent studies on diffusion models [7] for images [8]- [10] has led to major improvements in unconditional speech synthesis.",I. INTRODUCTION,"Before this, many studies used generative adversarial networks (GANs) [11] that map a latent vector to a sequence of speech features with a single forward pass through the model.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 11/37
==========================================================================================
Before this, many studies used generative adversarial networks (GANs) [11] that map a latent vector to a sequence of speech features with a single forward pass through the model.",5706676,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"The current best-performing approaches are all based on diffusion modelling [5], [6], which iteratively de-noise a sampled signal into a waveform through a Markov chain [7].",I. INTRODUCTION,"However, performance was limited [1], [2], leading to GANs falling out of favour for this task. ",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 12/37
==========================================================================================
However, performance was limited [1], [2], leading to GANs falling out of favour for this task. ",5706677,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Before this, many studies used generative adversarial networks (GANs) [11] that map a latent vector to a sequence of speech features with a single forward pass through the model.",I. INTRODUCTION,"Motivated by the recent developments in the StyleGAN literature [3], [12], [13] for image synthesis, we aim to reinvigorate GANs for unconditional speech synthesis, where we are particularly interested in their ability for learning continuous, disentangled latent spaces [13].",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 13/37
==========================================================================================
Motivated by the recent developments in the StyleGAN literature [3], [12], [13] for image synthesis, we aim to reinvigorate GANs for unconditional speech synthesis, where we are particularly interested in their ability for learning continuous, disentangled latent spaces [13].",5706678,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"However, performance was limited [1], [2], leading to GANs falling out of favour for this task. ",I. INTRODUCTION,"To this end, we propose AudioStyleGAN",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 14/37
==========================================================================================
To this end, we propose AudioStyleGAN",5706679,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Motivated by the recent developments in the StyleGAN literature [3], [12], [13] for image synthesis, we aim to reinvigorate GANs for unconditional speech synthesis, where we are particularly interested in their ability for learning continuous, disentangled latent spaces [13].",I. INTRODUCTION,"(ASGAN): a convolutional GAN which maps a single latent vector to a sequence of audio features, and is designed to have a disentangled latent space.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 15/37
==========================================================================================
(ASGAN): a convolutional GAN which maps a single latent vector to a sequence of audio features, and is designed to have a disentangled latent space.",5706680,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"To this end, we propose AudioStyleGAN",I. INTRODUCTION,"The model is based in large part on StyleGAN3 [3], which we adapt for audio synthesis.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 16/37
==========================================================================================
The model is based in large part on StyleGAN3 [3], which we adapt for audio synthesis.",5706681,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"(ASGAN): a convolutional GAN which maps a single latent vector to a sequence of audio features, and is designed to have a disentangled latent space.",I. INTRODUCTION,"Concretely, we adapt the style layers to remove signal aliasing caused by the non-linearities in the network.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 17/37
==========================================================================================
Concretely, we adapt the style layers to remove signal aliasing caused by the non-linearities in the network.",5706682,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"The model is based in large part on StyleGAN3 [3], which we adapt for audio synthesis.",I. INTRODUCTION,This is accomplished with anti-aliasing filters to ensure that the Nyquist-Shannon sampling limits are met in each layer.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 18/37
==========================================================================================
This is accomplished with anti-aliasing filters to ensure that the Nyquist-Shannon sampling limits are met in each layer.",5706683,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Concretely, we adapt the style layers to remove signal aliasing caused by the non-linearities in the network.",I. INTRODUCTION,We also propose a modification to adaptive discriminator augmentation [14] to stabilize training by randomly dropping discriminator updates based on a guiding signal. ,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 19/37
==========================================================================================
We also propose a modification to adaptive discriminator augmentation [14] to stabilize training by randomly dropping discriminator updates based on a guiding signal. ",5706684,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,This is accomplished with anti-aliasing filters to ensure that the Nyquist-Shannon sampling limits are met in each layer.,I. INTRODUCTION,"In unconditional speech synthesis experiments, we measure the quality and diversity of the generated samples using objective metrics.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 20/37
==========================================================================================
In unconditional speech synthesis experiments, we measure the quality and diversity of the generated samples using objective metrics.",5706685,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We also propose a modification to adaptive discriminator augmentation [14] to stabilize training by randomly dropping discriminator updates based on a guiding signal. ,I. INTRODUCTION,We show that ASGAN sets a new stateof-the-art in unconditional speech synthesis on the Google Speech Commands digits dataset [15].,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 21/37
==========================================================================================
We show that ASGAN sets a new stateof-the-art in unconditional speech synthesis on the Google Speech Commands digits dataset [15].",5706686,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"In unconditional speech synthesis experiments, we measure the quality and diversity of the generated samples using objective metrics.",I. INTRODUCTION,Objective metrics that measure latent space disentanglement indicate that ASGAN has a more disentangled latent representation compared to existing diffusion models.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 22/37
==========================================================================================
Objective metrics that measure latent space disentanglement indicate that ASGAN has a more disentangled latent representation compared to existing diffusion models.",5706687,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We show that ASGAN sets a new stateof-the-art in unconditional speech synthesis on the Google Speech Commands digits dataset [15].,I. INTRODUCTION,It not only outperforms the best existing models but is also faster to train and faster in inference.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 23/37
==========================================================================================
It not only outperforms the best existing models but is also faster to train and faster in inference.",5706688,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,Objective metrics that measure latent space disentanglement indicate that ASGAN has a more disentangled latent representation compared to existing diffusion models.,I. INTRODUCTION,"Subjective mean opinion scores (MOS) indicate that ASGAN's generated utterances sound more natural (MOS: 3.68) than the existing best model (SaShiMi [6], MOS: 3.33).",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 24/37
==========================================================================================
Subjective mean opinion scores (MOS) indicate that ASGAN's generated utterances sound more natural (MOS: 3.68) than the existing best model (SaShiMi [6], MOS: 3.33).",5706689,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,It not only outperforms the best existing models but is also faster to train and faster in inference.,I. INTRODUCTION,We also perform ablation experiments showing intrinsically that our proposed anti-aliasing and adaptive discriminator augmentation techniques are necessary for high-quality and diverse synthesis. ,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 25/37
==========================================================================================
We also perform ablation experiments showing intrinsically that our proposed anti-aliasing and adaptive discriminator augmentation techniques are necessary for high-quality and diverse synthesis. ",5706690,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Subjective mean opinion scores (MOS) indicate that ASGAN's generated utterances sound more natural (MOS: 3.68) than the existing best model (SaShiMi [6], MOS: 3.33).",I. INTRODUCTION,"This work is an extension of the conference paper [16], where (apart from the ablation experiments)",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 26/37
==========================================================================================
This work is an extension of the conference paper [16], where (apart from the ablation experiments)",5706691,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We also perform ablation experiments showing intrinsically that our proposed anti-aliasing and adaptive discriminator augmentation techniques are necessary for high-quality and diverse synthesis. ,I. INTRODUCTION,many of the above intrinsic evaluations were already presented.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 27/37
==========================================================================================
many of the above intrinsic evaluations were already presented.",5706692,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"This work is an extension of the conference paper [16], where (apart from the ablation experiments)",I. INTRODUCTION,"Here, for the first time, we profile the generalization benefits of a disentangled latent space.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 28/37
==========================================================================================
Here, for the first time, we profile the generalization benefits of a disentangled latent space.",5706693,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,many of the above intrinsic evaluations were already presented.,I. INTRODUCTION,Through an evaluation of ASGAN's abilities we show that its disentangled latent space allows us to perform several tasks unseen during training through simple linear operations in its latent space.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 29/37
==========================================================================================
Through an evaluation of ASGAN's abilities we show that its disentangled latent space allows us to perform several tasks unseen during training through simple linear operations in its latent space.",5706694,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Here, for the first time, we profile the generalization benefits of a disentangled latent space.",I. INTRODUCTION,"Concretely, we demonstrate compelling zero-shot performance on voice conversion, speech enhancement, speaker verification and keyword classification on the Google Speech Commands digits dataset.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 30/37
==========================================================================================
Concretely, we demonstrate compelling zero-shot performance on voice conversion, speech enhancement, speaker verification and keyword classification on the Google Speech Commands digits dataset.",5706695,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,Through an evaluation of ASGAN's abilities we show that its disentangled latent space allows us to perform several tasks unseen during training through simple linear operations in its latent space.,I. INTRODUCTION,"While not matching the performance of state-of-the-art task-specific systems on all these tasks, our experiments show that a single model designed for disentanglement can achieve reasonable performance across a range of tasks that it hasn't seen in training.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 31/37
==========================================================================================
While not matching the performance of state-of-the-art task-specific systems on all these tasks, our experiments show that a single model designed for disentanglement can achieve reasonable performance across a range of tasks that it hasn't seen in training.",5706696,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Concretely, we demonstrate compelling zero-shot performance on voice conversion, speech enhancement, speaker verification and keyword classification on the Google Speech Commands digits dataset.",I. INTRODUCTION,"Our work shows the continued competitive nature of GANs compared to diffusion models, and the generalization benefits of designing for disentanglement. ",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 32/37
==========================================================================================
Our work shows the continued competitive nature of GANs compared to diffusion models, and the generalization benefits of designing for disentanglement. ",5706697,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"While not matching the performance of state-of-the-art task-specific systems on all these tasks, our experiments show that a single model designed for disentanglement can achieve reasonable performance across a range of tasks that it hasn't seen in training.",I. INTRODUCTION,The paper is organized as follows.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 33/37
==========================================================================================
The paper is organized as follows.",5706698,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Our work shows the continued competitive nature of GANs compared to diffusion models, and the generalization benefits of designing for disentanglement. ",I. INTRODUCTION,We discuss related work in Sec.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 34/37
==========================================================================================
We discuss related work in Sec.",5706699,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,The paper is organized as follows.,I. INTRODUCTION,II and then go on to propose ASGAN in Sec.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 35/37
==========================================================================================
II and then go on to propose ASGAN in Sec.",5706700,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We discuss related work in Sec.,I. INTRODUCTION,The main unconditional speech synthesis experiments and their results are given in Sec.,I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 36/37
==========================================================================================
The main unconditional speech synthesis experiments and their results are given in Sec.",5706701,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,II and then go on to propose ASGAN in Sec.,I. INTRODUCTION,"This is followed by the experiments on the unseen tasks that ASGAN can be used for in Sec. VI, VII, and VIII.",I. INTRODUCTION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
1. I. INTRODUCTION -- 37/37
==========================================================================================
This is followed by the experiments on the unseen tasks that ASGAN can be used for in Sec. VI, VII, and VIII.",5706702,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,The main unconditional speech synthesis experiments and their results are given in Sec.,I. INTRODUCTION,"Since we focus on the proposed generalization abilities provided by a continuous latent space, we first distinguish what we call unconditional speech synthesis to the related but different task of generative spoken language modelling (GSLM)",II. RELATED WORK,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 1/26
==========================================================================================
We train and validate our models on the official training/validation/test split from SC09.",5706703,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"All utterances are roughly a second long and are sampled at 16 kHz, where utterances less than a second are padded to a full second.",A. Data,We then evaluate unconditional speech synthesis quality by seeing how well newly generated utterances match the distribution of the SC09 test split.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 2/26
==========================================================================================
We then evaluate unconditional speech synthesis quality by seeing how well newly generated utterances match the distribution of the SC09 test split.",5706704,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We train and validate our models on the official training/validation/test split from SC09.,B. Evaluation metrics,"We use metrics similar to those for image synthesis; they try to measure either the quality of generated utterances (realism compared to actual audio in the test set), or the diversity of generated utterances (how varied the utterances are relative to the test set), or a combination of both. ",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 3/26
==========================================================================================
We use metrics similar to those for image synthesis; they try to measure either the quality of generated utterances (realism compared to actual audio in the test set), or the diversity of generated utterances (how varied the utterances are relative to the test set), or a combination of both. ",5706705,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We then evaluate unconditional speech synthesis quality by seeing how well newly generated utterances match the distribution of the SC09 test split.,B. Evaluation metrics,These metrics require extracting features or predictions from a supervised speech classifier network trained to classify the utterances from SC09 by what digit is spoken.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 4/26
==========================================================================================
These metrics require extracting features or predictions from a supervised speech classifier network trained to classify the utterances from SC09 by what digit is spoken.",5706706,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We use metrics similar to those for image synthesis; they try to measure either the quality of generated utterances (realism compared to actual audio in the test set), or the diversity of generated utterances (how varied the utterances are relative to the test set), or a combination of both. ",B. Evaluation metrics,"While there is no consistent pretrained classifier used for this purpose, we opt to use a ResNeXT architecture [39], similar to previous studies [5], [6].",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 5/26
==========================================================================================
While there is no consistent pretrained classifier used for this purpose, we opt to use a ResNeXT architecture [39], similar to previous studies [5], [6].",5706707,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,These metrics require extracting features or predictions from a supervised speech classifier network trained to classify the utterances from SC09 by what digit is spoken.,B. Evaluation metrics,"The trained model has a 98.1% word classification accuracy on the SC09 test set, and we make the model code and checkpoints available for future comparisons.",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 6/26
==========================================================================================
The trained model has a 98.1% word classification accuracy on the SC09 test set, and we make the model code and checkpoints available for future comparisons.",5706708,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"While there is no consistent pretrained classifier used for this purpose, we opt to use a ResNeXT architecture [39], similar to previous studies [5], [6].",B. Evaluation metrics,"1 Using either the classification output or 1024-dimensional features extracted from the penultimate layer in the classifier, we consider the following metrics. ",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 7/26
==========================================================================================
1 Using either the classification output or 1024-dimensional features extracted from the penultimate layer in the classifier, we consider the following metrics. ",5706709,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"The trained model has a 98.1% word classification accuracy on the SC09 test set, and we make the model code and checkpoints available for future comparisons.",B. Evaluation metrics,• Inception score (IS) measures the diversity and quality of generated samples by evaluating the Kullback-Leibler (KL) divergence between the label distribution from the classifier output and the mean label distribution over a set of generated utterances [40]. •,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 8/26
==========================================================================================
• Inception score (IS) measures the diversity and quality of generated samples by evaluating the Kullback-Leibler (KL) divergence between the label distribution from the classifier output and the mean label distribution over a set of generated utterances [40]. •",5706710,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"1 Using either the classification output or 1024-dimensional features extracted from the penultimate layer in the classifier, we consider the following metrics. ",B. Evaluation metrics,Modified Inception score (mIS) extends the diversity measurement aspect of IS by incorporating a measure 1 https://github.com/RF5/simple-speech-commands of intra-class diversity (in our case over the ten digits) to reward models with higher intra-class entropy [41]. •,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 9/26
==========================================================================================
Modified Inception score (mIS) extends the diversity measurement aspect of IS by incorporating a measure 1 https://github.com/RF5/simple-speech-commands of intra-class diversity (in our case over the ten digits) to reward models with higher intra-class entropy [41]. •",5706711,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,• Inception score (IS) measures the diversity and quality of generated samples by evaluating the Kullback-Leibler (KL) divergence between the label distribution from the classifier output and the mean label distribution over a set of generated utterances [40]. •,B. Evaluation metrics,Fréchet Inception distance (FID) computes a measure of how well the distribution of generated utterances matches the test-set utterances by comparing the classifier features of generated and real data [42].,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 10/26
==========================================================================================
Fréchet Inception distance (FID) computes a measure of how well the distribution of generated utterances matches the test-set utterances by comparing the classifier features of generated and real data [42].",5706712,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,Modified Inception score (mIS) extends the diversity measurement aspect of IS by incorporating a measure 1 https://github.com/RF5/simple-speech-commands of intra-class diversity (in our case over the ten digits) to reward models with higher intra-class entropy [41]. •,B. Evaluation metrics,"• Activation maximization (AM) measures generator quality by comparing the KL divergence between the classifier class probabilities from real and generated data, while penalizing high classifier entropy samples produced by the generator [43].",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 11/26
==========================================================================================
• Activation maximization (AM) measures generator quality by comparing the KL divergence between the classifier class probabilities from real and generated data, while penalizing high classifier entropy samples produced by the generator [43].",5706713,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,Fréchet Inception distance (FID) computes a measure of how well the distribution of generated utterances matches the test-set utterances by comparing the classifier features of generated and real data [42].,B. Evaluation metrics,"Intuitively, this attempts to account for possible class imbalances in the training set and intra-class diversity by incorporating a term for the entropy of the classifier outputs for generated samples.",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 12/26
==========================================================================================
Intuitively, this attempts to account for possible class imbalances in the training set and intra-class diversity by incorporating a term for the entropy of the classifier outputs for generated samples.",5706714,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"• Activation maximization (AM) measures generator quality by comparing the KL divergence between the classifier class probabilities from real and generated data, while penalizing high classifier entropy samples produced by the generator [43].",B. Evaluation metrics,A major motivation for ASGAN's design is latent-space disentanglement.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 13/26
==========================================================================================
A major motivation for ASGAN's design is latent-space disentanglement.",5706715,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Intuitively, this attempts to account for possible class imbalances in the training set and intra-class diversity by incorporating a term for the entropy of the classifier outputs for generated samples.",B. Evaluation metrics,"In the experiments proceeding from Sec. VIII and onwards, we show this property allows the model to be applied to extrinsic tasks that it is not explicitly trained for.",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 14/26
==========================================================================================
In the experiments proceeding from Sec. VIII and onwards, we show this property allows the model to be applied to extrinsic tasks that it is not explicitly trained for.",5706716,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,A major motivation for ASGAN's design is latent-space disentanglement.,B. Evaluation metrics,"But before this (Sec. V), we intrinsically evaluate disentanglement using two metrics on the Z and W latent spaces. ",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 15/26
==========================================================================================
But before this (Sec. V), we intrinsically evaluate disentanglement using two metrics on the Z and W latent spaces. ",5706717,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"In the experiments proceeding from Sec. VIII and onwards, we show this property allows the model to be applied to extrinsic tasks that it is not explicitly trained for.",B. Evaluation metrics,"• Path length measures the mean L 2 distance moved by the classifier features when the latent point (z or w) is randomly perturbed slightly, averaged over many perturbations [12].",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 16/26
==========================================================================================
• Path length measures the mean L 2 distance moved by the classifier features when the latent point (z or w) is randomly perturbed slightly, averaged over many perturbations [12].",5706718,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"But before this (Sec. V), we intrinsically evaluate disentanglement using two metrics on the Z and W latent spaces. ",B. Evaluation metrics,A lower value indicates a smoother latent space. ,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 17/26
==========================================================================================
A lower value indicates a smoother latent space. ",5706719,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"• Path length measures the mean L 2 distance moved by the classifier features when the latent point (z or w) is randomly perturbed slightly, averaged over many perturbations [12].",B. Evaluation metrics,• Linear separability utilizes a linear support vector machine (SVM) to classify the digit of a latent point.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 18/26
==========================================================================================
• Linear separability utilizes a linear support vector machine (SVM) to classify the digit of a latent point.",5706720,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,A lower value indicates a smoother latent space. ,B. Evaluation metrics,The metric is computed as the additional information (in terms of mean entropy) necessary to correctly classify an utterance given the class prediction from the linear SVM [12].,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 19/26
==========================================================================================
The metric is computed as the additional information (in terms of mean entropy) necessary to correctly classify an utterance given the class prediction from the linear SVM [12].",5706721,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,• Linear separability utilizes a linear support vector machine (SVM) to classify the digit of a latent point.,B. Evaluation metrics,A lower value indicates a more linearly disentangled latent space.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 20/26
==========================================================================================
A lower value indicates a more linearly disentangled latent space.",5706722,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,The metric is computed as the additional information (in terms of mean entropy) necessary to correctly classify an utterance given the class prediction from the linear SVM [12].,B. Evaluation metrics,These metrics are averaged over 5000 generated utterances for each model.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 21/26
==========================================================================================
These metrics are averaged over 5000 generated utterances for each model.",5706723,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,A lower value indicates a more linearly disentangled latent space.,B. Evaluation metrics,"As in [12], for linear separability we exclude half the generated utterances for which the ResNeXT classifier is least confident in its prediction. ",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 22/26
==========================================================================================
As in [12], for linear separability we exclude half the generated utterances for which the ResNeXT classifier is least confident in its prediction. ",5706724,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,These metrics are averaged over 5000 generated utterances for each model.,B. Evaluation metrics,"To give an indication of naturalness, we compute an estimated mean opinion score (eMOS) using a pretrained Wav2Vec2 small baseline from the VoiceMOS challenge [44].",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 23/26
==========================================================================================
To give an indication of naturalness, we compute an estimated mean opinion score (eMOS) using a pretrained Wav2Vec2 small baseline from the VoiceMOS challenge [44].",5706725,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"As in [12], for linear separability we exclude half the generated utterances for which the ResNeXT classifier is least confident in its prediction. ",B. Evaluation metrics,This model is trained to predict the naturalness score that a human would assign to an utterance from 1 (least natural) to 5 (most natural).,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 24/26
==========================================================================================
This model is trained to predict the naturalness score that a human would assign to an utterance from 1 (least natural) to 5 (most natural).",5706726,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"To give an indication of naturalness, we compute an estimated mean opinion score (eMOS) using a pretrained Wav2Vec2 small baseline from the VoiceMOS challenge [44].",B. Evaluation metrics,We also perform an actual subjective MOS evaluation using Amazon Mechanical Turk to obtain 240 opinion scores for each model with 12 speakers listening to each utterance.,B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 25/26
==========================================================================================
We also perform an actual subjective MOS evaluation using Amazon Mechanical Turk to obtain 240 opinion scores for each model with 12 speakers listening to each utterance.",5706727,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,This model is trained to predict the naturalness score that a human would assign to an utterance from 1 (least natural) to 5 (most natural).,B. Evaluation metrics,"Finally, the speed of each model is also evaluated to highlight the benefit that GANs can produce utterances in a single inference call, as opposed to the many inference calls necessary with autoregressive or diffusion models.",B. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
9. B. Evaluation metrics -- 26/26
==========================================================================================
Finally, the speed of each model is also evaluated to highlight the benefit that GANs can produce utterances in a single inference call, as opposed to the many inference calls necessary with autoregressive or diffusion models.",5706728,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We also perform an actual subjective MOS evaluation using Amazon Mechanical Turk to obtain 240 opinion scores for each model with 12 speakers listening to each utterance.,B. Evaluation metrics,We compare to the following unconditional speech synthesis methods (Sec. II): WaveGAN,C. Baseline systems,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 1/17
==========================================================================================
While the previous comparisons demonstrated the overall success of ASGAN's design, we are still not certain which specific decisions from Sec.",5706729,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Regardless of performance, the speed of all the convolutional GAN models (WaveGAN and ASGAN) is significantly better than the diffusion and autoregressive models, as reasoned in Sec.",A. Comparison to baselines,III are responsible for its performance.,B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 2/17
==========================================================================================
III are responsible for its performance.",5706730,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"While the previous comparisons demonstrated the overall success of ASGAN's design, we are still not certain which specific decisions from Sec.",B. Ablation experiments,"So, we perform several ablations of the HuBERT ASGAN",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 3/17
==========================================================================================
So, we perform several ablations of the HuBERT ASGAN",5706731,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,III are responsible for its performance.,B. Ablation experiments,"Finally, we also train a variant without modulated convolutions (Sec.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 4/17
==========================================================================================
Finally, we also train a variant without modulated convolutions (Sec.",5706732,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"So, we perform several ablations of the HuBERT ASGAN",B. Ablation experiments,III-A2) such that w only controls the initial features passed to the generator's convolutional encoder. ,B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 5/17
==========================================================================================
III-A2) such that w only controls the initial features passed to the generator's convolutional encoder. ",5706733,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Finally, we also train a variant without modulated convolutions (Sec.",B. Ablation experiments,Table III shows the results for the ablated ASGAN approaches on a subset of the metrics.,B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 6/17
==========================================================================================
Table III shows the results for the ablated ASGAN approaches on a subset of the metrics.",5706734,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,III-A2) such that w only controls the initial features passed to the generator's convolutional encoder. ,B. Ablation experiments,"We see that on the quality and diversity metrics, the base ASGAN is best, while the models without adaptive discriminator updates and augmentation have better latent space disentanglement.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 7/17
==========================================================================================
We see that on the quality and diversity metrics, the base ASGAN is best, while the models without adaptive discriminator updates and augmentation have better latent space disentanglement.",5706735,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,Table III shows the results for the ablated ASGAN approaches on a subset of the metrics.,B. Ablation experiments,"However, recall that the main reason for including these was not to optimize disentanglement, but rather to ensure training stability and performance.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 8/17
==========================================================================================
However, recall that the main reason for including these was not to optimize disentanglement, but rather to ensure training stability and performance.",5706736,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We see that on the quality and diversity metrics, the base ASGAN is best, while the models without adaptive discriminator updates and augmentation have better latent space disentanglement.",B. Ablation experiments,As reasoned in Sec.,B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 9/17
==========================================================================================
As reasoned in Sec.",5706737,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"However, recall that the main reason for including these was not to optimize disentanglement, but rather to ensure training stability and performance.",B. Ablation experiments,"III, without either adaptive updates or augmentation, the discriminator has a much easier task and begins to dominate the generator, confidently distinguishing between real and generated samples.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 10/17
==========================================================================================
III, without either adaptive updates or augmentation, the discriminator has a much easier task and begins to dominate the generator, confidently distinguishing between real and generated samples.",5706738,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,As reasoned in Sec.,B. Ablation experiments,"So, while this makes optimization easier (leading to a smoother latent space), it means that the generator does not effectively learn from the adversarial task.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 11/17
==========================================================================================
So, while this makes optimization easier (leading to a smoother latent space), it means that the generator does not effectively learn from the adversarial task.",5706739,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"III, without either adaptive updates or augmentation, the discriminator has a much easier task and begins to dominate the generator, confidently distinguishing between real and generated samples.",B. Ablation experiments,"A similar phenomenon can be seen with WaveGAN in Table II, where it scored well on the disentanglement metrics but had poor output quality in Table I. ",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 12/17
==========================================================================================
A similar phenomenon can be seen with WaveGAN in Table II, where it scored well on the disentanglement metrics but had poor output quality in Table I. ",5706740,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"So, while this makes optimization easier (leading to a smoother latent space), it means that the generator does not effectively learn from the adversarial task.",B. Ablation experiments,"When anti-aliasing filters are removed, both latent space disentanglement and synthesis quality are reduced, being slightly worse in all metrics compared to the full model.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 13/17
==========================================================================================
When anti-aliasing filters are removed, both latent space disentanglement and synthesis quality are reduced, being slightly worse in all metrics compared to the full model.",5706741,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"A similar phenomenon can be seen with WaveGAN in Table II, where it scored well on the disentanglement metrics but had poor output quality in Table I. ",B. Ablation experiments,This validates our design motivation for the inclusion of low-pass filters to suppress aliased high-frequency content in the layer activations in Fig 1 .,B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 14/17
==========================================================================================
This validates our design motivation for the inclusion of low-pass filters to suppress aliased high-frequency content in the layer activations in Fig 1 .",5706742,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"When anti-aliasing filters are removed, both latent space disentanglement and synthesis quality are reduced, being slightly worse in all metrics compared to the full model.",B. Ablation experiments,"Finally, the variant without the linear influence of w on each layer's activations (i.e. without the modulated convolutions) is also worse than the baseline model in all metrics considered. ",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 15/17
==========================================================================================
Finally, the variant without the linear influence of w on each layer's activations (i.e. without the modulated convolutions) is also worse than the baseline model in all metrics considered. ",5706743,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,This validates our design motivation for the inclusion of low-pass filters to suppress aliased high-frequency content in the layer activations in Fig 1 .,B. Ablation experiments,"Overall, we can see from Table III that each of the key design aspects from Sec.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 16/17
==========================================================================================
Overall, we can see from Table III that each of the key design aspects from Sec.",5706744,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Finally, the variant without the linear influence of w on each layer's activations (i.e. without the modulated convolutions) is also worse than the baseline model in all metrics considered. ",B. Ablation experiments,"III are necessary to achieve both high latent space disentanglement and synthesis quality in a single model -the main requirement for it to be performant on unseen downstream tasks, which we look at next.",B. Ablation experiments,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
12. B. Ablation experiments -- 17/17
==========================================================================================
III are necessary to achieve both high latent space disentanglement and synthesis quality in a single model -the main requirement for it to be performant on unseen downstream tasks, which we look at next.",5706745,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Overall, we can see from Table III that each of the key design aspects from Sec.",B. Ablation experiments,We have now shown intrinsically that ASGAN leads to a disentangled latent space.,VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 1/6
==========================================================================================
To evaluate ASGAN's performance in each unseen task, we use the standard objective metrics from the literature: • Voice conversion: We measure conversion intelligibility following [47], [48], whereby we perform voice conversion and then apply a speech recognition system to the output and compute a character error rate (CER) and F 1 classification score to the word spoken in the original utterance.",5706746,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We compare our latent-space LDA-based speaker verification and keyword classification approaches to task-specific models in Sec. VIII.,B. Downstream tasks,"Speaker similarity is measured as described in [47] whereby we find similarity scores between real/generated utterance pairs using a trained speaker classifier, and then compute an EER with real/generated scores assigned a label of 0 and real/real pair scores assigned a label of 1. •",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 2/6
==========================================================================================
Speaker similarity is measured as described in [47] whereby we find similarity scores between real/generated utterance pairs using a trained speaker classifier, and then compute an EER with real/generated scores assigned a label of 0 and real/real pair scores assigned a label of 1. •",5706747,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"To evaluate ASGAN's performance in each unseen task, we use the standard objective metrics from the literature: • Voice conversion: We measure conversion intelligibility following [47], [48], whereby we perform voice conversion and then apply a speech recognition system to the output and compute a character error rate (CER) and F 1 classification score to the word spoken in the original utterance.",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,"Speech enhancement: Given a series of original clean and noisy utterances, and the models' denoised output, we compute standard measures of denoising performance: narrow-band perceptual evaluation of speech quality (PESQ) [49] and short term objective intelligibility (STOI) scores [50]. •",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 3/6
==========================================================================================
Speech enhancement: Given a series of original clean and noisy utterances, and the models' denoised output, we compute standard measures of denoising performance: narrow-band perceptual evaluation of speech quality (PESQ) [49] and short term objective intelligibility (STOI) scores [50]. •",5706748,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Speaker similarity is measured as described in [47] whereby we find similarity scores between real/generated utterance pairs using a trained speaker classifier, and then compute an EER with real/generated scores assigned a label of 0 and real/real pair scores assigned a label of 1. •",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,"Speaker verification: Using the similarity scores on randomly sampled pairs of utterances from matching and nonmatching speakers, we compute an EER as the measure of performance.",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 4/6
==========================================================================================
Speaker verification: Using the similarity scores on randomly sampled pairs of utterances from matching and nonmatching speakers, we compute an EER as the measure of performance.",5706749,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Speech enhancement: Given a series of original clean and noisy utterances, and the models' denoised output, we compute standard measures of denoising performance: narrow-band perceptual evaluation of speech quality (PESQ) [49] and short term objective intelligibility (STOI) scores [50]. •",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,We pair each evaluation utterance with another utterance from the same or different speaker with equal probability.,VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 5/6
==========================================================================================
We pair each evaluation utterance with another utterance from the same or different speaker with equal probability.",5706750,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Speaker verification: Using the similarity scores on randomly sampled pairs of utterances from matching and nonmatching speakers, we compute an EER as the measure of performance.",VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,Keyword classification: We use the standard classification metrics of accuracy and F 1 score.,VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics -- 6/6
==========================================================================================
Keyword classification: We use the standard classification metrics of accuracy and F 1 score.",5706751,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,We pair each evaluation utterance with another utterance from the same or different speaker with equal probability.,VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics,"For each downstream task, we compare ASGAN to a strong task-specific baseline trained on the SC09 dataset.",B. Baseline systems,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
18. VIII. RESULTS: UNSEEN TASKS -- 1/2
==========================================================================================
We apply ASGAN to a range of tasks that it didn't see during training, comparing it to established baselines.",5706752,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Finally, for keyword classification we, use the same ResNeXT classifier described in Sec.",B. Baseline systems,"Note, however, that the goal isn't to outperform these tailored systems on every task, but rather to show that a single model, ASGAN, can provide robust performance across a range of tasks that it was never trained on.",VIII. RESULTS: UNSEEN TASKS,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
18. VIII. RESULTS: UNSEEN TASKS -- 2/2
==========================================================================================
Note, however, that the goal isn't to outperform these tailored systems on every task, but rather to show that a single model, ASGAN, can provide robust performance across a range of tasks that it was never trained on.",5706753,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We apply ASGAN to a range of tasks that it didn't see during training, comparing it to established baselines.",VIII. RESULTS: UNSEEN TASKS,Following Sec.,A. Voice conversion,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 1/7
==========================================================================================
We introduced ASGAN, a model for unconditional speech synthesis designed to learn a disentangled latent space.",5706754,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"From both the LDA plot in Fig. 4 and Table V, we again observe strong evidence that our design for disentanglement is successful.",D. Keyword classification,"We adapted existing and incorporated new GAN design and training techniques to enable ASGAN to learn a continuous, linearly disentangled latent space in order to outperform existing autoregressive and diffusion models.",IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 2/7
==========================================================================================
We adapted existing and incorporated new GAN design and training techniques to enable ASGAN to learn a continuous, linearly disentangled latent space in order to outperform existing autoregressive and diffusion models.",5706755,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We introduced ASGAN, a model for unconditional speech synthesis designed to learn a disentangled latent space.",IX. CONCLUSION,"Experiments on the SC09 dataset demonstrated that ASGAN outperforms previous state-of-the-art models on most unconditional speech synthesis metrics, while also being substantially faster.",IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 3/7
==========================================================================================
Experiments on the SC09 dataset demonstrated that ASGAN outperforms previous state-of-the-art models on most unconditional speech synthesis metrics, while also being substantially faster.",5706756,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"We adapted existing and incorporated new GAN design and training techniques to enable ASGAN to learn a continuous, linearly disentangled latent space in order to outperform existing autoregressive and diffusion models.",IX. CONCLUSION,"Further experiments also demonstrated the benefit of the disentangled latent space: ASGAN can, without any additional training, perform several speech processing tasks in a zero-shot fashion through linear operations in its latent space, showing reasonable performance in voice conversion, speech enhancement, speaker verification, and keyword classification. ",IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 4/7
==========================================================================================
Further experiments also demonstrated the benefit of the disentangled latent space: ASGAN can, without any additional training, perform several speech processing tasks in a zero-shot fashion through linear operations in its latent space, showing reasonable performance in voice conversion, speech enhancement, speaker verification, and keyword classification. ",5706757,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Experiments on the SC09 dataset demonstrated that ASGAN outperforms previous state-of-the-art models on most unconditional speech synthesis metrics, while also being substantially faster.",IX. CONCLUSION,"One major limitation of our work is scale: once trained, ASGAN can only generate utterances of a fixed length, and the model struggles to generate coherent full sentences on datasets with longer utterances (a limitation shared by existing unconditional synthesis models [1], [5], [6]).",IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 5/7
==========================================================================================
One major limitation of our work is scale: once trained, ASGAN can only generate utterances of a fixed length, and the model struggles to generate coherent full sentences on datasets with longer utterances (a limitation shared by existing unconditional synthesis models [1], [5], [6]).",5706758,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Further experiments also demonstrated the benefit of the disentangled latent space: ASGAN can, without any additional training, perform several speech processing tasks in a zero-shot fashion through linear operations in its latent space, showing reasonable performance in voice conversion, speech enhancement, speaker verification, and keyword classification. ",IX. CONCLUSION,"Furthermore, the method used to project utterances to the latent space could be improved by incorporating recent inversion methods [55].",IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 6/7
==========================================================================================
Furthermore, the method used to project utterances to the latent space could be improved by incorporating recent inversion methods [55].",5706759,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"One major limitation of our work is scale: once trained, ASGAN can only generate utterances of a fixed length, and the model struggles to generate coherent full sentences on datasets with longer utterances (a limitation shared by existing unconditional synthesis models [1], [5], [6]).",IX. CONCLUSION,Future work will aim to address these shortcomings.,IX. CONCLUSION,
"Disentanglement in a GAN for Unconditional Speech Synthesis
==========================================================================================
23. IX. CONCLUSION -- 7/7
==========================================================================================
Future work will aim to address these shortcomings.",5706760,Disentanglement in a GAN for Unconditional Speech Synthesis,"0. abstract
1. I. INTRODUCTION
2. II. RELATED WORK
3. III. AUDIO STYLE GAN
4. A. Generator
5. B. Discriminator
6. C. Vocoder
7. D. Implementation
8. A. Data
9. B. Evaluation metrics
10. C. Baseline systems
11. A. Comparison to baselines
12. B. Ablation experiments
13. VI. SOLVING UNSEEN TASKS THROUGH LINEAR LATENT OPERATIONS
14. A. Projecting to the latent space
15. B. Downstream tasks
16. VII. EXPERIMENTAL SETUP: UNSEEN TASKS A. Evaluation metrics
17. B. Baseline systems
18. VIII. RESULTS: UNSEEN TASKS
19. A. Voice conversion
20. B. Speech enhancement
21. C. Speaker verification
22. D. Keyword classification
23. IX. CONCLUSION
",2023,"Furthermore, the method used to project utterances to the latent space could be improved by incorporating recent inversion methods [55].",IX. CONCLUSION,,,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 1/9
==========================================================================================
Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query.",577285,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,,,"Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 2/9
==========================================================================================
Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas.",577286,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query.",abstract,"To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 3/9
==========================================================================================
To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels.",577287,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas.",abstract,"By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 4/9
==========================================================================================
By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema.",577288,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels.",abstract,"Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 5/9
==========================================================================================
Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema.",577289,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema.",abstract,"Finally, a SQL decoder with context-free grammar is applied.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 6/9
==========================================================================================
Finally, a SQL decoder with context-free grammar is applied.",577290,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema.",abstract,"On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 7/9
==========================================================================================
On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models.",577291,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Finally, a SQL decoder with context-free grammar is applied.",abstract,"When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability.",abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 8/9
==========================================================================================
When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability.",577292,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models.",abstract,Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.,abstract,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
0. abstract -- 9/9
==========================================================================================
Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.",577293,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability.",abstract,"Recently, Text-to-SQL has drawn a great deal of attention from the semantic parsing community (Berant et al., 2013;Cao et al., 2019Cao et al., , 2020)).",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 1/33
==========================================================================================
Recently, Text-to-SQL has drawn a great deal of attention from the semantic parsing community (Berant et al., 2013;Cao et al., 2019Cao et al., , 2020)).",577294,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.,abstract,"The ability to query a database with natural language (NL) engages the majority of users, who are not familiar with SQL language, in visiting large databases.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 2/33
==========================================================================================
The ability to query a database with natural language (NL) engages the majority of users, who are not familiar with SQL language, in visiting large databases.",577295,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Recently, Text-to-SQL has drawn a great deal of attention from the semantic parsing community (Berant et al., 2013;Cao et al., 2019Cao et al., , 2020)).",Introduction,A number of neural approaches have been proposed to translate questions into executable SQL queries.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 3/33
==========================================================================================
A number of neural approaches have been proposed to translate questions into executable SQL queries.",577296,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The ability to query a database with natural language (NL) engages the majority of users, who are not familiar with SQL language, in visiting large databases.",Introduction,"On public Text-to-SQL benchmarks (Zhong et al., *",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 4/33
==========================================================================================
On public Text-to-SQL benchmarks (Zhong et al., *",577297,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,A number of neural approaches have been proposed to translate questions into executable SQL queries.,Introduction,"The corresponding authors are Lu Chen and Kai Yu. 2017; Krishnamurthy et al., 2017), exact match accuracy even excesses more than 80%.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 5/33
==========================================================================================
The corresponding authors are Lu Chen and Kai Yu. 2017; Krishnamurthy et al., 2017), exact match accuracy even excesses more than 80%.",577298,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"On public Text-to-SQL benchmarks (Zhong et al., *",Introduction,"However, the cross-domain problem for Text-to-SQL is a practical challenge and ignored by the prior datasets. ",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 6/33
==========================================================================================
However, the cross-domain problem for Text-to-SQL is a practical challenge and ignored by the prior datasets. ",577299,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The corresponding authors are Lu Chen and Kai Yu. 2017; Krishnamurthy et al., 2017), exact match accuracy even excesses more than 80%.",Introduction,"To be clarified, a database schema is regarded as a domain.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 7/33
==========================================================================================
To be clarified, a database schema is regarded as a domain.",577300,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"However, the cross-domain problem for Text-to-SQL is a practical challenge and ignored by the prior datasets. ",Introduction,"The domain information consists of two parts: the semantic information (e.g., the table name) of the schema components and the structure information (e.g., the primary-key relation between a table and a column) of the schema. ",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 8/33
==========================================================================================
The domain information consists of two parts: the semantic information (e.g., the table name) of the schema components and the structure information (e.g., the primary-key relation between a table and a column) of the schema. ",577301,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"To be clarified, a database schema is regarded as a domain.",Introduction,"The recently released dataset, Spider (Yu et al., 2018), hides the database schemas of the test set, which are totally unseen on the training set.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 9/33
==========================================================================================
The recently released dataset, Spider (Yu et al., 2018), hides the database schemas of the test set, which are totally unseen on the training set.",577302,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The domain information consists of two parts: the semantic information (e.g., the table name) of the schema components and the structure information (e.g., the primary-key relation between a table and a column) of the schema. ",Introduction,"In this cross-domain setup, domain adaptation is challenging for two main reasons.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 10/33
==========================================================================================
In this cross-domain setup, domain adaptation is challenging for two main reasons.",577303,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The recently released dataset, Spider (Yu et al., 2018), hides the database schemas of the test set, which are totally unseen on the training set.",Introduction,"First, the semantic information of the domains in the test and development set are unseen in the training set.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 11/33
==========================================================================================
First, the semantic information of the domains in the test and development set are unseen in the training set.",577304,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"In this cross-domain setup, domain adaptation is challenging for two main reasons.",Introduction,"On the given development set, 35% of words in database schemas do not occur in the schemas on the training set.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 12/33
==========================================================================================
On the given development set, 35% of words in database schemas do not occur in the schemas on the training set.",577305,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"First, the semantic information of the domains in the test and development set are unseen in the training set.",Introduction,It is hard to match the domain representations in the question and the schema.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 13/33
==========================================================================================
It is hard to match the domain representations in the question and the schema.",577306,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"On the given development set, 35% of words in database schemas do not occur in the schemas on the training set.",Introduction,"Second, there is a considerable discrepancy among the structure of the database schemas.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 14/33
==========================================================================================
Second, there is a considerable discrepancy among the structure of the database schemas.",577307,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,It is hard to match the domain representations in the question and the schema.,Introduction,"Especially, the database schemas always contain semantic information.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 15/33
==========================================================================================
Especially, the database schemas always contain semantic information.",577308,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Second, there is a considerable discrepancy among the structure of the database schemas.",Introduction,It is difficult to get the unified representation of the database schema.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 16/33
==========================================================================================
It is difficult to get the unified representation of the database schema.",577309,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Especially, the database schemas always contain semantic information.",Introduction,"Under the cross-domain setup, the essential challenge is to alleviate the impact of the domain information. ",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 17/33
==========================================================================================
Under the cross-domain setup, the essential challenge is to alleviate the impact of the domain information. ",577310,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,It is difficult to get the unified representation of the database schema.,Introduction,"First, it is necessary to figure out which role the semantic information of the schema components play during translating an NL question into a SQL query.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 18/33
==========================================================================================
First, it is necessary to figure out which role the semantic information of the schema components play during translating an NL question into a SQL query.",577311,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Under the cross-domain setup, the essential challenge is to alleviate the impact of the domain information. ",Introduction,"Consider the example in Fig. 1(a), for the Text-to-SQL model, the basic task is to find out all the mentioned columns (name) and tables (team, match season) by looking up the schema with semantic information (named as semantic schema).",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 19/33
==========================================================================================
Consider the example in Fig. 1(a), for the Text-to-SQL model, the basic task is to find out all the mentioned columns (name) and tables (team, match season) by looking up the schema with semantic information (named as semantic schema).",577312,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"First, it is necessary to figure out which role the semantic information of the schema components play during translating an NL question into a SQL query.",Introduction,"Once the mentioned columns and tables in the NL question are exactly matched with schema components, we can abstract the NL question and the semantic schema by replacing the general component type with the specific schema components.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 20/33
==========================================================================================
Once the mentioned columns and tables in the NL question are exactly matched with schema components, we can abstract the NL question and the semantic schema by replacing the general component type with the specific schema components.",577313,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Consider the example in Fig. 1(a), for the Text-to-SQL model, the basic task is to find out all the mentioned columns (name) and tables (team, match season) by looking up the schema with semantic information (named as semantic schema).",Introduction,"As shown in Fig. 1(b), we can still infer the structure of the SQL query using the abstract NL question and the schema structure.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 21/33
==========================================================================================
As shown in Fig. 1(b), we can still infer the structure of the SQL query using the abstract NL question and the schema structure.",577314,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Once the mentioned columns and tables in the NL question are exactly matched with schema components, we can abstract the NL question and the semantic schema by replacing the general component type with the specific schema components.",Introduction,"With the corresponding relation between semantic schema and abstract schema, we can restore the abstract query to executable SQL query with domain information.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 22/33
==========================================================================================
With the corresponding relation between semantic schema and abstract schema, we can restore the abstract query to executable SQL query with domain information.",577315,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"As shown in Fig. 1(b), we can still infer the structure of the SQL query using the abstract NL question and the schema structure.",Introduction,"Inspired by this phenomenon, we decompose the encoder of the Text-to-SQL model into two modules.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 23/33
==========================================================================================
Inspired by this phenomenon, we decompose the encoder of the Text-to-SQL model into two modules.",577316,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"With the corresponding relation between semantic schema and abstract schema, we can restore the abstract query to executable SQL query with domain information.",Introduction,"First, we propose a Graph Projection Neural Network (GPNN) to abstract the NL question and the semantic schema, where the domain information is removed as much as possible.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 24/33
==========================================================================================
First, we propose a Graph Projection Neural Network (GPNN) to abstract the NL question and the semantic schema, where the domain information is removed as much as possible.",577317,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Inspired by this phenomenon, we decompose the encoder of the Text-to-SQL model into two modules.",Introduction,"Then, we use the relation-aware transformer to get unified representations of abstract NL question and abstract schema. ",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 25/33
==========================================================================================
Then, we use the relation-aware transformer to get unified representations of abstract NL question and abstract schema. ",577318,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"First, we propose a Graph Projection Neural Network (GPNN) to abstract the NL question and the semantic schema, where the domain information is removed as much as possible.",Introduction,"Our approach, named ShadowGNN, is evaluated on the challenging cross-domain Text-to-SQL dataset, Spider.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 26/33
==========================================================================================
Our approach, named ShadowGNN, is evaluated on the challenging cross-domain Text-to-SQL dataset, Spider.",577319,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Then, we use the relation-aware transformer to get unified representations of abstract NL question and abstract schema. ",Introduction,Contributions are summarized as: •,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 27/33
==========================================================================================
Contributions are summarized as: •",577320,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Our approach, named ShadowGNN, is evaluated on the challenging cross-domain Text-to-SQL dataset, Spider.",Introduction,We propose the ShadowGNN to alleviate the impact of the domain information by abstracting the representation of NL question and SQL query.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 28/33
==========================================================================================
We propose the ShadowGNN to alleviate the impact of the domain information by abstracting the representation of NL question and SQL query.",577321,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,Contributions are summarized as: •,Introduction,It is a meaningful method to apply to similar cross-domain tasks. ,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 29/33
==========================================================================================
It is a meaningful method to apply to similar cross-domain tasks. ",577322,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We propose the ShadowGNN to alleviate the impact of the domain information by abstracting the representation of NL question and SQL query.,Introduction,"• To validate the generalization capability of our proposed ShadowGNN, we conduct the experiments with limited annotated data.",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 30/33
==========================================================================================
• To validate the generalization capability of our proposed ShadowGNN, we conduct the experiments with limited annotated data.",577323,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,It is a meaningful method to apply to similar cross-domain tasks. ,Introduction,"The results show that our proposed ShadowGNN can obtain absolute over 5% accuracy gain compared with state-of-the-art model, when the annotated data only has the scale of 10% of the training set. ",Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 31/33
==========================================================================================
The results show that our proposed ShadowGNN can obtain absolute over 5% accuracy gain compared with state-of-the-art model, when the annotated data only has the scale of 10% of the training set. ",577324,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"• To validate the generalization capability of our proposed ShadowGNN, we conduct the experiments with limited annotated data.",Introduction,The empirical results show that our approach outperforms state-of-the-art models (66.1% accuracy on test set) on the challenging Spider benchmark.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 32/33
==========================================================================================
The empirical results show that our approach outperforms state-of-the-art models (66.1% accuracy on test set) on the challenging Spider benchmark.",577325,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The results show that our proposed ShadowGNN can obtain absolute over 5% accuracy gain compared with state-of-the-art model, when the annotated data only has the scale of 10% of the training set. ",Introduction,The ablation studies further confirm that GPNN is important to abstract the representation of the NL question and the schema.,Introduction,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
1. Introduction -- 33/33
==========================================================================================
The ablation studies further confirm that GPNN is important to abstract the representation of the NL question and the schema.",577326,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The empirical results show that our approach outperforms state-of-the-art models (66.1% accuracy on test set) on the challenging Spider benchmark.,Introduction,"In this section, we first introduce relational graph convolution network (R-GCN) (Schlichtkrull et al., 2018), which is the basis of our proposed GPNN.",Background,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 1/6
==========================================================================================
In this section, we evaluate the effectiveness of our proposed ShadowGNN than other strong baselines.",577327,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Then, a detail decoder fills in the missing details in the skeleton by selecting columns and tables.",Decoder with SemQL Grammar,We further conduct the experiments with limited annotated training data to validate the generalization capability of the proposed ShadowGNN.,Experiments,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 2/6
==========================================================================================
We further conduct the experiments with limited annotated training data to validate the generalization capability of the proposed ShadowGNN.",577328,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"In this section, we evaluate the effectiveness of our proposed ShadowGNN than other strong baselines.",Experiments,"Finally, we ablate other designed choices to understand their contributions.",Experiments,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 3/6
==========================================================================================
Finally, we ablate other designed choices to understand their contributions.",577329,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We further conduct the experiments with limited annotated training data to validate the generalization capability of the proposed ShadowGNN.,Experiments,"Dataset & Metrics We conduct the experiments on the Spider (Yu et al., 2018), which is a large-scale, complex and cross-domain Text-to-SQL benchmark.",Experiment Setup,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 4/6
==========================================================================================
0. Experiment Setup -- 1/3
------------------------------------------------------------------------------------------
Dataset & Metrics We conduct the experiments on the Spider (Yu et al., 2018), which is a large-scale, complex and cross-domain Text-to-SQL benchmark.",577330,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Finally, we ablate other designed choices to understand their contributions.",Experiments,"The databases on the Spider are split into 146 training, 20 development and 40 test.",Experiment Setup,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 5/6
==========================================================================================
0. Experiment Setup -- 2/3
------------------------------------------------------------------------------------------
The databases on the Spider are split into 146 training, 20 development and 40 test.",577331,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Dataset & Metrics We conduct the experiments on the Spider (Yu et al., 2018), which is a large-scale, complex and cross-domain Text-to-SQL benchmark.",Experiment Setup,The humanlabeled question-SQL query pairs are divided into,Experiment Setup,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
4. Experiments -- 6/6
==========================================================================================
0. Experiment Setup -- 3/3
------------------------------------------------------------------------------------------
The humanlabeled question-SQL query pairs are divided into",577332,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The databases on the Spider are split into 146 training, 20 development and 40 test.",Experiment Setup,"Global-GNN (Bogin et al., 2019b) 52.7% 47.4% R-GCN + Bertrand-DR (Kelkar et al., 2020) 57.9% 54.6% IRNet v2 (Guo et al., 2019) 63.9% 55.0% RATSQL v3 +",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 1/50
==========================================================================================
Global-GNN (Bogin et al., 2019b) 52.7% 47.4% R-GCN + Bertrand-DR (Kelkar et al., 2020) 57.9% 54.6% IRNet v2 (Guo et al., 2019) 63.9% 55.0% RATSQL v3 +",577333,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The humanlabeled question-SQL query pairs are divided into,Experiment Setup,"BERT-large (Wang et al., 2020)  ",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 2/50
==========================================================================================
BERT-large (Wang et al., 2020)  ",577334,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Global-GNN (Bogin et al., 2019b) 52.7% 47.4% R-GCN + Bertrand-DR (Kelkar et al., 2020) 57.9% 54.6% IRNet v2 (Guo et al., 2019) 63.9% 55.0% RATSQL v3 +",Dev. Test,8625/1034/2147 for train/development/test.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 3/50
==========================================================================================
8625/1034/2147 for train/development/test.",577335,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"BERT-large (Wang et al., 2020)  ",Dev. Test,"The test set is not available for the public, like all the competition challenges.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 4/50
==========================================================================================
The test set is not available for the public, like all the competition challenges.",577336,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,8625/1034/2147 for train/development/test.,Dev. Test,"We report the results with the same metrics as (Yu et al., 2018): exact match accuracy and component match accuracy. ",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 5/50
==========================================================================================
We report the results with the same metrics as (Yu et al., 2018): exact match accuracy and component match accuracy. ",577337,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The test set is not available for the public, like all the competition challenges.",Dev. Test,Baselines The main contribution of this paper lies on the encoder of the Text-to-SQL model.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 6/50
==========================================================================================
Baselines The main contribution of this paper lies on the encoder of the Text-to-SQL model.",577338,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"We report the results with the same metrics as (Yu et al., 2018): exact match accuracy and component match accuracy. ",Dev. Test,"As for the decoder of our evaluated models, we improve the SemQL grammar of the IRNet (Guo et al., 2019), where the recover success rate raises from 89.6% to 99.9%.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 7/50
==========================================================================================
As for the decoder of our evaluated models, we improve the SemQL grammar of the IRNet (Guo et al., 2019), where the recover success rate raises from 89.6% to 99.9%.",577339,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,Baselines The main contribution of this paper lies on the encoder of the Text-to-SQL model.,Dev. Test,"The SQL query first is represented by an abstract syntax tree (AST) following the well-designed grammar (Lin et al., 2019).",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 8/50
==========================================================================================
The SQL query first is represented by an abstract syntax tree (AST) following the well-designed grammar (Lin et al., 2019).",577340,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"As for the decoder of our evaluated models, we improve the SemQL grammar of the IRNet (Guo et al., 2019), where the recover success rate raises from 89.6% to 99.9%.",Dev. Test,"Then, the AST is flattened as a sequence (named SemQL query) by the deep-first search (DFS) method.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 9/50
==========================================================================================
Then, the AST is flattened as a sequence (named SemQL query) by the deep-first search (DFS) method.",577341,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The SQL query first is represented by an abstract syntax tree (AST) following the well-designed grammar (Lin et al., 2019).",Dev. Test,"During decoding, it is still predicted one by one with LSTM decoder.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 10/50
==========================================================================================
During decoding, it is still predicted one by one with LSTM decoder.",577342,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Then, the AST is flattened as a sequence (named SemQL query) by the deep-first search (DFS) method.",Dev. Test,We also leverage the coarse-to-fine approach to the decoder as IRNet.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 11/50
==========================================================================================
We also leverage the coarse-to-fine approach to the decoder as IRNet.",577343,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"During decoding, it is still predicted one by one with LSTM decoder.",Dev. Test,A skeleton decoder first outputs a skeleton of the SemQL query.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 12/50
==========================================================================================
A skeleton decoder first outputs a skeleton of the SemQL query.",577344,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We also leverage the coarse-to-fine approach to the decoder as IRNet.,Dev. Test,"Then, a detail decoder fills in the missing details in the skeleton by selecting columns and tables.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 13/50
==========================================================================================
Then, a detail decoder fills in the missing details in the skeleton by selecting columns and tables.",577345,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,A skeleton decoder first outputs a skeleton of the SemQL query.,Dev. Test,"R-GCN (Bogin et al., 2019a;Kelkar et al., 2020) and RATSQL (Wang et al., 2020) are two other strong baselines, which improve the representation ability of the encoder. ",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 14/50
==========================================================================================
R-GCN (Bogin et al., 2019a;Kelkar et al., 2020) and RATSQL (Wang et al., 2020) are two other strong baselines, which improve the representation ability of the encoder. ",577346,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Then, a detail decoder fills in the missing details in the skeleton by selecting columns and tables.",Dev. Test,"Implementations We implement ShadowGNN and our baseline approaches with PyTorch (Paszke et al., 2019).",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 15/50
==========================================================================================
Implementations We implement ShadowGNN and our baseline approaches with PyTorch (Paszke et al., 2019).",577347,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"R-GCN (Bogin et al., 2019a;Kelkar et al., 2020) and RATSQL (Wang et al., 2020) are two other strong baselines, which improve the representation ability of the encoder. ",Dev. Test,"We use the pretrained models RoBERTa from PyTorch transformer repository (Wolf et al., 2019).",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 16/50
==========================================================================================
We use the pretrained models RoBERTa from PyTorch transformer repository (Wolf et al., 2019).",577348,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Implementations We implement ShadowGNN and our baseline approaches with PyTorch (Paszke et al., 2019).",Dev. Test,We use Adam with default hyperparameters for optimization.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 17/50
==========================================================================================
We use Adam with default hyperparameters for optimization.",577349,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"We use the pretrained models RoBERTa from PyTorch transformer repository (Wolf et al., 2019).",Dev. Test,"The learning rate is set to 2e-4, but there is 0.1 weight decay for the learning rate of pretrained model.",Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 18/50
==========================================================================================
The learning rate is set to 2e-4, but there is 0.1 weight decay for the learning rate of pretrained model.",577350,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We use Adam with default hyperparameters for optimization.,Dev. Test,The hidden sizes of GPNN layer and RAT layer are set to 512.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 19/50
==========================================================================================
The hidden sizes of GPNN layer and RAT layer are set to 512.",577351,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The learning rate is set to 2e-4, but there is 0.1 weight decay for the learning rate of pretrained model.",Dev. Test,The dropout rate is 0.3.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 20/50
==========================================================================================
The dropout rate is 0.3.",577352,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The hidden sizes of GPNN layer and RAT layer are set to 512.,Dev. Test,Batch size is set to 16.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 21/50
==========================================================================================
Batch size is set to 16.",577353,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The dropout rate is 0.3.,Dev. Test,The layers of GPNN and RAT in ShadowGNN encoder are set to 4.,Dev. Test,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 22/50
==========================================================================================
The layers of GPNN and RAT in ShadowGNN encoder are set to 4.",577354,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,Batch size is set to 16.,Dev. Test,"To fairly compared with our proposed Shad-owGNN, we implement RATSQL (Wang et al., 2020) with the same coarse-to-fine decoder and RoBERTa augmentation of ShadowGNN model.",Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 23/50
==========================================================================================
0. Experimental Results -- 1/6
------------------------------------------------------------------------------------------
To fairly compared with our proposed Shad-owGNN, we implement RATSQL (Wang et al., 2020) with the same coarse-to-fine decoder and RoBERTa augmentation of ShadowGNN model.",577355,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The layers of GPNN and RAT in ShadowGNN encoder are set to 4.,Dev. Test,We also report the performance of GPNN encoder on test set.,Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 24/50
==========================================================================================
0. Experimental Results -- 2/6
------------------------------------------------------------------------------------------
We also report the performance of GPNN encoder on test set.",577356,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"To fairly compared with our proposed Shad-owGNN, we implement RATSQL (Wang et al., 2020) with the same coarse-to-fine decoder and RoBERTa augmentation of ShadowGNN model.",Experimental Results,The detail implementations of these two baselines show as following: • RATSQL ♣ RATSQL model replaces the four projection layers with another four relationaware self-attention layers.,Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 25/50
==========================================================================================
0. Experimental Results -- 3/6
------------------------------------------------------------------------------------------
The detail implementations of these two baselines show as following: • RATSQL ♣ RATSQL model replaces the four projection layers with another four relationaware self-attention layers.",577357,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We also report the performance of GPNN encoder on test set.,Experimental Results,"There are totally eight relation-aware self-attention layers in the encoder, which is consistent with orignal RAT-SQL setup (Wang et al., 2020). ",Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 26/50
==========================================================================================
0. Experimental Results -- 4/6
------------------------------------------------------------------------------------------
There are totally eight relation-aware self-attention layers in the encoder, which is consistent with orignal RAT-SQL setup (Wang et al., 2020). ",577358,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The detail implementations of these two baselines show as following: • RATSQL ♣ RATSQL model replaces the four projection layers with another four relationaware self-attention layers.,Experimental Results,"• GPNN Compared with ShadowGNN, GPNN model directly removes the relation-aware transformer.",Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 27/50
==========================================================================================
0. Experimental Results -- 5/6
------------------------------------------------------------------------------------------
• GPNN Compared with ShadowGNN, GPNN model directly removes the relation-aware transformer.",577359,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"There are totally eight relation-aware self-attention layers in the encoder, which is consistent with orignal RAT-SQL setup (Wang et al., 2020). ",Experimental Results,"There are only four projection layers in the encoder, which can get better performance than eight layers.",Experimental Results,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 28/50
==========================================================================================
0. Experimental Results -- 6/6
------------------------------------------------------------------------------------------
There are only four projection layers in the encoder, which can get better performance than eight layers.",577360,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"• GPNN Compared with ShadowGNN, GPNN model directly removes the relation-aware transformer.",Experimental Results,We design an experiment to validate the effectiveness of the graph projection neural network (GPNN).,Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 29/50
==========================================================================================
1. Generalization Capability -- 1/9
------------------------------------------------------------------------------------------
We design an experiment to validate the effectiveness of the graph projection neural network (GPNN).",577361,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"There are only four projection layers in the encoder, which can get better performance than eight layers.",Experimental Results,"Considering a question ""What is name and capacity of stadium with most concert after year ?"", which has been preprocessed, ""name"" and ""capacity"" are column names.",Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 30/50
==========================================================================================
1. Generalization Capability -- 2/9
------------------------------------------------------------------------------------------
Considering a question ""What is name and capacity of stadium with most concert after year ?"", which has been preprocessed, ""name"" and ""capacity"" are column names.",577362,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We design an experiment to validate the effectiveness of the graph projection neural network (GPNN).,Generalization Capability,We exchange their positions and calculate the cosine similarity with the representations of the final GPNN layer in Shad-owGNN model.,Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 31/50
==========================================================================================
1. Generalization Capability -- 3/9
------------------------------------------------------------------------------------------
We exchange their positions and calculate the cosine similarity with the representations of the final GPNN layer in Shad-owGNN model.",577363,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Considering a question ""What is name and capacity of stadium with most concert after year ?"", which has been preprocessed, ""name"" and ""capacity"" are column names.",Generalization Capability,"Interestingly, we find that ""name"" has the most similar with ""capacity"", as shown in Figure 3.",Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 32/50
==========================================================================================
1. Generalization Capability -- 4/9
------------------------------------------------------------------------------------------
Interestingly, we find that ""name"" has the most similar with ""capacity"", as shown in Figure 3.",577364,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We exchange their positions and calculate the cosine similarity with the representations of the final GPNN layer in Shad-owGNN model.,Generalization Capability,The semantic meaning of the two column names seems to be removed that the representations of the two column names only dependent on the existed positions.,Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 33/50
==========================================================================================
1. Generalization Capability -- 5/9
------------------------------------------------------------------------------------------
The semantic meaning of the two column names seems to be removed that the representations of the two column names only dependent on the existed positions.",577365,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Interestingly, we find that ""name"" has the most similar with ""capacity"", as shown in Figure 3.",Generalization Capability,It indicates the GPNN can get the abstract representation of the question. ,Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 34/50
==========================================================================================
1. Generalization Capability -- 6/9
------------------------------------------------------------------------------------------
It indicates the GPNN can get the abstract representation of the question. ",577366,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The semantic meaning of the two column names seems to be removed that the representations of the two column names only dependent on the existed positions.,Generalization Capability,"To further validate the generalization ability of our proposed ShadowGNN, we conduct the experiments on the limited annotated training datasets.",Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 35/50
==========================================================================================
1. Generalization Capability -- 7/9
------------------------------------------------------------------------------------------
To further validate the generalization ability of our proposed ShadowGNN, we conduct the experiments on the limited annotated training datasets.",577367,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,It indicates the GPNN can get the abstract representation of the question. ,Generalization Capability,"The limited training datasets are sampled from fully training dataset with 10%, 50% and 100% sampling rate.",Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 36/50
==========================================================================================
1. Generalization Capability -- 8/9
------------------------------------------------------------------------------------------
The limited training datasets are sampled from fully training dataset with 10%, 50% and 100% sampling rate.",577368,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"To further validate the generalization ability of our proposed ShadowGNN, we conduct the experiments on the limited annotated training datasets.",Generalization Capability,"As shown in Figure 4, there is a large performance gap between RATSQL and ShadowGNN, when the annotated data is extremely limited only occupied 10% of the fully training dataset.",Generalization Capability,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 37/50
==========================================================================================
1. Generalization Capability -- 9/9
------------------------------------------------------------------------------------------
As shown in Figure 4, there is a large performance gap between RATSQL and ShadowGNN, when the annotated data is extremely limited only occupied 10% of the fully training dataset.",577369,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"The limited training datasets are sampled from fully training dataset with 10%, 50% and 100% sampling rate.",Generalization Capability,We conduct ablation studies to analyze the contributions of well-designed graph projection neural network (GPNN).,Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 38/50
==========================================================================================
2. Ablation Studies -- 1/13
------------------------------------------------------------------------------------------
We conduct ablation studies to analyze the contributions of well-designed graph projection neural network (GPNN).",577370,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"As shown in Figure 4, there is a large performance gap between RATSQL and ShadowGNN, when the annotated data is extremely limited only occupied 10% of the fully training dataset.",Generalization Capability,"Except RATSQL and GPNN models, we implement other two ablation models: R-GCN and R-GCN+RAT.",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 39/50
==========================================================================================
2. Ablation Studies -- 2/13
------------------------------------------------------------------------------------------
Except RATSQL and GPNN models, we implement other two ablation models: R-GCN and R-GCN+RAT.",577371,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We conduct ablation studies to analyze the contributions of well-designed graph projection neural network (GPNN).,Ablation Studies,"First, we introduce the implementations of the ablation models. ",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 40/50
==========================================================================================
2. Ablation Studies -- 3/13
------------------------------------------------------------------------------------------
First, we introduce the implementations of the ablation models. ",577372,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Except RATSQL and GPNN models, we implement other two ablation models: R-GCN and R-GCN+RAT.",Ablation Studies,• R-GCN ♣ We directly remove the projection part in the GPNN.,Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 41/50
==========================================================================================
2. Ablation Studies -- 4/13
------------------------------------------------------------------------------------------
• R-GCN ♣ We directly remove the projection part in the GPNN.",577373,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"First, we introduce the implementations of the ablation models. ",Ablation Studies,"When updating the question representation, we use the representation of semantic schema as attention value instead of abstract representation. ",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 42/50
==========================================================================================
2. Ablation Studies -- 5/13
------------------------------------------------------------------------------------------
When updating the question representation, we use the representation of semantic schema as attention value instead of abstract representation. ",577374,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,• R-GCN ♣ We directly remove the projection part in the GPNN.,Ablation Studies,"• R-GCN+RAT In this model, there are four R-GCN layers and four relation-aware selfattention layers.",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 43/50
==========================================================================================
2. Ablation Studies -- 6/13
------------------------------------------------------------------------------------------
• R-GCN+RAT In this model, there are four R-GCN layers and four relation-aware selfattention layers.",577375,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"When updating the question representation, we use the representation of semantic schema as attention value instead of abstract representation. ",Ablation Studies,"To be comparable, the initial input of R-GCN is the sum of semantic schema and abstract schema. ",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 44/50
==========================================================================================
2. Ablation Studies -- 7/13
------------------------------------------------------------------------------------------
To be comparable, the initial input of R-GCN is the sum of semantic schema and abstract schema. ",577376,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"• R-GCN+RAT In this model, there are four R-GCN layers and four relation-aware selfattention layers.",Ablation Studies,The decoder parts of these four ablation models are the same as the decoder of ShadowGNN.,Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 45/50
==========================================================================================
2. Ablation Studies -- 8/13
------------------------------------------------------------------------------------------
The decoder parts of these four ablation models are the same as the decoder of ShadowGNN.",577377,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"To be comparable, the initial input of R-GCN is the sum of semantic schema and abstract schema. ",Ablation Studies,"We present the accuracy of the ablation models at the four hardness levels on the development set, which is defined in (Yu et al., 2018).",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 46/50
==========================================================================================
2. Ablation Studies -- 9/13
------------------------------------------------------------------------------------------
We present the accuracy of the ablation models at the four hardness levels on the development set, which is defined in (Yu et al., 2018).",577378,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The decoder parts of these four ablation models are the same as the decoder of ShadowGNN.,Ablation Studies,"As shown in Table 2, ShadowGNN can get the best performance at three hardness levels.",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 47/50
==========================================================================================
2. Ablation Studies -- 10/13
------------------------------------------------------------------------------------------
As shown in Table 2, ShadowGNN can get the best performance at three hardness levels.",577379,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"We present the accuracy of the ablation models at the four hardness levels on the development set, which is defined in (Yu et al., 2018).",Ablation Studies,"Compared with R-GCN (Kelkar et al., 2020), our implemented R-GCN based on SemQL grammar gets higher performance.",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 48/50
==========================================================================================
2. Ablation Studies -- 11/13
------------------------------------------------------------------------------------------
Compared with R-GCN (Kelkar et al., 2020), our implemented R-GCN based on SemQL grammar gets higher performance.",577380,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"As shown in Table 2, ShadowGNN can get the best performance at three hardness levels.",Ablation Studies,"Compared with R-GCN+RAT model, ShadowGNN still gets the better performance, where the initial input information is absolutely the same.",Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 49/50
==========================================================================================
2. Ablation Studies -- 12/13
------------------------------------------------------------------------------------------
Compared with R-GCN+RAT model, ShadowGNN still gets the better performance, where the initial input information is absolutely the same.",577381,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Compared with R-GCN (Kelkar et al., 2020), our implemented R-GCN based on SemQL grammar gets higher performance.",Ablation Studies,It denotes that it is necessary and effective to abstract the representation of question and schema explicitly.,Ablation Studies,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
5. Dev. Test -- 50/50
==========================================================================================
2. Ablation Studies -- 13/13
------------------------------------------------------------------------------------------
It denotes that it is necessary and effective to abstract the representation of question and schema explicitly.",577382,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Compared with R-GCN+RAT model, ShadowGNN still gets the better performance, where the initial input information is absolutely the same.",Ablation Studies,Text-to-SQL Recent models evaluated on Spider have pointed out several interesting directions for Text-to-SQL research.,Related Work,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 1/8
==========================================================================================
In this paper, we propose a graph project neural network (GPNN) to abstract the representation of question and schema with simple attention way.",577383,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,Our proposed GPNN aims to use the schema semantics as the bridge to get abstract representation of the question and schema.,Related Work,We further unify the abstract representation of question and schema outputted from GPNN with relativeaware transformer (RAT).,Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 2/8
==========================================================================================
We further unify the abstract representation of question and schema outputted from GPNN with relativeaware transformer (RAT).",577384,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"In this paper, we propose a graph project neural network (GPNN) to abstract the representation of question and schema with simple attention way.",Conclusion,The experiments demonstrate that our proposed ShadowGNN can get excellent performance on the challenging Text-to-SQL task.,Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 3/8
==========================================================================================
The experiments demonstrate that our proposed ShadowGNN can get excellent performance on the challenging Text-to-SQL task.",577385,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,We further unify the abstract representation of question and schema outputted from GPNN with relativeaware transformer (RAT).,Conclusion,"Especially when the annotated training data is limited, our proposed ShadowGNN gets more performance gain on exact match accuracy and convergence speed.",Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 4/8
==========================================================================================
Especially when the annotated training data is limited, our proposed ShadowGNN gets more performance gain on exact match accuracy and convergence speed.",577386,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The experiments demonstrate that our proposed ShadowGNN can get excellent performance on the challenging Text-to-SQL task.,Conclusion,The ablation studies further indicate the effectiveness of our proposed GPNN.,Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 5/8
==========================================================================================
The ablation studies further indicate the effectiveness of our proposed GPNN.",577387,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Especially when the annotated training data is limited, our proposed ShadowGNN gets more performance gain on exact match accuracy and convergence speed.",Conclusion,"Recently, we notice that some Text2SQL-specific pretrained models have been proposed, e.g., TaBERT (Yin et al., 2020) and",Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 6/8
==========================================================================================
Recently, we notice that some Text2SQL-specific pretrained models have been proposed, e.g., TaBERT (Yin et al., 2020) and",577388,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,The ablation studies further indicate the effectiveness of our proposed GPNN.,Conclusion,"GraPPa (Yu et al., 2020).",Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 7/8
==========================================================================================
GraPPa (Yu et al., 2020).",577389,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"Recently, we notice that some Text2SQL-specific pretrained models have been proposed, e.g., TaBERT (Yin et al., 2020) and",Conclusion,"In future work, we will evaluate our proposed ShadowGNN with these adaptive pretrained models.",Conclusion,
"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser
==========================================================================================
7. Conclusion -- 8/8
==========================================================================================
In future work, we will evaluate our proposed ShadowGNN with these adaptive pretrained models.",577390,{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser,"0. abstract
1. Introduction
2. Background
3. Method
4. Experiments
5. Dev. Test
6. Related Work
7. Conclusion
",2021,"GraPPa (Yu et al., 2020).",Conclusion,,,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 1/12
==========================================================================================
This study examines the influence of lexical tone on voice onset time (VOT) in Mandarin and Hakka spoken in Taiwan.",1097158,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,,,"The examination of VOT values for Mandarin and Hakka word-initial stops /p, t, k, p h , t h , k h / followed by three vowels /i, u, a/ in different lexical tones revealed that lexical tone has significant influence on the VOT values for stops.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 2/12
==========================================================================================
The examination of VOT values for Mandarin and Hakka word-initial stops /p, t, k, p h , t h , k h / followed by three vowels /i, u, a/ in different lexical tones revealed that lexical tone has significant influence on the VOT values for stops.",1097159,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,This study examines the influence of lexical tone on voice onset time (VOT) in Mandarin and Hakka spoken in Taiwan.,abstract,The results are important as they suggest that future studies should take the influence of lexical tone into account when studying VOT values and when designing wordlists for stops in tonal languages.,abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 3/12
==========================================================================================
The results are important as they suggest that future studies should take the influence of lexical tone into account when studying VOT values and when designing wordlists for stops in tonal languages.",1097160,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The examination of VOT values for Mandarin and Hakka word-initial stops /p, t, k, p h , t h , k h / followed by three vowels /i, u, a/ in different lexical tones revealed that lexical tone has significant influence on the VOT values for stops.",abstract,"In Mandarin, stops' VOT values, from the longest to the shortest, are in MR, FR, HL, and HF tones.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 4/12
==========================================================================================
In Mandarin, stops' VOT values, from the longest to the shortest, are in MR, FR, HL, and HF tones.",1097161,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The results are important as they suggest that future studies should take the influence of lexical tone into account when studying VOT values and when designing wordlists for stops in tonal languages.,abstract,"This sequence is the same as in Liu, Ng, Wan, Wang, and Zhang (2008).",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 5/12
==========================================================================================
This sequence is the same as in Liu, Ng, Wan, Wang, and Zhang (2008).",1097162,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In Mandarin, stops' VOT values, from the longest to the shortest, are in MR, FR, HL, and HF tones.",abstract,"Later, however, it was found that it is very likely that the sequence results from the existence of non-words.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 6/12
==========================================================================================
Later, however, it was found that it is very likely that the sequence results from the existence of non-words.",1097163,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"This sequence is the same as in Liu, Ng, Wan, Wang, and Zhang (2008).",abstract,"In order to produce non-words correctly, participants tended to pronounce them at a slower speed, especially those in MR tone.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 7/12
==========================================================================================
In order to produce non-words correctly, participants tended to pronounce them at a slower speed, especially those in MR tone.",1097164,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Later, however, it was found that it is very likely that the sequence results from the existence of non-words.",abstract,"Therefore, we further examined the data without non-words, in which no clear sequence was found.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 8/12
==========================================================================================
Therefore, we further examined the data without non-words, in which no clear sequence was found.",1097165,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In order to produce non-words correctly, participants tended to pronounce them at a slower speed, especially those in MR tone.",abstract,"For Hakka, post-hoc tests (Scheffe) show that aspirated stops in entering tones, which are syllables ending with a stop, have significantly shorter VOT values than they have in other tones.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 9/12
==========================================================================================
For Hakka, post-hoc tests (Scheffe) show that aspirated stops in entering tones, which are syllables ending with a stop, have significantly shorter VOT values than they have in other tones.",1097166,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, we further examined the data without non-words, in which no clear sequence was found.",abstract,"Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 10/12
==========================================================================================
Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",1097167,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For Hakka, post-hoc tests (Scheffe) show that aspirated stops in entering tones, which are syllables ending with a stop, have significantly shorter VOT values than they have in other tones.",abstract,"Tonal effect, thus, should be taken into consideration in designing word lists for VOT studies.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 11/12
==========================================================================================
Tonal effect, thus, should be taken into consideration in designing word lists for VOT studies.",1097168,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",abstract,"Moreover, further studies should include both real words and non-words in separate sets of word lists to verify the current study results.",abstract,
"Tonal Effects on Voice Onset Time
==========================================================================================
0. abstract -- 12/12
==========================================================================================
Moreover, further studies should include both real words and non-words in separate sets of word lists to verify the current study results.",1097169,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Tonal effect, thus, should be taken into consideration in designing word lists for VOT studies.",abstract,The aim of this paper is to explore whether lexical tones influence the VOT values for word-initial stops.,Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 1/8
==========================================================================================
The aim of this paper is to explore whether lexical tones influence the VOT values for word-initial stops.",1097170,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Moreover, further studies should include both real words and non-words in separate sets of word lists to verify the current study results.",abstract,"This issue is important because VOT is considered one of the reliable acoustic features for differentiating consonant stops (Cho & Ladefoged, 1999;Gósy, 2001;Lisker & Abramson, 1964;Riney, Takagi, Ota, & Uchida, 2007;Rochet & Fei, 1991;Zheng & Li, 2005) and it has been applied recently to the study of the language production of patients with language deficits or disorders (Auzou, Ozsancak, Morris, Jan, Eustache, & Hannequin, 2000;Jäncke, 1994).",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 2/8
==========================================================================================
This issue is important because VOT is considered one of the reliable acoustic features for differentiating consonant stops (Cho & Ladefoged, 1999;Gósy, 2001;Lisker & Abramson, 1964;Riney, Takagi, Ota, & Uchida, 2007;Rochet & Fei, 1991;Zheng & Li, 2005) and it has been applied recently to the study of the language production of patients with language deficits or disorders (Auzou, Ozsancak, Morris, Jan, Eustache, & Hannequin, 2000;Jäncke, 1994).",1097171,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The aim of this paper is to explore whether lexical tones influence the VOT values for word-initial stops.,Introduction,"Among the languages being investigated, some are tonal languages, i.e. Mandarin, Cantonese, and Taiwanese.",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 3/8
==========================================================================================
Among the languages being investigated, some are tonal languages, i.e. Mandarin, Cantonese, and Taiwanese.",1097172,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"This issue is important because VOT is considered one of the reliable acoustic features for differentiating consonant stops (Cho & Ladefoged, 1999;Gósy, 2001;Lisker & Abramson, 1964;Riney, Takagi, Ota, & Uchida, 2007;Rochet & Fei, 1991;Zheng & Li, 2005) and it has been applied recently to the study of the language production of patients with language deficits or disorders (Auzou, Ozsancak, Morris, Jan, Eustache, & Hannequin, 2000;Jäncke, 1994).",Introduction,"In a tonal language, the duration of each lexical tone (which can change the meaning of a word) differs slightly.",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 4/8
==========================================================================================
In a tonal language, the duration of each lexical tone (which can change the meaning of a word) differs slightly.",1097173,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Among the languages being investigated, some are tonal languages, i.e. Mandarin, Cantonese, and Taiwanese.",Introduction,"Consequently, it is possible that lexical tone will affect stops' VOT to some extent; nevertheless, few studies have taken this factor into consideration when studying tonal languages.",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 5/8
==========================================================================================
Consequently, it is possible that lexical tone will affect stops' VOT to some extent; nevertheless, few studies have taken this factor into consideration when studying tonal languages.",1097174,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In a tonal language, the duration of each lexical tone (which can change the meaning of a word) differs slightly.",Introduction,"Therefore, the current study examines two tonal languages, Mandarin and Hakka spoken in Taiwan, to verify the effects of lexical tone.",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 6/8
==========================================================================================
Therefore, the current study examines two tonal languages, Mandarin and Hakka spoken in Taiwan, to verify the effects of lexical tone.",1097175,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Consequently, it is possible that lexical tone will affect stops' VOT to some extent; nevertheless, few studies have taken this factor into consideration when studying tonal languages.",Introduction,It is hoped that the results of the current study can establish the groundwork for future studies related to VOTs in tonal languages.,Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 7/8
==========================================================================================
It is hoped that the results of the current study can establish the groundwork for future studies related to VOTs in tonal languages.",1097176,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, the current study examines two tonal languages, Mandarin and Hakka spoken in Taiwan, to verify the effects of lexical tone.",Introduction,"If lexical tone does influence VOT, it should be considered when creating speech materials in future studies for tonal languages.",Introduction,
"Tonal Effects on Voice Onset Time
==========================================================================================
1. Introduction -- 8/8
==========================================================================================
If lexical tone does influence VOT, it should be considered when creating speech materials in future studies for tonal languages.",1097177,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,It is hoped that the results of the current study can establish the groundwork for future studies related to VOTs in tonal languages.,Introduction,2.1 Voice Onset Time (VOT) Lisker and Abramson (1964) defined VOT as the temporal interval from the release of an initial stop to the onset of glottal pulsing for a following vowel.,Literature Review,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 1/36
==========================================================================================
This study examined word-initial unaspirated stops /p, t, k/, and aspirated stops /p h , t h , k h /, in combination with three corner vowels /i, u, a/ in Mandarin and Hakka spoken in Taiwan. ",1097178,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"LR Tone → LL Tone / {LR Tone, HL Tone, HE Tone}",Tones in Mandarin and Hakka Spoken in Taiwan,"Except for participants and speech materials, the methodology employed for both languages was the same.",Methodology,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 2/36
==========================================================================================
Except for participants and speech materials, the methodology employed for both languages was the same.",1097179,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"This study examined word-initial unaspirated stops /p, t, k/, and aspirated stops /p h , t h , k h /, in combination with three corner vowels /i, u, a/ in Mandarin and Hakka spoken in Taiwan. ",Methodology,"In this study, the Mandarin and Hakka participants were different.",Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 3/36
==========================================================================================
0. Participants -- 1/8
------------------------------------------------------------------------------------------
In this study, the Mandarin and Hakka participants were different.",1097180,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Except for participants and speech materials, the methodology employed for both languages was the same.",Methodology,The Mandarin participants included 15 male and 15 female Mandarin speakers from Tainan City with an age range from 23 to 33 years (mean = 27.2 years).,Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 4/36
==========================================================================================
0. Participants -- 2/8
------------------------------------------------------------------------------------------
The Mandarin participants included 15 male and 15 female Mandarin speakers from Tainan City with an age range from 23 to 33 years (mean = 27.2 years).",1097181,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In this study, the Mandarin and Hakka participants were different.",Participants,All participants had grown up in Taiwan and had no hearing or speech defects.,Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 5/36
==========================================================================================
0. Participants -- 3/8
------------------------------------------------------------------------------------------
All participants had grown up in Taiwan and had no hearing or speech defects.",1097182,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The Mandarin participants included 15 male and 15 female Mandarin speakers from Tainan City with an age range from 23 to 33 years (mean = 27.2 years).,Participants,"For Hakka, Sixian Hakka was chosen because it is the most extensively used Hakka dialect in Taiwan.",Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 6/36
==========================================================================================
0. Participants -- 4/8
------------------------------------------------------------------------------------------
For Hakka, Sixian Hakka was chosen because it is the most extensively used Hakka dialect in Taiwan.",1097183,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,All participants had grown up in Taiwan and had no hearing or speech defects.,Participants,"The average age of the 21 participants -11 men and 10 women -was 51 years, with the oldest being 80 and the youngest being 36.",Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 7/36
==========================================================================================
0. Participants -- 5/8
------------------------------------------------------------------------------------------
The average age of the 21 participants -11 men and 10 women -was 51 years, with the oldest being 80 and the youngest being 36.",1097184,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For Hakka, Sixian Hakka was chosen because it is the most extensively used Hakka dialect in Taiwan.",Participants,All of the participants for Hakka were also fluent Mandarin speakers as Mandarin is the official language in Taiwan.,Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 8/36
==========================================================================================
0. Participants -- 6/8
------------------------------------------------------------------------------------------
All of the participants for Hakka were also fluent Mandarin speakers as Mandarin is the official language in Taiwan.",1097185,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The average age of the 21 participants -11 men and 10 women -was 51 years, with the oldest being 80 and the youngest being 36.",Participants,"In the current study, the age range of Mandarin participants is controlled to within 10 years to avoid the effect of age difference.",Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 9/36
==========================================================================================
0. Participants -- 7/8
------------------------------------------------------------------------------------------
In the current study, the age range of Mandarin participants is controlled to within 10 years to avoid the effect of age difference.",1097186,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,All of the participants for Hakka were also fluent Mandarin speakers as Mandarin is the official language in Taiwan.,Participants,"As for Hakka participants, the age-range was quite wide because it is not easy to find fluent Hakka speakers.",Participants,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 10/36
==========================================================================================
0. Participants -- 8/8
------------------------------------------------------------------------------------------
As for Hakka participants, the age-range was quite wide because it is not easy to find fluent Hakka speakers.",1097187,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In the current study, the age range of Mandarin participants is controlled to within 10 years to avoid the effect of age difference.",Participants,"The speech materials in both languages were combinations of six stops /p, t, k, p h , t h , k h / and three vowels /i, u, a/, resulting in 18 combinations.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 11/36
==========================================================================================
1. Data Collection -- 1/17
------------------------------------------------------------------------------------------
The speech materials in both languages were combinations of six stops /p, t, k, p h , t h , k h / and three vowels /i, u, a/, resulting in 18 combinations.",1097188,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"As for Hakka participants, the age-range was quite wide because it is not easy to find fluent Hakka speakers.",Participants,"Mandarin's 4 contrasting lexical tones meant that a total of 72 monosyllabic words were created; among them, 18 combinations do not have corresponding Chinese characters in Mandarin.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 12/36
==========================================================================================
1. Data Collection -- 2/17
------------------------------------------------------------------------------------------
Mandarin's 4 contrasting lexical tones meant that a total of 72 monosyllabic words were created; among them, 18 combinations do not have corresponding Chinese characters in Mandarin.",1097189,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The speech materials in both languages were combinations of six stops /p, t, k, p h , t h , k h / and three vowels /i, u, a/, resulting in 18 combinations.",Data Collection,"The 6 contrasting lexical tones in Sixian Hakka resulted in 108 monosyllabic words, 12 of which do not have corresponding Chinese characters.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 13/36
==========================================================================================
1. Data Collection -- 3/17
------------------------------------------------------------------------------------------
The 6 contrasting lexical tones in Sixian Hakka resulted in 108 monosyllabic words, 12 of which do not have corresponding Chinese characters.",1097190,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Mandarin's 4 contrasting lexical tones meant that a total of 72 monosyllabic words were created; among them, 18 combinations do not have corresponding Chinese characters in Mandarin.",Data Collection,Chen et al. (2007) claimed that disyllabic words can create a more natural-like context for participants.,Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 14/36
==========================================================================================
1. Data Collection -- 4/17
------------------------------------------------------------------------------------------
Chen et al. (2007) claimed that disyllabic words can create a more natural-like context for participants.",1097191,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The 6 contrasting lexical tones in Sixian Hakka resulted in 108 monosyllabic words, 12 of which do not have corresponding Chinese characters.",Data Collection,"Therefore, in order to make speakers produce the words more naturally, all of the words were followed by another word in order to create meaningful disyllables, including non-words.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 15/36
==========================================================================================
1. Data Collection -- 5/17
------------------------------------------------------------------------------------------
Therefore, in order to make speakers produce the words more naturally, all of the words were followed by another word in order to create meaningful disyllables, including non-words.",1097192,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Chen et al. (2007) claimed that disyllabic words can create a more natural-like context for participants.,Data Collection,"For example, the Mandarin word /pi/ was followed by another word /p h uo/ to become the existing disyllable /pi p h uo/ ""force.""",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 16/36
==========================================================================================
1. Data Collection -- 6/17
------------------------------------------------------------------------------------------
For example, the Mandarin word /pi/ was followed by another word /p h uo/ to become the existing disyllable /pi p h uo/ ""force.""",1097193,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, in order to make speakers produce the words more naturally, all of the words were followed by another word in order to create meaningful disyllables, including non-words.",Data Collection,Even non-words were arranged in disyllabic forms to give them a more natural-like quality.,Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 17/36
==========================================================================================
1. Data Collection -- 7/17
------------------------------------------------------------------------------------------
Even non-words were arranged in disyllabic forms to give them a more natural-like quality.",1097194,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For example, the Mandarin word /pi/ was followed by another word /p h uo/ to become the existing disyllable /pi p h uo/ ""force.""",Data Collection,"Since the neutral tone in Mandarin never occurs in phrase-initial position, it was not evaluated in this study.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 18/36
==========================================================================================
1. Data Collection -- 8/17
------------------------------------------------------------------------------------------
Since the neutral tone in Mandarin never occurs in phrase-initial position, it was not evaluated in this study.",1097195,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Even non-words were arranged in disyllabic forms to give them a more natural-like quality.,Data Collection,"The structure of non-words was the same as real words, which is a CV syllable with one consonant (stop) and one vowel (corner vowel).",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 19/36
==========================================================================================
1. Data Collection -- 9/17
------------------------------------------------------------------------------------------
The structure of non-words was the same as real words, which is a CV syllable with one consonant (stop) and one vowel (corner vowel).",1097196,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Since the neutral tone in Mandarin never occurs in phrase-initial position, it was not evaluated in this study.",Data Collection,"For example, there is no /k h a/ in MR tone in Mandarin or /pu/ in MF tone in Hakka, so non-words were created for these combinations.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 20/36
==========================================================================================
1. Data Collection -- 10/17
------------------------------------------------------------------------------------------
For example, there is no /k h a/ in MR tone in Mandarin or /pu/ in MF tone in Hakka, so non-words were created for these combinations.",1097197,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The structure of non-words was the same as real words, which is a CV syllable with one consonant (stop) and one vowel (corner vowel).",Data Collection,The way we measure VOTs of non-words is the same as with the real words.,Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 21/36
==========================================================================================
1. Data Collection -- 11/17
------------------------------------------------------------------------------------------
The way we measure VOTs of non-words is the same as with the real words.",1097198,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For example, there is no /k h a/ in MR tone in Mandarin or /pu/ in MF tone in Hakka, so non-words were created for these combinations.",Data Collection,"For example, /k h a/ is measured from target consonant /k",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 22/36
==========================================================================================
1. Data Collection -- 12/17
------------------------------------------------------------------------------------------
For example, /k h a/ is measured from target consonant /k",1097199,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The way we measure VOTs of non-words is the same as with the real words.,Data Collection,The corpus was arranged randomly.,Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 23/36
==========================================================================================
1. Data Collection -- 13/17
------------------------------------------------------------------------------------------
The corpus was arranged randomly.",1097200,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For example, /k h a/ is measured from target consonant /k",Data Collection,Participants were asked to read the words out loud in a normal voice and at a comfortable rate.,Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 24/36
==========================================================================================
1. Data Collection -- 14/17
------------------------------------------------------------------------------------------
Participants were asked to read the words out loud in a normal voice and at a comfortable rate.",1097201,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The corpus was arranged randomly.,Data Collection,"After finishing, the participants were asked to read the words a second time.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 25/36
==========================================================================================
1. Data Collection -- 15/17
------------------------------------------------------------------------------------------
After finishing, the participants were asked to read the words a second time.",1097202,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Participants were asked to read the words out loud in a normal voice and at a comfortable rate.,Data Collection,"Therefore, two groups of data were gathered for each participant.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 26/36
==========================================================================================
1. Data Collection -- 16/17
------------------------------------------------------------------------------------------
Therefore, two groups of data were gathered for each participant.",1097203,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"After finishing, the participants were asked to read the words a second time.",Data Collection,"All speech was recorded using a 24 bit WAVE/MPS recorder, connected to AKG C520 Head-Worn Condenser Microphone positioned approximately 10 to 15 centimeters from the participant's mouth in a quiet room.",Data Collection,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 27/36
==========================================================================================
1. Data Collection -- 17/17
------------------------------------------------------------------------------------------
All speech was recorded using a 24 bit WAVE/MPS recorder, connected to AKG C520 Head-Worn Condenser Microphone positioned approximately 10 to 15 centimeters from the participant's mouth in a quiet room.",1097204,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, two groups of data were gathered for each participant.",Data Collection,"After recording, data were edited into individual files and analyzed using the Praat software.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 28/36
==========================================================================================
2. Data Measurement and Analysis -- 1/9
------------------------------------------------------------------------------------------
After recording, data were edited into individual files and analyzed using the Praat software.",1097205,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"All speech was recorded using a 24 bit WAVE/MPS recorder, connected to AKG C520 Head-Worn Condenser Microphone positioned approximately 10 to 15 centimeters from the participant's mouth in a quiet room.",Data Collection,"VOT, measured in milliseconds (ms), was obtained by measuring the temporal interval between the beginning of the release burst and the onset of the following vowel, as shown in Figure 2.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 29/36
==========================================================================================
2. Data Measurement and Analysis -- 2/9
------------------------------------------------------------------------------------------
VOT, measured in milliseconds (ms), was obtained by measuring the temporal interval between the beginning of the release burst and the onset of the following vowel, as shown in Figure 2.",1097206,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"After recording, data were edited into individual files and analyzed using the Praat software.",Data Measurement and Analysis,"The values of both the waveform and spectrogram were recorded, but the VOTs were determined primarily through waveform analysis, with the values in the spectrogram being provided as references.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 30/36
==========================================================================================
2. Data Measurement and Analysis -- 3/9
------------------------------------------------------------------------------------------
The values of both the waveform and spectrogram were recorded, but the VOTs were determined primarily through waveform analysis, with the values in the spectrogram being provided as references.",1097207,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"VOT, measured in milliseconds (ms), was obtained by measuring the temporal interval between the beginning of the release burst and the onset of the following vowel, as shown in Figure 2.",Data Measurement and Analysis,"If the values in waveform differed from the values in the spectrogram by more than five milliseconds, the data were re-measured to verify accuracy.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 31/36
==========================================================================================
2. Data Measurement and Analysis -- 4/9
------------------------------------------------------------------------------------------
If the values in waveform differed from the values in the spectrogram by more than five milliseconds, the data were re-measured to verify accuracy.",1097208,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The values of both the waveform and spectrogram were recorded, but the VOTs were determined primarily through waveform analysis, with the values in the spectrogram being provided as references.",Data Measurement and Analysis,"When analyzing the data, VOT values for mispronounced words were omitted.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 32/36
==========================================================================================
2. Data Measurement and Analysis -- 5/9
------------------------------------------------------------------------------------------
When analyzing the data, VOT values for mispronounced words were omitted.",1097209,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"If the values in waveform differed from the values in the spectrogram by more than five milliseconds, the data were re-measured to verify accuracy.",Data Measurement and Analysis,"Moreover, data for Hakka /pi/ in HE Tone were not analyzed due to incorrect word choices.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 33/36
==========================================================================================
2. Data Measurement and Analysis -- 6/9
------------------------------------------------------------------------------------------
Moreover, data for Hakka /pi/ in HE Tone were not analyzed due to incorrect word choices.",1097210,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"When analyzing the data, VOT values for mispronounced words were omitted.",Data Measurement and Analysis,"A four-way mixed factorial ANOVA (Montgomery, 2009; the same test was used in Francis, Ciocca, & Yu, 2003) (place of articulation by vowel context by lexical tone by gender) was used to examine whether the variables significantly influenced each stop's VOT.",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 34/36
==========================================================================================
2. Data Measurement and Analysis -- 7/9
------------------------------------------------------------------------------------------
A four-way mixed factorial ANOVA (Montgomery, 2009; the same test was used in Francis, Ciocca, & Yu, 2003) (place of articulation by vowel context by lexical tone by gender) was used to examine whether the variables significantly influenced each stop's VOT.",1097211,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Moreover, data for Hakka /pi/ in HE Tone were not analyzed due to incorrect word choices.",Data Measurement and Analysis,"In addition, differences between the examined targets were analyzed using T-test or post-hoc tests (Scheffe) (Gravetter & Wallnau, 2008); results were considered significant when the p value was less than 0.05. ",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 35/36
==========================================================================================
2. Data Measurement and Analysis -- 8/9
------------------------------------------------------------------------------------------
In addition, differences between the examined targets were analyzed using T-test or post-hoc tests (Scheffe) (Gravetter & Wallnau, 2008); results were considered significant when the p value was less than 0.05. ",1097212,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"A four-way mixed factorial ANOVA (Montgomery, 2009; the same test was used in Francis, Ciocca, & Yu, 2003) (place of articulation by vowel context by lexical tone by gender) was used to examine whether the variables significantly influenced each stop's VOT.",Data Measurement and Analysis,"Four-way mixed factorial ANOVA can be illustrated in the following formula (Montgomery, 2009)",Data Measurement and Analysis,
"Tonal Effects on Voice Onset Time
==========================================================================================
3. Methodology -- 36/36
==========================================================================================
2. Data Measurement and Analysis -- 9/9
------------------------------------------------------------------------------------------
Four-way mixed factorial ANOVA can be illustrated in the following formula (Montgomery, 2009)",1097213,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In addition, differences between the examined targets were analyzed using T-test or post-hoc tests (Scheffe) (Gravetter & Wallnau, 2008); results were considered significant when the p value was less than 0.05. ",Data Measurement and Analysis,"When examining the VOT values for Mandarin stops, it became apparent that they tend to be longer than the mean VOT values reported in the studies by Liao (2005), Chao et al. (2006), and Chen et al. (2007) and shorter than those reported by Rochet and Fei (1991).",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 1/22
==========================================================================================
When examining the VOT values for Mandarin stops, it became apparent that they tend to be longer than the mean VOT values reported in the studies by Liao (2005), Chao et al. (2006), and Chen et al. (2007) and shorter than those reported by Rochet and Fei (1991).",1097214,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Four-way mixed factorial ANOVA can be illustrated in the following formula (Montgomery, 2009)",Data Measurement and Analysis,"Examining the methodologies in these previous studies indicated that the speech materials in Rochet and Fei's study were monosyllabic, but disyllabic in the remaining four studies.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 2/22
==========================================================================================
Examining the methodologies in these previous studies indicated that the speech materials in Rochet and Fei's study were monosyllabic, but disyllabic in the remaining four studies.",1097215,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"When examining the VOT values for Mandarin stops, it became apparent that they tend to be longer than the mean VOT values reported in the studies by Liao (2005), Chao et al. (2006), and Chen et al. (2007) and shorter than those reported by Rochet and Fei (1991).",Results and Discussion,Gósy (2001) claimed that speakers speak in a careful and disciplined way while uttering syllables and words in isolation.,Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 3/22
==========================================================================================
Gósy (2001) claimed that speakers speak in a careful and disciplined way while uttering syllables and words in isolation.",1097216,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Examining the methodologies in these previous studies indicated that the speech materials in Rochet and Fei's study were monosyllabic, but disyllabic in the remaining four studies.",Results and Discussion,"Therefore, participants are expected to produce monosyllables in a more careful manner as their speech tempo decreases.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 4/22
==========================================================================================
Therefore, participants are expected to produce monosyllables in a more careful manner as their speech tempo decreases.",1097217,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Gósy (2001) claimed that speakers speak in a careful and disciplined way while uttering syllables and words in isolation.,Results and Discussion,"According to Kessinger and Blumstein (1998), VOTs get longer while the speaking rate slows down.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 5/22
==========================================================================================
According to Kessinger and Blumstein (1998), VOTs get longer while the speaking rate slows down.",1097218,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, participants are expected to produce monosyllables in a more careful manner as their speech tempo decreases.",Results and Discussion,This may explain why the mean VOTs for Mandarin stops in Rochet and Fei's study were longer than in other reports.,Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 6/22
==========================================================================================
This may explain why the mean VOTs for Mandarin stops in Rochet and Fei's study were longer than in other reports.",1097219,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"According to Kessinger and Blumstein (1998), VOTs get longer while the speaking rate slows down.",Results and Discussion,Another possible explanation might be regional differences in the target language.,Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 7/22
==========================================================================================
Another possible explanation might be regional differences in the target language.",1097220,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,This may explain why the mean VOTs for Mandarin stops in Rochet and Fei's study were longer than in other reports.,Results and Discussion,"Although the target language is the same, participants in Rochet and Fei's study grew up in Mainland China, while participants in the other studies were raised in Taiwan.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 8/22
==========================================================================================
Although the target language is the same, participants in Rochet and Fei's study grew up in Mainland China, while participants in the other studies were raised in Taiwan.",1097221,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Another possible explanation might be regional differences in the target language.,Results and Discussion,"Consequently, regional differences might be another origin of the variations.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 9/22
==========================================================================================
Consequently, regional differences might be another origin of the variations.",1097222,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Although the target language is the same, participants in Rochet and Fei's study grew up in Mainland China, while participants in the other studies were raised in Taiwan.",Results and Discussion,"As a result, comparing the results of Liao (2005), Chao et al. (2006), and Chen et al. (2007) to those of the present study indicates that the mean VOT values for the stops in the present study tend to be longer than their counterparts in the other studies.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 10/22
==========================================================================================
As a result, comparing the results of Liao (2005), Chao et al. (2006), and Chen et al. (2007) to those of the present study indicates that the mean VOT values for the stops in the present study tend to be longer than their counterparts in the other studies.",1097223,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Consequently, regional differences might be another origin of the variations.",Results and Discussion,Such differences stem primarily from the existence of non-words in the current study.,Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 11/22
==========================================================================================
Such differences stem primarily from the existence of non-words in the current study.",1097224,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"As a result, comparing the results of Liao (2005), Chao et al. (2006), and Chen et al. (2007) to those of the present study indicates that the mean VOT values for the stops in the present study tend to be longer than their counterparts in the other studies.",Results and Discussion,"During the recording process, speakers tended to produce non-words more carefully and at a lower speed because they were not familiar with the non-words.",Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 12/22
==========================================================================================
During the recording process, speakers tended to produce non-words more carefully and at a lower speed because they were not familiar with the non-words.",1097225,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Such differences stem primarily from the existence of non-words in the current study.,Results and Discussion,An examination of the data both with and without non-words separately demonstrated that the existence of non-words resulted in stops having longer mean VOTs.,Results and Discussion,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 13/22
==========================================================================================
An examination of the data both with and without non-words separately demonstrated that the existence of non-words resulted in stops having longer mean VOTs.",1097226,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"During the recording process, speakers tended to produce non-words more carefully and at a lower speed because they were not familiar with the non-words.",Results and Discussion,The statistical analyses were conducted using a four-way mixed factorial ANOVA.,Mandarin Chinese,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 14/22
==========================================================================================
0. Mandarin Chinese -- 1/5
------------------------------------------------------------------------------------------
The statistical analyses were conducted using a four-way mixed factorial ANOVA.",1097227,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,An examination of the data both with and without non-words separately demonstrated that the existence of non-words resulted in stops having longer mean VOTs.,Results and Discussion,"For Mandarin unaspirated stops, the results showed a primary effect of place of articulation, F(2, 972) = 522.9680; vowel context, F(2, 972) = 117.3569; lexical tone, F(3, 972) = 6.5506; and gender F(1, 972) = 56.9180 (all p < .001).",Mandarin Chinese,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 15/22
==========================================================================================
0. Mandarin Chinese -- 2/5
------------------------------------------------------------------------------------------
For Mandarin unaspirated stops, the results showed a primary effect of place of articulation, F(2, 972) = 522.9680; vowel context, F(2, 972) = 117.3569; lexical tone, F(3, 972) = 6.5506; and gender F(1, 972) = 56.9180 (all p < .001).",1097228,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The statistical analyses were conducted using a four-way mixed factorial ANOVA.,Mandarin Chinese,"These results indicate that the stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do create significant differences in VOT values of word-initial unaspirated stops.",Mandarin Chinese,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 16/22
==========================================================================================
0. Mandarin Chinese -- 3/5
------------------------------------------------------------------------------------------
These results indicate that the stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do create significant differences in VOT values of word-initial unaspirated stops.",1097229,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For Mandarin unaspirated stops, the results showed a primary effect of place of articulation, F(2, 972) = 522.9680; vowel context, F(2, 972) = 117.3569; lexical tone, F(3, 972) = 6.5506; and gender F(1, 972) = 56.9180 (all p < .001).",Mandarin Chinese,"Furthermore, significant two-way interactions were also observed (Figures 3-4) between place of articulation and vowel context and between vowel context and gender.",Mandarin Chinese,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 17/22
==========================================================================================
0. Mandarin Chinese -- 4/5
------------------------------------------------------------------------------------------
Furthermore, significant two-way interactions were also observed (Figures 3-4) between place of articulation and vowel context and between vowel context and gender.",1097230,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"These results indicate that the stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do create significant differences in VOT values of word-initial unaspirated stops.",Mandarin Chinese,A complete ANOVA table showing interaction between variables is listed in Appendix 1.,Mandarin Chinese,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 18/22
==========================================================================================
0. Mandarin Chinese -- 5/5
------------------------------------------------------------------------------------------
A complete ANOVA table showing interaction between variables is listed in Appendix 1.",1097231,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Furthermore, significant two-way interactions were also observed (Figures 3-4) between place of articulation and vowel context and between vowel context and gender.",Mandarin Chinese,"For Hakka unaspirated stops, the results showed a main effect of place of articulation, F(2, 843) = 404.3395; vowel context, F(2, 843) = 69.8958; lexical tone, F(5, 843) = 6.3054; and gender F(1, 843) = 34.2724 (all p < .001).",Lexical Tone and VOT in Hakka,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 19/22
==========================================================================================
1. Lexical Tone and VOT in Hakka -- 1/4
------------------------------------------------------------------------------------------
For Hakka unaspirated stops, the results showed a main effect of place of articulation, F(2, 843) = 404.3395; vowel context, F(2, 843) = 69.8958; lexical tone, F(5, 843) = 6.3054; and gender F(1, 843) = 34.2724 (all p < .001).",1097232,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,A complete ANOVA table showing interaction between variables is listed in Appendix 1.,Mandarin Chinese,"Similar to the findings in Mandarin stops, these results indicate that stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do make significant differences in VOT values of word-initial unaspirated stops.",Lexical Tone and VOT in Hakka,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 20/22
==========================================================================================
1. Lexical Tone and VOT in Hakka -- 2/4
------------------------------------------------------------------------------------------
Similar to the findings in Mandarin stops, these results indicate that stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do make significant differences in VOT values of word-initial unaspirated stops.",1097233,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"For Hakka unaspirated stops, the results showed a main effect of place of articulation, F(2, 843) = 404.3395; vowel context, F(2, 843) = 69.8958; lexical tone, F(5, 843) = 6.3054; and gender F(1, 843) = 34.2724 (all p < .001).",Lexical Tone and VOT in Hakka,"Significant two-way interactions occurred between the place of articulation and lexical tone (Figure 8), and significant three-way interactions occurred among place of articulation, vowel context, and lexical tone.",Lexical Tone and VOT in Hakka,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 21/22
==========================================================================================
1. Lexical Tone and VOT in Hakka -- 3/4
------------------------------------------------------------------------------------------
Significant two-way interactions occurred between the place of articulation and lexical tone (Figure 8), and significant three-way interactions occurred among place of articulation, vowel context, and lexical tone.",1097234,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Similar to the findings in Mandarin stops, these results indicate that stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do make significant differences in VOT values of word-initial unaspirated stops.",Lexical Tone and VOT in Hakka,A complete ANOVA table showing interaction between variables is listed in Appendix 3.,Lexical Tone and VOT in Hakka,
"Tonal Effects on Voice Onset Time
==========================================================================================
4. Results and Discussion -- 22/22
==========================================================================================
1. Lexical Tone and VOT in Hakka -- 4/4
------------------------------------------------------------------------------------------
A complete ANOVA table showing interaction between variables is listed in Appendix 3.",1097235,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Significant two-way interactions occurred between the place of articulation and lexical tone (Figure 8), and significant three-way interactions occurred among place of articulation, vowel context, and lexical tone.",Lexical Tone and VOT in Hakka,"The study results revealed that lexical tones significantly influence VOT values for stops in Mandarin (with both real words and non-words), and significant tonal effect was also found in Hakka data of real words.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 1/14
==========================================================================================
The study results revealed that lexical tones significantly influence VOT values for stops in Mandarin (with both real words and non-words), and significant tonal effect was also found in Hakka data of real words.",1097236,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,A complete ANOVA table showing interaction between variables is listed in Appendix 3.,Lexical Tone and VOT in Hakka,"Nevertheless, there is no significant tonal effect on VOT in Mandarin data with only real words. ",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 2/14
==========================================================================================
Nevertheless, there is no significant tonal effect on VOT in Mandarin data with only real words. ",1097237,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"The study results revealed that lexical tones significantly influence VOT values for stops in Mandarin (with both real words and non-words), and significant tonal effect was also found in Hakka data of real words.",Conclusion,The study results are important as they suggest that future studies should take the influence of lexical tones into account when studying VOT values and when designing wordlists for stops in tonal languages.,Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 3/14
==========================================================================================
The study results are important as they suggest that future studies should take the influence of lexical tones into account when studying VOT values and when designing wordlists for stops in tonal languages.",1097238,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Nevertheless, there is no significant tonal effect on VOT in Mandarin data with only real words. ",Conclusion,"Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 4/14
==========================================================================================
Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",1097239,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,The study results are important as they suggest that future studies should take the influence of lexical tones into account when studying VOT values and when designing wordlists for stops in tonal languages.,Conclusion,Several factors might contribute to this inconsistency.,Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 5/14
==========================================================================================
Several factors might contribute to this inconsistency.",1097240,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded.",Conclusion,"First, we used different methods in eliciting non-words production in these two languages.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 6/14
==========================================================================================
First, we used different methods in eliciting non-words production in these two languages.",1097241,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,Several factors might contribute to this inconsistency.,Conclusion,"In Mandarin, we used Zhuyin Fuhao to guide non-words productions, which might force participants to take a few seconds to figure out the new combinations of Chinese phonetic symbols.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 7/14
==========================================================================================
In Mandarin, we used Zhuyin Fuhao to guide non-words productions, which might force participants to take a few seconds to figure out the new combinations of Chinese phonetic symbols.",1097242,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"First, we used different methods in eliciting non-words production in these two languages.",Conclusion,"In contrast, we asked participants to read a real word first as a clue in producing a target non-word in Hakka.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 8/14
==========================================================================================
In contrast, we asked participants to read a real word first as a clue in producing a target non-word in Hakka.",1097243,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In Mandarin, we used Zhuyin Fuhao to guide non-words productions, which might force participants to take a few seconds to figure out the new combinations of Chinese phonetic symbols.",Conclusion,"Second, many of the participants in Hakka were at the age range of 50-80, and we found the participants were not flexible enough to comprehend our instructions in producing non-words.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 9/14
==========================================================================================
Second, many of the participants in Hakka were at the age range of 50-80, and we found the participants were not flexible enough to comprehend our instructions in producing non-words.",1097244,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"In contrast, we asked participants to read a real word first as a clue in producing a target non-word in Hakka.",Conclusion,"Therefore, we decided not to take the data of non-words (mostly by guessing with uncertainly) into analysis in order to have a reliable result.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 10/14
==========================================================================================
Therefore, we decided not to take the data of non-words (mostly by guessing with uncertainly) into analysis in order to have a reliable result.",1097245,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Second, many of the participants in Hakka were at the age range of 50-80, and we found the participants were not flexible enough to comprehend our instructions in producing non-words.",Conclusion,"(The discarded cells (non-words) are less than 25% of the total, which fulfills the requirements of ANOVA test for reliable test results (Gravetter & Wallnau, 2008).",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 11/14
==========================================================================================
(The discarded cells (non-words) are less than 25% of the total, which fulfills the requirements of ANOVA test for reliable test results (Gravetter & Wallnau, 2008).",1097246,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Therefore, we decided not to take the data of non-words (mostly by guessing with uncertainly) into analysis in order to have a reliable result.",Conclusion,"Future studies should keep the method of elicitation consistent across languages, recruit participants of similar age range, and include both real words and non-words in separate sets of word lists in order to verify the current study findings.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 12/14
==========================================================================================
Future studies should keep the method of elicitation consistent across languages, recruit participants of similar age range, and include both real words and non-words in separate sets of word lists in order to verify the current study findings.",1097247,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"(The discarded cells (non-words) are less than 25% of the total, which fulfills the requirements of ANOVA test for reliable test results (Gravetter & Wallnau, 2008).",Conclusion,"Moreover, although the results of this study indicate that lexical tones influence VOT of stops to some extent, we cannot exclude the possibility of correlation between VOT and the duration of lexical tones.",Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 13/14
==========================================================================================
Moreover, although the results of this study indicate that lexical tones influence VOT of stops to some extent, we cannot exclude the possibility of correlation between VOT and the duration of lexical tones.",1097248,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Future studies should keep the method of elicitation consistent across languages, recruit participants of similar age range, and include both real words and non-words in separate sets of word lists in order to verify the current study findings.",Conclusion,Further study can explore this possibility by adopting mean durations of each tone in Mandarin as the normalized parameters upon calculating mean VOTs of stops.,Conclusion,
"Tonal Effects on Voice Onset Time
==========================================================================================
5. Conclusion -- 14/14
==========================================================================================
Further study can explore this possibility by adopting mean durations of each tone in Mandarin as the normalized parameters upon calculating mean VOTs of stops.",1097249,Tonal Effects on Voice Onset Time,"0. abstract
1. Introduction
2. Literature Review
3. Methodology
4. Results and Discussion
5. Conclusion
",2009,"Moreover, although the results of this study indicate that lexical tones influence VOT of stops to some extent, we cannot exclude the possibility of correlation between VOT and the duration of lexical tones.",Conclusion,,,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 1/8
==========================================================================================
Recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models.",50161,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,,,This can be partially explained by the lack of large specialized comparable corpora to build efficient representations.,abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 2/8
==========================================================================================
This can be partially explained by the lack of large specialized comparable corpora to build efficient representations.",50162,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models.,abstract,"Within this context, we try to answer the following questions: First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction?",abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 3/8
==========================================================================================
Within this context, we try to answer the following questions: First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction?",50163,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,This can be partially explained by the lack of large specialized comparable corpora to build efficient representations.,abstract,Second (ii) is it worth it to combine multiple embeddings trained on different data sets?,abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 4/8
==========================================================================================
Second (ii) is it worth it to combine multiple embeddings trained on different data sets?",50164,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Within this context, we try to answer the following questions: First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction?",abstract,"For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora.",abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 5/8
==========================================================================================
For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora.",50165,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Second (ii) is it worth it to combine multiple embeddings trained on different data sets?,abstract,We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons.,abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 6/8
==========================================================================================
We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons.",50166,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora.",abstract,"Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets.",abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 7/8
==========================================================================================
Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets.",50167,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons.,abstract,Our approach leads to higher performance than the best individual embedding model.,abstract,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
0. abstract -- 8/8
==========================================================================================
Our approach leads to higher performance than the best individual embedding model.",50168,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets.",abstract,"Bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation (Och and Ney, 2003), cross-language information retrieval (Nie, 2010) or computerassisted translation (Delpech, 2014).",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 1/21
==========================================================================================
Bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation (Och and Ney, 2003), cross-language information retrieval (Nie, 2010) or computerassisted translation (Delpech, 2014).",50169,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Our approach leads to higher performance than the best individual embedding model.,abstract,"Because a manual compilation of bilingual lexicons requires substantial human efforts, bilingual lexicons are automatically extracted from bilingual corpora.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 2/21
==========================================================================================
Because a manual compilation of bilingual lexicons requires substantial human efforts, bilingual lexicons are automatically extracted from bilingual corpora.",50170,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation (Och and Ney, 2003), cross-language information retrieval (Nie, 2010) or computerassisted translation (Delpech, 2014).",Introduction,These corpora can be parallel or comparable data sets.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 3/21
==========================================================================================
These corpora can be parallel or comparable data sets.",50171,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Because a manual compilation of bilingual lexicons requires substantial human efforts, bilingual lexicons are automatically extracted from bilingual corpora.",Introduction,"Despite good results obtained when compiling bilingual lexicons from parallel corpora, the latter are scarce resources, especially for specialized and technical domains and for language pairs not involving English.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 4/21
==========================================================================================
Despite good results obtained when compiling bilingual lexicons from parallel corpora, the latter are scarce resources, especially for specialized and technical domains and for language pairs not involving English.",50172,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,These corpora can be parallel or comparable data sets.,Introduction,"In this context, comparable corpora are an interesting and practical alternative to the use of parallel corpora. Comparable corpora, which gather texts sharing common features such as domain, topic, discourse, etc. without having a parallel source text-target text relationship, allow access to the original vocabulary without falling under the influence of the human translation.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 5/21
==========================================================================================
In this context, comparable corpora are an interesting and practical alternative to the use of parallel corpora. Comparable corpora, which gather texts sharing common features such as domain, topic, discourse, etc. without having a parallel source text-target text relationship, allow access to the original vocabulary without falling under the influence of the human translation.",50173,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Despite good results obtained when compiling bilingual lexicons from parallel corpora, the latter are scarce resources, especially for specialized and technical domains and for language pairs not involving English.",Introduction,"Compiling a large comparable corpus is easier, especially for general language (Talvensaari et al., 2007).",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 6/21
==========================================================================================
Compiling a large comparable corpus is easier, especially for general language (Talvensaari et al., 2007).",50174,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In this context, comparable corpora are an interesting and practical alternative to the use of parallel corpora. Comparable corpora, which gather texts sharing common features such as domain, topic, discourse, etc. without having a parallel source text-target text relationship, allow access to the original vocabulary without falling under the influence of the human translation.",Introduction,"In contrast, specialized comparable corpora are traditionally of modest size due to the difficulty to obtain many specialized documents in a language other than English.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 7/21
==========================================================================================
In contrast, specialized comparable corpora are traditionally of modest size due to the difficulty to obtain many specialized documents in a language other than English.",50175,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Compiling a large comparable corpus is easier, especially for general language (Talvensaari et al., 2007).",Introduction,"Specialized comparable corpora have a size of around one million words whereas general-domain comparable corpora can gather several million words (Morin and Hazem, 2014). ",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 8/21
==========================================================================================
Specialized comparable corpora have a size of around one million words whereas general-domain comparable corpora can gather several million words (Morin and Hazem, 2014). ",50176,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In contrast, specialized comparable corpora are traditionally of modest size due to the difficulty to obtain many specialized documents in a language other than English.",Introduction,One way to overcome the small size of specialized comparable corpora is to associate external resources.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 9/21
==========================================================================================
One way to overcome the small size of specialized comparable corpora is to associate external resources.",50177,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Specialized comparable corpora have a size of around one million words whereas general-domain comparable corpora can gather several million words (Morin and Hazem, 2014). ",Introduction,"These resources may be close specialized corpora (e.g. a breast cancer corpus may benefit from contexts derived from a more general oncology corpus), corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 10/21
==========================================================================================
These resources may be close specialized corpora (e.g. a breast cancer corpus may benefit from contexts derived from a more general oncology corpus), corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data.",50178,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,One way to overcome the small size of specialized comparable corpora is to associate external resources.,Introduction,The main challenge is to know how to associate such resources with a comparable specialized corpus. ,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 11/21
==========================================================================================
The main challenge is to know how to associate such resources with a comparable specialized corpus. ",50179,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"These resources may be close specialized corpora (e.g. a breast cancer corpus may benefit from contexts derived from a more general oncology corpus), corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data.",Introduction,This work is licensed under a Creative Commons Attribution 4.0 International License.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 12/21
==========================================================================================
This work is licensed under a Creative Commons Attribution 4.0 International License.",50180,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The main challenge is to know how to associate such resources with a comparable specialized corpus. ,Introduction,"License details: http://creativecommons.org/licenses/by/4.0/. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 13/21
==========================================================================================
License details: http://creativecommons.org/licenses/by/4.0/. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora.",50181,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,This work is licensed under a Creative Commons Attribution 4.0 International License.,Introduction,This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 14/21
==========================================================================================
This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task.",50182,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"License details: http://creativecommons.org/licenses/by/4.0/. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora.",Introduction,The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 15/21
==========================================================================================
The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models.",50183,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task.,Introduction,We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 16/21
==========================================================================================
We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction.",50184,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models.,Introduction,"However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997;Rapp, 1999).",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 17/21
==========================================================================================
However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997;Rapp, 1999).",50185,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction.,Introduction,Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 18/21
==========================================================================================
Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination.",50186,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997;Rapp, 1999).",Introduction,"More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW.",Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 19/21
==========================================================================================
More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW.",50187,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination.,Introduction,We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 20/21
==========================================================================================
We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model.",50188,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW.",Introduction,Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora.,Introduction,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
1. Introduction -- 21/21
==========================================================================================
Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora.",50189,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model.,Introduction,"According to Hermann and Blunsom (2014), methods dealing with bilingual lexicon extraction from comparable corpora can be classified as distributional-based or distributed-based approaches.",Related Work,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 1/41
==========================================================================================
We conducted two sets of experiments.",50190,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The bilingual terminology reference list is composed of 248 French/English single words for the Breast cancer corpus and 150 French/English single words for the Wind energy corpus.,Gold Standard,The first one aims at providing insights into the behaviour of each state-of-the-art embedding model on the specialized comparable corpora.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 2/41
==========================================================================================
The first one aims at providing insights into the behaviour of each state-of-the-art embedding model on the specialized comparable corpora.",50191,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We conducted two sets of experiments.,Experiments and Results,The second one aims at studying the contribution of ensemble models.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 3/41
==========================================================================================
The second one aims at studying the contribution of ensemble models.",50192,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The first one aims at providing insights into the behaviour of each state-of-the-art embedding model on the specialized comparable corpora.,Experiments and Results,"We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: M AP (Ref ) = 1 |Ref",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 4/41
==========================================================================================
We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: M AP (Ref ) = 1 |Ref",50193,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The second one aims at studying the contribution of ensemble models.,Experiments and Results,| |Ref | i=1 1 r i (4) where |Ref | is the number of terms of the reference list and r i the rank of the correct candidate translation i. Figure 1 shows the results of each embedding model for the task of bilingual terminology extraction from the two specialized comparable corpora.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 5/41
==========================================================================================
| |Ref | i=1 1 r i (4) where |Ref | is the number of terms of the reference list and r i the rank of the correct candidate translation i. Figure 1 shows the results of each embedding model for the task of bilingual terminology extraction from the two specialized comparable corpora.",50194,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: M AP (Ref ) = 1 |Ref",Experiments and Results,"We report the results of the continuous bag-of-word model (CBOW ), the Skip-gram model (SG), the glove model (Glove), the structured continuous window model (Cwindow) and the two character n-gram models, namely the character skip-gram model (CharSG) and the character CBOW model (CharCBOW ) 13 .",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 6/41
==========================================================================================
We report the results of the continuous bag-of-word model (CBOW ), the Skip-gram model (SG), the glove model (Glove), the structured continuous window model (Cwindow) and the two character n-gram models, namely the character skip-gram model (CharSG) and the character CBOW model (CharCBOW ) 13 .",50195,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,| |Ref | i=1 1 r i (4) where |Ref | is the number of terms of the reference list and r i the rank of the correct candidate translation i. Figure 1 shows the results of each embedding model for the task of bilingual terminology extraction from the two specialized comparable corpora.,Experiments and Results,"For each specialized comparable corpus, we varied the context window size (w) (see sub-figures 1(a) and 1(b)) and the embeddings dimension size (dim) (see sub-figures 1(c) and 1(d)).",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 7/41
==========================================================================================
For each specialized comparable corpus, we varied the context window size (w) (see sub-figures 1(a) and 1(b)) and the embeddings dimension size (dim) (see sub-figures 1(c) and 1(d)).",50196,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"We report the results of the continuous bag-of-word model (CBOW ), the Skip-gram model (SG), the glove model (Glove), the structured continuous window model (Cwindow) and the two character n-gram models, namely the character skip-gram model (CharSG) and the character CBOW model (CharCBOW ) 13 .",Experiments and Results,The first remarkable point is the performance of the character n-gram embedding models which far outperform other models.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 8/41
==========================================================================================
The first remarkable point is the performance of the character n-gram embedding models which far outperform other models.",50197,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For each specialized comparable corpus, we varied the context window size (w) (see sub-figures 1(a) and 1(b)) and the embeddings dimension size (dim) (see sub-figures 1(c) and 1(d)).",Experiments and Results,"This is an important finding since to our knowledge, no previous evaluation of character n-gram-based models have been conducted so far for bilingual terminology extraction from specialized corpora.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 9/41
==========================================================================================
This is an important finding since to our knowledge, no previous evaluation of character n-gram-based models have been conducted so far for bilingual terminology extraction from specialized corpora.",50198,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The first remarkable point is the performance of the character n-gram embedding models which far outperform other models.,Experiments and Results,"The first competitive model which is CharCBOW , is not sensitive to the context window size as well as the embeddings dimension size.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 10/41
==========================================================================================
The first competitive model which is CharCBOW , is not sensitive to the context window size as well as the embeddings dimension size.",50199,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"This is an important finding since to our knowledge, no previous evaluation of character n-gram-based models have been conducted so far for bilingual terminology extraction from specialized corpora.",Experiments and Results,"The second competitive model which is CharSG, is also not sensitive to the embeddings dimension size but sensitive to the context window size.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 11/41
==========================================================================================
The second competitive model which is CharSG, is also not sensitive to the embeddings dimension size but sensitive to the context window size.",50200,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"The first competitive model which is CharCBOW , is not sensitive to the context window size as well as the embeddings dimension size.",Experiments and Results,"CBOW which is the third competitive model, turned out to be the best word-based model.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 12/41
==========================================================================================
CBOW which is the third competitive model, turned out to be the best word-based model.",50201,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"The second competitive model which is CharSG, is also not sensitive to the embeddings dimension size but sensitive to the context window size.",Experiments and Results,"Nevertheless, its performance is far below the character-based models.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 13/41
==========================================================================================
Nevertheless, its performance is far below the character-based models.",50202,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"CBOW which is the third competitive model, turned out to be the best word-based model.",Experiments and Results,CBOW is also not very sensitive to the context window and the embeddings dimension sizes. ,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 14/41
==========================================================================================
CBOW is also not very sensitive to the context window and the embeddings dimension sizes. ",50203,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Nevertheless, its performance is far below the character-based models.",Experiments and Results,Tables 2 and 3 show a comparison of several combinations of word embedding models for the two specialized comparable corpora.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 15/41
==========================================================================================
Tables 2 and 3 show a comparison of several combinations of word embedding models for the two specialized comparable corpora.",50204,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,CBOW is also not very sensitive to the context window and the embeddings dimension sizes. ,Experiments and Results,Each combination is based on L2 normalization at vector length level (len) or at vector dimension level (dim).,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 16/41
==========================================================================================
Each combination is based on L2 normalization at vector length level (len) or at vector dimension level (dim).",50205,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Tables 2 and 3 show a comparison of several combinations of word embedding models for the two specialized comparable corpora.,Experiments and Results,"We chose the three embedding models that have shown the best performance individually (according to the Figure 1: CharSG, CharCBOW and CBOW ) and we combine them with all the models.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 17/41
==========================================================================================
We chose the three embedding models that have shown the best performance individually (according to the Figure 1: CharSG, CharCBOW and CBOW ) and we combine them with all the models.",50206,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Each combination is based on L2 normalization at vector length level (len) or at vector dimension level (dim).,Experiments and Results,"For each Table, the first line is a quick reminder of the three embedding models and their performance in terms of MAP scores shown between brackets.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 18/41
==========================================================================================
For each Table, the first line is a quick reminder of the three embedding models and their performance in terms of MAP scores shown between brackets.",50207,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"We chose the three embedding models that have shown the best performance individually (according to the Figure 1: CharSG, CharCBOW and CBOW ) and we combine them with all the models.",Experiments and Results,The following two lines also remind the embedding models used for combination and their MAP scores when used individually.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 19/41
==========================================================================================
The following two lines also remind the embedding models used for combination and their MAP scores when used individually.",50208,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For each Table, the first line is a quick reminder of the three embedding models and their performance in terms of MAP scores shown between brackets.",Experiments and Results,The following four lines present the results of the combination of these models by addition or concatenation for both normalizations.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 20/41
==========================================================================================
The following four lines present the results of the combination of these models by addition or concatenation for both normalizations.",50209,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The following two lines also remind the embedding models used for combination and their MAP scores when used individually.,Experiments and Results,"For instance in Table 2, the CharSG and CBOW embedding models used individually have respectively 36.4% and 21.9% of MAP and the combination of the two models gives 22.9% of MAP by addition and 34.9% of MAP by concatenation with L2 normalization at vector length level.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 21/41
==========================================================================================
For instance in Table 2, the CharSG and CBOW embedding models used individually have respectively 36.4% and 21.9% of MAP and the combination of the two models gives 22.9% of MAP by addition and 34.9% of MAP by concatenation with L2 normalization at vector length level.",50210,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The following four lines present the results of the combination of these models by addition or concatenation for both normalizations.,Experiments and Results,Combining embedding models is very effective when using concatenation in most cases.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 22/41
==========================================================================================
Combining embedding models is very effective when using concatenation in most cases.",50211,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For instance in Table 2, the CharSG and CBOW embedding models used individually have respectively 36.4% and 21.9% of MAP and the combination of the two models gives 22.9% of MAP by addition and 34.9% of MAP by concatenation with L2 normalization at vector length level.",Experiments and Results,The best combination model is obtained by the concatenation of CharSG and CharCBOW using length L2 Norm (70.3% of MAP in Table 2) and by the concatenation of CharCBOW and CBOW (49.5% of MAP in Table 3) followed by the same model using dimension L2 Norm for breast cancer corpus (68.1% of MAP in Table 2) and using CharSG with SG concatenation for wind energy corpus (45.4% of MAP in Table 3).,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 23/41
==========================================================================================
The best combination model is obtained by the concatenation of CharSG and CharCBOW using length L2 Norm (70.3% of MAP in Table 2) and by the concatenation of CharCBOW and CBOW (49.5% of MAP in Table 3) followed by the same model using dimension L2 Norm for breast cancer corpus (68.1% of MAP in Table 2) and using CharSG with SG concatenation for wind energy corpus (45.4% of MAP in Table 3).",50212,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Combining embedding models is very effective when using concatenation in most cases.,Experiments and Results,We observe that both types of L2 normalization are in general useful for concatenation with a better performance when using length normalization.   ,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 24/41
==========================================================================================
We observe that both types of L2 normalization are in general useful for concatenation with a better performance when using length normalization.   ",50213,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The best combination model is obtained by the concatenation of CharSG and CharCBOW using length L2 Norm (70.3% of MAP in Table 2) and by the concatenation of CharCBOW and CBOW (49.5% of MAP in Table 3) followed by the same model using dimension L2 Norm for breast cancer corpus (68.1% of MAP in Table 2) and using CharSG with SG concatenation for wind energy corpus (45.4% of MAP in Table 3).,Experiments and Results,"Table 4: Results (MAP %) of different embedding models on the specialize corpora as well as several outof-domain corpora and their combinations ( (Bojanowski et al., 2016) is the pre-trained CharSG model). ",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 25/41
==========================================================================================
Table 4: Results (MAP %) of different embedding models on the specialize corpora as well as several outof-domain corpora and their combinations ( (Bojanowski et al., 2016) is the pre-trained CharSG model). ",50214,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We observe that both types of L2 normalization are in general useful for concatenation with a better performance when using length normalization.   ,Experiments and Results,Table 4 shows the results of different combinations 14 of specialized corpora with external resources. ,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 26/41
==========================================================================================
Table 4 shows the results of different combinations 14 of specialized corpora with external resources. ",50215,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Table 4: Results (MAP %) of different embedding models on the specialize corpora as well as several outof-domain corpora and their combinations ( (Bojanowski et al., 2016) is the pre-trained CharSG model). ",Experiments and Results,"14 Combination means that we first merge different data sets in one single corpus and then, we learn an embedding model We represent in the 1 st column the results for the specialized corpora taken individually (BC and WE) and then, from the 2 nd to the 4 th column we show the results of the external resources taken individually and their combination represented in the 5 th column (ALL).",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 27/41
==========================================================================================
14 Combination means that we first merge different data sets in one single corpus and then, we learn an embedding model We represent in the 1 st column the results for the specialized corpora taken individually (BC and WE) and then, from the 2 nd to the 4 th column we show the results of the external resources taken individually and their combination represented in the 5 th column (ALL).",50216,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Table 4 shows the results of different combinations 14 of specialized corpora with external resources. ,Experiments and Results,"Finally, from the 6 th to the 8 th column we combine the specialized corpora with each external data and then their entire combination 15 (All).",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 28/41
==========================================================================================
Finally, from the 6 th to the 8 th column we combine the specialized corpora with each external data and then their entire combination 15 (All).",50217,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"14 Combination means that we first merge different data sets in one single corpus and then, we learn an embedding model We represent in the 1 st column the results for the specialized corpora taken individually (BC and WE) and then, from the 2 nd to the 4 th column we show the results of the external resources taken individually and their combination represented in the 5 th column (ALL).",Experiments and Results,"Overall, we see that combining different resources gives significant improvements over the two specialized corpora.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 29/41
==========================================================================================
Overall, we see that combining different resources gives significant improvements over the two specialized corpora.",50218,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Finally, from the 6 th to the 8 th column we combine the specialized corpora with each external data and then their entire combination 15 (All).",Experiments and Results,"We also notice the usefulness of external data used individually which performs better than small specialized corpora, except for CharCBOW which shows the strength of this model and its usefulness over other types of embedding models.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 30/41
==========================================================================================
We also notice the usefulness of external data used individually which performs better than small specialized corpora, except for CharCBOW which shows the strength of this model and its usefulness over other types of embedding models.",50219,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Overall, we see that combining different resources gives significant improvements over the two specialized corpora.",Experiments and Results,"Also, using the pre-trained embedding model of Bojanowski et al. (2016) obtained good results (72.4% for BC and 68.2% for WE) but if we compare it to the same model (CharSG) learned while combining specialized and external data sets, we observe better performance (76.5% for BC and 72.4% for WE) which shows that using jointly specialized and external data is more efficient than the use of external data only.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 31/41
==========================================================================================
Also, using the pre-trained embedding model of Bojanowski et al. (2016) obtained good results (72.4% for BC and 68.2% for WE) but if we compare it to the same model (CharSG) learned while combining specialized and external data sets, we observe better performance (76.5% for BC and 72.4% for WE) which shows that using jointly specialized and external data is more efficient than the use of external data only.",50220,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"We also notice the usefulness of external data used individually which performs better than small specialized corpora, except for CharCBOW which shows the strength of this model and its usefulness over other types of embedding models.",Experiments and Results,"This statement is confirmed by the results obtained by individual external data sets which are always lower than their combination with specialized corpora. and the Selective Standard Approach (SSA) and our approaches using CharCBOW and CharSG and their combinations (SCharCBOW, SCharSG and Meta-Emb) for the breast cancer corpus (BC) using the different external data (the improvements indicate a significance at the 0.05 level using the Student t-test). ",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 32/41
==========================================================================================
This statement is confirmed by the results obtained by individual external data sets which are always lower than their combination with specialized corpora. and the Selective Standard Approach (SSA) and our approaches using CharCBOW and CharSG and their combinations (SCharCBOW, SCharSG and Meta-Emb) for the breast cancer corpus (BC) using the different external data (the improvements indicate a significance at the 0.05 level using the Student t-test). ",50221,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Also, using the pre-trained embedding model of Bojanowski et al. (2016) obtained good results (72.4% for BC and 68.2% for WE) but if we compare it to the same model (CharSG) learned while combining specialized and external data sets, we observe better performance (76.5% for BC and 72.4% for WE) which shows that using jointly specialized and external data is more efficient than the use of external data only.",Experiments and Results,"In Table 5, we report the results obtained in Hazem and Morin (2017) (SA, CBOW, SG, SCBOW, SSG ,SCBOW+SSG and SSA) and our results using combination and meta-embeddings of character n-gram models (SCharCBOW, SCharSG, SCharCBOW+SCharSG and Meta-Emb (Best) 16 ).",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 33/41
==========================================================================================
In Table 5, we report the results obtained in Hazem and Morin (2017) (SA, CBOW, SG, SCBOW, SSG ,SCBOW+SSG and SSA) and our results using combination and meta-embeddings of character n-gram models (SCharCBOW, SCharSG, SCharCBOW+SCharSG and Meta-Emb (Best) 16 ).",50222,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"This statement is confirmed by the results obtained by individual external data sets which are always lower than their combination with specialized corpora. and the Selective Standard Approach (SSA) and our approaches using CharCBOW and CharSG and their combinations (SCharCBOW, SCharSG and Meta-Emb) for the breast cancer corpus (BC) using the different external data (the improvements indicate a significance at the 0.05 level using the Student t-test). ",Experiments and Results,The main conclusion in Hazem and Morin (2017) is that the best embedding combination (SCBOW+SSG) couldn't outperform the selective standard approach (SSA).,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 34/41
==========================================================================================
The main conclusion in Hazem and Morin (2017) is that the best embedding combination (SCBOW+SSG) couldn't outperform the selective standard approach (SSA).",50223,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In Table 5, we report the results obtained in Hazem and Morin (2017) (SA, CBOW, SG, SCBOW, SSG ,SCBOW+SSG and SSA) and our results using combination and meta-embeddings of character n-gram models (SCharCBOW, SCharSG, SCharCBOW+SCharSG and Meta-Emb (Best) 16 ).",Experiments and Results,"According to our results, character n-gram models and their combination obtained better results than the best CBOW and Skip-gram combination (SCBOW+SSG) and also outperformed the selective standard approach (SSA obtained 66.6% using BC + JRC and 82.3% using BC + CC while our best model obtained 74.8% and 83.1% on the same corpora).",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 35/41
==========================================================================================
According to our results, character n-gram models and their combination obtained better results than the best CBOW and Skip-gram combination (SCBOW+SSG) and also outperformed the selective standard approach (SSA obtained 66.6% using BC + JRC and 82.3% using BC + CC while our best model obtained 74.8% and 83.1% on the same corpora).",50224,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The main conclusion in Hazem and Morin (2017) is that the best embedding combination (SCBOW+SSG) couldn't outperform the selective standard approach (SSA).,Experiments and Results,The results of Table 5 provide strong support for data combination and meta-embeddings using character n-gram models.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 36/41
==========================================================================================
The results of Table 5 provide strong support for data combination and meta-embeddings using character n-gram models.",50225,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"According to our results, character n-gram models and their combination obtained better results than the best CBOW and Skip-gram combination (SCBOW+SSG) and also outperformed the selective standard approach (SSA obtained 66.6% using BC + JRC and 82.3% using BC + CC while our best model obtained 74.8% and 83.1% on the same corpora).",Experiments and Results,"Also, we highlight the fact that character ngram models and their combination is much faster than CBOW and Skip-gram models.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 37/41
==========================================================================================
Also, we highlight the fact that character ngram models and their combination is much faster than CBOW and Skip-gram models.",50226,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The results of Table 5 provide strong support for data combination and meta-embeddings using character n-gram models.,Experiments and Results,"In addition, the dimension size of embedding models on the merged corpus. ",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 38/41
==========================================================================================
In addition, the dimension size of embedding models on the merged corpus. ",50227,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Also, we highlight the fact that character ngram models and their combination is much faster than CBOW and Skip-gram models.",Experiments and Results,"15 ALL in the 5 th column means that we combine JRC, CC and WIKI, while ALL in the 9 th column means that we combine JRC, CC and WIKI and the specialized corpora BC or WE.",Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 39/41
==========================================================================================
15 ALL in the 5 th column means that we combine JRC, CC and WIKI, while ALL in the 9 th column means that we combine JRC, CC and WIKI and the specialized corpora BC or WE.",50228,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In addition, the dimension size of embedding models on the merged corpus. ",Experiments and Results,16 Meta-Emb (best) stands for the combination of SCharSG on specialized corpus with SCharCBOW+SCharSG on external data. ,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 40/41
==========================================================================================
16 Meta-Emb (best) stands for the combination of SCharSG on specialized corpus with SCharCBOW+SCharSG on external data. ",50229,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"15 ALL in the 5 th column means that we combine JRC, CC and WIKI, while ALL in the 9 th column means that we combine JRC, CC and WIKI and the specialized corpora BC or WE.",Experiments and Results,is very low (around 300) while in the standard approach it corresponds to the vocabulary size which leads to sparse vectors and high computational cost.,Experiments and Results,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
7. Experiments and Results -- 41/41
==========================================================================================
is very low (around 300) while in the standard approach it corresponds to the vocabulary size which leads to sparse vectors and high computational cost.",50230,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,16 Meta-Emb (best) stands for the combination of SCharSG on specialized corpus with SCharCBOW+SCharSG on external data. ,Experiments and Results,"The first important finding of this work is the efficiency of the character n-gram models (CharCBOW and CharSG) which drastically outperform other models, whether on specialized or general domain data sets.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 1/22
==========================================================================================
The first important finding of this work is the efficiency of the character n-gram models (CharCBOW and CharSG) which drastically outperform other models, whether on specialized or general domain data sets.",50231,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,is very low (around 300) while in the standard approach it corresponds to the vocabulary size which leads to sparse vectors and high computational cost.,Experiments and Results,Their better performance can be explained by the fact that both models are based on characters to build the embedding models.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 2/22
==========================================================================================
Their better performance can be explained by the fact that both models are based on characters to build the embedding models.",50232,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"The first important finding of this work is the efficiency of the character n-gram models (CharCBOW and CharSG) which drastically outperform other models, whether on specialized or general domain data sets.",Discussion,"While CBOW and Skip-gram suffer from the lack of data to build efficient models over specialized corpora, the character-based approaches benefit of much more training examples as they use characters for their models.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 3/22
==========================================================================================
While CBOW and Skip-gram suffer from the lack of data to build efficient models over specialized corpora, the character-based approaches benefit of much more training examples as they use characters for their models.",50233,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,Their better performance can be explained by the fact that both models are based on characters to build the embedding models.,Discussion,The second important finding is the performance of external data when applied for extracting bilingual terms.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 4/22
==========================================================================================
The second important finding is the performance of external data when applied for extracting bilingual terms.",50234,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"While CBOW and Skip-gram suffer from the lack of data to build efficient models over specialized corpora, the character-based approaches benefit of much more training examples as they use characters for their models.",Discussion,"This is not surprising as external data sets such as wikipedia or common crawl for instance, contain several scientific and specialized documents.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 5/22
==========================================================================================
This is not surprising as external data sets such as wikipedia or common crawl for instance, contain several scientific and specialized documents.",50235,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,The second important finding is the performance of external data when applied for extracting bilingual terms.,Discussion,"In addition, we could observe the complementarity of external resources with the specialized domain data sets.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 6/22
==========================================================================================
In addition, we could observe the complementarity of external resources with the specialized domain data sets.",50236,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"This is not surprising as external data sets such as wikipedia or common crawl for instance, contain several scientific and specialized documents.",Discussion,"More precisely, concatenating embeddings of specialized and external resources significantly improves the results.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 7/22
==========================================================================================
More precisely, concatenating embeddings of specialized and external resources significantly improves the results.",50237,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In addition, we could observe the complementarity of external resources with the specialized domain data sets.",Discussion,This can be explained by the nature of the captured information which can be resumed in the concatenated embedding vectors.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 8/22
==========================================================================================
This can be explained by the nature of the captured information which can be resumed in the concatenated embedding vectors.",50238,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"More precisely, concatenating embeddings of specialized and external resources significantly improves the results.",Discussion,"If a single generic embedding model is difficult to obtain, character ngram and word-based embedding models can be efficiently combined to improve bilingual terminology extraction from comparable corpora. ",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 9/22
==========================================================================================
If a single generic embedding model is difficult to obtain, character ngram and word-based embedding models can be efficiently combined to improve bilingual terminology extraction from comparable corpora. ",50239,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,This can be explained by the nature of the captured information which can be resumed in the concatenated embedding vectors.,Discussion,We also conducted an error analysis of the different proposed models and we couldn't find a strong relation between the embedding models and the non captured terms.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 10/22
==========================================================================================
We also conducted an error analysis of the different proposed models and we couldn't find a strong relation between the embedding models and the non captured terms.",50240,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"If a single generic embedding model is difficult to obtain, character ngram and word-based embedding models can be efficiently combined to improve bilingual terminology extraction from comparable corpora. ",Discussion,"However, we observed that the CharCBOW model with the meta-embedding combination using BC with CC corpora improves overall the rank of the translations obtained for each individual corpus.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 11/22
==========================================================================================
However, we observed that the CharCBOW model with the meta-embedding combination using BC with CC corpora improves overall the rank of the translations obtained for each individual corpus.",50241,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We also conducted an error analysis of the different proposed models and we couldn't find a strong relation between the embedding models and the non captured terms.,Discussion,"By looking at the 115 translations of BC and the 80 of BC+CC not found in the first rank, we found 58 terms in common including 44 terms with a better rank with BC+CC corpora.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 12/22
==========================================================================================
By looking at the 115 translations of BC and the 80 of BC+CC not found in the first rank, we found 58 terms in common including 44 terms with a better rank with BC+CC corpora.",50242,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"However, we observed that the CharCBOW model with the meta-embedding combination using BC with CC corpora improves overall the rank of the translations obtained for each individual corpus.",Discussion,"In the same way, from the 75 translations of CC and the 80 of BC+CC not found in the first rank, we found 62 terms in common including 46 terms with a better rank with BC+CC corpora.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 13/22
==========================================================================================
In the same way, from the 75 translations of CC and the 80 of BC+CC not found in the first rank, we found 62 terms in common including 46 terms with a better rank with BC+CC corpora.",50243,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"By looking at the 115 translations of BC and the 80 of BC+CC not found in the first rank, we found 58 terms in common including 44 terms with a better rank with BC+CC corpora.",Discussion,It might seem surprising that we found more terms outside the first rank with CC corpus than BC+CC (80 versus 75) since the MAP is lower with CC than BC+CC (57.4 versus 73.9).,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 14/22
==========================================================================================
It might seem surprising that we found more terms outside the first rank with CC corpus than BC+CC (80 versus 75) since the MAP is lower with CC than BC+CC (57.4 versus 73.9).",50244,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In the same way, from the 75 translations of CC and the 80 of BC+CC not found in the first rank, we found 62 terms in common including 46 terms with a better rank with BC+CC corpora.",Discussion,"In fact, the CharCBOW model with BC+CC improves overall the rank of all the translations of CC and more particularly the rank of the first hundred translations taken into account in the calculation of the MAP.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 15/22
==========================================================================================
In fact, the CharCBOW model with BC+CC improves overall the rank of all the translations of CC and more particularly the rank of the first hundred translations taken into account in the calculation of the MAP.",50245,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,It might seem surprising that we found more terms outside the first rank with CC corpus than BC+CC (80 versus 75) since the MAP is lower with CC than BC+CC (57.4 versus 73.9).,Discussion,We also observed that the more frequent terms are the best translated for SA and SSA approaches.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 16/22
==========================================================================================
We also observed that the more frequent terms are the best translated for SA and SSA approaches.",50246,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"In fact, the CharCBOW model with BC+CC improves overall the rank of all the translations of CC and more particularly the rank of the first hundred translations taken into account in the calculation of the MAP.",Discussion,"For the embedding approaches, this observation is not relevant.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 17/22
==========================================================================================
For the embedding approaches, this observation is not relevant.",50247,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We also observed that the more frequent terms are the best translated for SA and SSA approaches.,Discussion,"For instance, the translations of frequent terms such as cancer and breast are found in the first ranks and the translations of infrequent terms such as lumpectomy and fibroadenoma are found in the last ranks with SA and SSA approaches.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 18/22
==========================================================================================
For instance, the translations of frequent terms such as cancer and breast are found in the first ranks and the translations of infrequent terms such as lumpectomy and fibroadenoma are found in the last ranks with SA and SSA approaches.",50248,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For the embedding approaches, this observation is not relevant.",Discussion,"With embedding approaches, cancer and fibroadenoma are found in the first ranks and breast and lumpectomy in the last ranks.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 19/22
==========================================================================================
With embedding approaches, cancer and fibroadenoma are found in the first ranks and breast and lumpectomy in the last ranks.",50249,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"For instance, the translations of frequent terms such as cancer and breast are found in the first ranks and the translations of infrequent terms such as lumpectomy and fibroadenoma are found in the last ranks with SA and SSA approaches.",Discussion,We have not been able to better characterize terms in the last ranks with embedding approaches.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 20/22
==========================================================================================
We have not been able to better characterize terms in the last ranks with embedding approaches.",50250,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"With embedding approaches, cancer and fibroadenoma are found in the first ranks and breast and lumpectomy in the last ranks.",Discussion,This is strongly dependent on embedding parameters and also context size and embedding dimensions.,Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 21/22
==========================================================================================
This is strongly dependent on embedding parameters and also context size and embedding dimensions.",50251,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We have not been able to better characterize terms in the last ranks with embedding approaches.,Discussion,"Our best system, however obtained 90% of precision on top 5, in the perspective of providing first translation terms for translation aided systems, our proposed approach is certainly more appropriate than the standard approach whether in terms of computational cost or in terms of accuracy.",Discussion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
8. Discussion -- 22/22
==========================================================================================
Our best system, however obtained 90% of precision on top 5, in the perspective of providing first translation terms for translation aided systems, our proposed approach is certainly more appropriate than the standard approach whether in terms of computational cost or in terms of accuracy.",50252,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,This is strongly dependent on embedding parameters and also context size and embedding dimensions.,Discussion,In this paper we have explored a variety of embedding models and there impact on the task of bilingual terminology extraction from specialized comparable corpora.,Conclusion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
9. Conclusion -- 1/3
==========================================================================================
In this paper we have explored a variety of embedding models and there impact on the task of bilingual terminology extraction from specialized comparable corpora.",50253,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,"Our best system, however obtained 90% of precision on top 5, in the perspective of providing first translation terms for translation aided systems, our proposed approach is certainly more appropriate than the standard approach whether in terms of computational cost or in terms of accuracy.",Discussion,We have also proposed meta-embedding representations and have shown under which conditions they can be jointly used for better performance.,Conclusion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
9. Conclusion -- 2/3
==========================================================================================
We have also proposed meta-embedding representations and have shown under which conditions they can be jointly used for better performance.",50254,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,In this paper we have explored a variety of embedding models and there impact on the task of bilingual terminology extraction from specialized comparable corpora.,Conclusion,"If further investigations are probably needed, our findings strengthens the idea that using metaembeddings based on specialized and general domain data sets improves the performance of mining bilingual specialized lexicons.",Conclusion,
"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora
==========================================================================================
9. Conclusion -- 3/3
==========================================================================================
If further investigations are probably needed, our findings strengthens the idea that using metaembeddings based on specialized and general domain data sets improves the performance of mining bilingual specialized lexicons.",50255,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,"0. abstract
1. Introduction
2. Related Work
3. Embedding Models
4. Approach
5. Data and Resources
6. Common crawl corpus (CC
7. Experiments and Results
8. Discussion
9. Conclusion
",2018,We have also proposed meta-embedding representations and have shown under which conditions they can be jointly used for better performance.,Conclusion,,,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 1/12
==========================================================================================
  In this work, we present an approach for mining user preferences and recommendation based on reviews.",4137030,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,,,There have been various studies worked on recommendation problem.,abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 2/12
==========================================================================================
There have been various studies worked on recommendation problem.",4137031,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"  In this work, we present an approach for mining user preferences and recommendation based on reviews.",abstract,"However, most of the studies beyond one aspect user generated- content such as user ratings, user feedback and so on to state user preferences.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 3/12
==========================================================================================
However, most of the studies beyond one aspect user generated- content such as user ratings, user feedback and so on to state user preferences.",4137032,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,There have been various studies worked on recommendation problem.,abstract,There is a prob- lem in one aspect mining is lacking for stating user preferences.,abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 4/12
==========================================================================================
There is a prob- lem in one aspect mining is lacking for stating user preferences.",4137033,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"However, most of the studies beyond one aspect user generated- content such as user ratings, user feedback and so on to state user preferences.",abstract,"As a demonstration, in collaborative filter recommendation, we try to figure out the preference trend of crowded users, then use that trend to predict current user preference.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 5/12
==========================================================================================
As a demonstration, in collaborative filter recommendation, we try to figure out the preference trend of crowded users, then use that trend to predict current user preference.",4137034,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,There is a prob- lem in one aspect mining is lacking for stating user preferences.,abstract,"Therefore, there is a gap between real user preferences and the trend of the crowded people.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 6/12
==========================================================================================
Therefore, there is a gap between real user preferences and the trend of the crowded people.",4137035,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"As a demonstration, in collaborative filter recommendation, we try to figure out the preference trend of crowded users, then use that trend to predict current user preference.",abstract,"Additionally, user preferences can be addressed from mining user reviews since user often comment about various aspects of products.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 7/12
==========================================================================================
Additionally, user preferences can be addressed from mining user reviews since user often comment about various aspects of products.",4137036,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, there is a gap between real user preferences and the trend of the crowded people.",abstract,"To solve this problem, we mainly focus on mining product aspects and user aspects inside user reviews to directly state user preferences.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 8/12
==========================================================================================
To solve this problem, we mainly focus on mining product aspects and user aspects inside user reviews to directly state user preferences.",4137037,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Additionally, user preferences can be addressed from mining user reviews since user often comment about various aspects of products.",abstract,We also take into account Social Network Analysis for cold-start item problem.,abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 9/12
==========================================================================================
We also take into account Social Network Analysis for cold-start item problem.",4137038,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"To solve this problem, we mainly focus on mining product aspects and user aspects inside user reviews to directly state user preferences.",abstract,"With cold-start user problem, collaborative filter algorithm is employed in our work.",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 10/12
==========================================================================================
With cold-start user problem, collaborative filter algorithm is employed in our work.",4137039,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,We also take into account Social Network Analysis for cold-start item problem.,abstract,The framework is general enough to be applied to different recommendation domains.,abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 11/12
==========================================================================================
The framework is general enough to be applied to different recommendation domains.",4137040,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"With cold-start user problem, collaborative filter algorithm is employed in our work.",abstract,"Theoretically, our method would achieve a significant enhancement. ",abstract,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
0. abstract -- 12/12
==========================================================================================
Theoretically, our method would achieve a significant enhancement. ",4137041,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The framework is general enough to be applied to different recommendation domains.,abstract,User generated content in the form of comments has witnessed an explosive growth on the web.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 1/23
==========================================================================================
User generated content in the form of comments has witnessed an explosive growth on the web.",4137042,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Theoretically, our method would achieve a significant enhancement. ",abstract,"Websites like Amazon.com, IMDb.com allow users to comment on diverse contents like news articles.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 2/23
==========================================================================================
Websites like Amazon.com, IMDb.com allow users to comment on diverse contents like news articles.",4137043,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,User generated content in the form of comments has witnessed an explosive growth on the web.,Introduction,"These ""crowd-sourced"" comments are highly engaging because they reflect the views and opinions of real users.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 3/23
==========================================================================================
These ""crowd-sourced"" comments are highly engaging because they reflect the views and opinions of real users.",4137044,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Websites like Amazon.com, IMDb.com allow users to comment on diverse contents like news articles.",Introduction,"Moreover, as the statistic from 1 70% customers consult reviews Figure 1: A user review on IMDb.com or ratings before purchasing.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 4/23
==========================================================================================
Moreover, as the statistic from 1 70% customers consult reviews Figure 1: A user review on IMDb.com or ratings before purchasing.",4137045,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"These ""crowd-sourced"" comments are highly engaging because they reflect the views and opinions of real users.",Introduction,"Along with this, viewers also check movie reviews before making decision to buy movie tickets.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 5/23
==========================================================================================
Along with this, viewers also check movie reviews before making decision to buy movie tickets.",4137046,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Moreover, as the statistic from 1 70% customers consult reviews Figure 1: A user review on IMDb.com or ratings before purchasing.",Introduction,There have been very few studies about movie recommendation that employ movie reviews.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 6/23
==========================================================================================
There have been very few studies about movie recommendation that employ movie reviews.",4137047,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Along with this, viewers also check movie reviews before making decision to buy movie tickets.",Introduction,"Therefore, in this work, we focus on mining user reviews to state user preferences for recommendation. ",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 7/23
==========================================================================================
Therefore, in this work, we focus on mining user reviews to state user preferences for recommendation. ",4137048,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,There have been very few studies about movie recommendation that employ movie reviews.,Introduction,"However, the key challenges with user generated comments is that inside a review, people praise and criticize various aspects of the target such as the noise level of a computer or the taste of a dish.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 8/23
==========================================================================================
However, the key challenges with user generated comments is that inside a review, people praise and criticize various aspects of the target such as the noise level of a computer or the taste of a dish.",4137049,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, in this work, we focus on mining user reviews to state user preferences for recommendation. ",Introduction,"In the Figure 1, the reviewer evaluates aspects of a new blockbuster film called ""Captain Philips"" such as director, actors, movie scenes.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 9/23
==========================================================================================
In the Figure 1, the reviewer evaluates aspects of a new blockbuster film called ""Captain Philips"" such as director, actors, movie scenes.",4137050,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"However, the key challenges with user generated comments is that inside a review, people praise and criticize various aspects of the target such as the noise level of a computer or the taste of a dish.",Introduction,"Based on different aspect, the reviewer has different opinion.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 10/23
==========================================================================================
Based on different aspect, the reviewer has different opinion.",4137051,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In the Figure 1, the reviewer evaluates aspects of a new blockbuster film called ""Captain Philips"" such as director, actors, movie scenes.",Introduction,"Therefore, user reviews are sufficient for stating user preference for recommendation.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 11/23
==========================================================================================
Therefore, user reviews are sufficient for stating user preference for recommendation.",4137052,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Based on different aspect, the reviewer has different opinion.",Introduction,"However, reviews are plain text without any structure, therefore how to mining aspects in reviews is a big challenge. ",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 12/23
==========================================================================================
However, reviews are plain text without any structure, therefore how to mining aspects in reviews is a big challenge. ",4137053,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, user reviews are sufficient for stating user preference for recommendation.",Introduction,"In this work, we propose a new approach for movie recommendation based on mining user reviews to state movie aspects and user preferences based on aspects.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 13/23
==========================================================================================
In this work, we propose a new approach for movie recommendation based on mining user reviews to state movie aspects and user preferences based on aspects.",4137054,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"However, reviews are plain text without any structure, therefore how to mining aspects in reviews is a big challenge. ",Introduction,"Generally, we apply LDA for finding hidden aspects for addressing user preference aspects and movie feature aspects.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 14/23
==========================================================================================
Generally, we apply LDA for finding hidden aspects for addressing user preference aspects and movie feature aspects.",4137055,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In this work, we propose a new approach for movie recommendation based on mining user reviews to state movie aspects and user preferences based on aspects.",Introduction,"After user preferences based on aspects and movie aspects are addressed, KL divergence is used for measuring similarity between movie and user.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 15/23
==========================================================================================
After user preferences based on aspects and movie aspects are addressed, KL divergence is used for measuring similarity between movie and user.",4137056,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Generally, we apply LDA for finding hidden aspects for addressing user preference aspects and movie feature aspects.",Introduction,Top K movies that close to user preferences are recommended to user. ,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 16/23
==========================================================================================
Top K movies that close to user preferences are recommended to user. ",4137057,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"After user preferences based on aspects and movie aspects are addressed, KL divergence is used for measuring similarity between movie and user.",Introduction,"Our main contributions in this work are including: (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb. ",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 17/23
==========================================================================================
Our main contributions in this work are including: (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb. ",4137058,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Top K movies that close to user preferences are recommended to user. ,Introduction,The rest of the paper is laid out as follows.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 18/23
==========================================================================================
The rest of the paper is laid out as follows.",4137059,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Our main contributions in this work are including: (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb. ",Introduction,Section 2 introduces some related works.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 19/23
==========================================================================================
Section 2 introduces some related works.",4137060,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The rest of the paper is laid out as follows.,Introduction,"Section 3, we address our approach to take advantage of topic modeling method to do recommendation.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 20/23
==========================================================================================
Section 3, we address our approach to take advantage of topic modeling method to do recommendation.",4137061,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Section 2 introduces some related works.,Introduction,"The Social Network Analysis along with Collaborative Filtering to overcome the coldstart item and cold-start user problem, respectively is deeply demonstrated in Section 4.",Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 21/23
==========================================================================================
The Social Network Analysis along with Collaborative Filtering to overcome the coldstart item and cold-start user problem, respectively is deeply demonstrated in Section 4.",4137062,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Section 3, we address our approach to take advantage of topic modeling method to do recommendation.",Introduction,Experiment is shown in Section 5.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 22/23
==========================================================================================
Experiment is shown in Section 5.",4137063,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"The Social Network Analysis along with Collaborative Filtering to overcome the coldstart item and cold-start user problem, respectively is deeply demonstrated in Section 4.",Introduction,Section 6 gives conclusion and our future studies.,Introduction,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
1. Introduction -- 23/23
==========================================================================================
Section 6 gives conclusion and our future studies.",4137064,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Experiment is shown in Section 5.,Introduction,"In this section, we will show here the previous studies on mining user aspects, some studies by using movie reviews for recommendation and showing how are they different from our work. ",Related Work,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 1/40
==========================================================================================
In this section, we will briefly describe about our method.",4137065,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Overall, there are three significant differences in our work from others including: (1) in our work, LDA is employed to state preferred features for both user and movie based on reviews, (2) we address and solve cold-start problems in our approach, and (3) we empirically apply our approach to the first large collected user reviews for recommendation.",Related Work,"Due to page limitation of this short paper, we cannot show our approach in more detail.",Proposed Framework,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 2/40
==========================================================================================
Due to page limitation of this short paper, we cannot show our approach in more detail.",4137066,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In this section, we will briefly describe about our method.",Proposed Framework,The workflow of our system is deeply presented in figure 2 which consists of 8 major processing steps: 1) modeling overall Model for all reviews in corpus; 2) getting all user-related reviews in corpus; 3) inference user feature preferences by using global model; 4) using collaborative filter between user to find cross relatedpreferences to solve cold-start user problem; 5) getting Tweets information for cold-start movie; 6) inference movies' feature preference by using global model achieved in step 2; 7) using similarity measurement (e.g cosine similarity) to find the similarity between user feature preferences and movie features; and 8) top K movies is generated to recommend to the user.,Framework overview,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 3/40
==========================================================================================
0. Framework overview -- 1/1
------------------------------------------------------------------------------------------
The workflow of our system is deeply presented in figure 2 which consists of 8 major processing steps: 1) modeling overall Model for all reviews in corpus; 2) getting all user-related reviews in corpus; 3) inference user feature preferences by using global model; 4) using collaborative filter between user to find cross relatedpreferences to solve cold-start user problem; 5) getting Tweets information for cold-start movie; 6) inference movies' feature preference by using global model achieved in step 2; 7) using similarity measurement (e.g cosine similarity) to find the similarity between user feature preferences and movie features; and 8) top K movies is generated to recommend to the user.",4137067,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Due to page limitation of this short paper, we cannot show our approach in more detail.",Proposed Framework,"For users who has sufficient information (enough reviews), we will construct their feature preferences from their reviews to the movies.",Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 4/40
==========================================================================================
1. Stating User Feature Preferences -- 1/6
------------------------------------------------------------------------------------------
For users who has sufficient information (enough reviews), we will construct their feature preferences from their reviews to the movies.",4137068,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The workflow of our system is deeply presented in figure 2 which consists of 8 major processing steps: 1) modeling overall Model for all reviews in corpus; 2) getting all user-related reviews in corpus; 3) inference user feature preferences by using global model; 4) using collaborative filter between user to find cross relatedpreferences to solve cold-start user problem; 5) getting Tweets information for cold-start movie; 6) inference movies' feature preference by using global model achieved in step 2; 7) using similarity measurement (e.g cosine similarity) to find the similarity between user feature preferences and movie features; and 8) top K movies is generated to recommend to the user.,Framework overview,"Basically, when a user writes a comment, s/he will comment about their opinions on item features that called feature-opinion pairs in [2].",Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 5/40
==========================================================================================
1. Stating User Feature Preferences -- 2/6
------------------------------------------------------------------------------------------
Basically, when a user writes a comment, s/he will comment about their opinions on item features that called feature-opinion pairs in [2].",4137069,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"For users who has sufficient information (enough reviews), we will construct their feature preferences from their reviews to the movies.",Stating User Feature Preferences,"However, in [2], Feng et al. discover features of item based on Part-of-Speech that will decrease the accuracy of detected-features since a lot of words with same tag but they are not feature of item.",Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 6/40
==========================================================================================
1. Stating User Feature Preferences -- 3/6
------------------------------------------------------------------------------------------
However, in [2], Feng et al. discover features of item based on Part-of-Speech that will decrease the accuracy of detected-features since a lot of words with same tag but they are not feature of item.",4137070,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Basically, when a user writes a comment, s/he will comment about their opinions on item features that called feature-opinion pairs in [2].",Stating User Feature Preferences,"Therefore, in our work, LDA is employed to detect those features.",Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 7/40
==========================================================================================
1. Stating User Feature Preferences -- 4/6
------------------------------------------------------------------------------------------
Therefore, in our work, LDA is employed to detect those features.",4137071,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"However, in [2], Feng et al. discover features of item based on Part-of-Speech that will decrease the accuracy of detected-features since a lot of words with same tag but they are not feature of item.",Stating User Feature Preferences,Obviously some noised features are also detected by LDA but their proportion will be lower than the true features since preprocessing process.,Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 8/40
==========================================================================================
1. Stating User Feature Preferences -- 5/6
------------------------------------------------------------------------------------------
Obviously some noised features are also detected by LDA but their proportion will be lower than the true features since preprocessing process.",4137072,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, in our work, LDA is employed to detect those features.",Stating User Feature Preferences,"For the cold-start user, we will deeply address in the subsection 4.1.",Stating User Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 9/40
==========================================================================================
1. Stating User Feature Preferences -- 6/6
------------------------------------------------------------------------------------------
For the cold-start user, we will deeply address in the subsection 4.1.",4137073,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Obviously some noised features are also detected by LDA but their proportion will be lower than the true features since preprocessing process.,Stating User Feature Preferences,All reviews of a movie are consider as a unseen document to the global topic model.,Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 10/40
==========================================================================================
2. Stating Movie Feature Preferences -- 1/8
------------------------------------------------------------------------------------------
All reviews of a movie are consider as a unseen document to the global topic model.",4137074,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"For the cold-start user, we will deeply address in the subsection 4.1.",Stating User Feature Preferences,"By referring from global topic model, movie preferred features are achieved.",Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 11/40
==========================================================================================
2. Stating Movie Feature Preferences -- 2/8
------------------------------------------------------------------------------------------
By referring from global topic model, movie preferred features are achieved.",4137075,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,All reviews of a movie are consider as a unseen document to the global topic model.,Stating Movie Feature Preferences,"For the movie that has limited reviews, we solve it by taking into account social network information as addressed in the subsection 4.2.",Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 12/40
==========================================================================================
2. Stating Movie Feature Preferences -- 3/8
------------------------------------------------------------------------------------------
For the movie that has limited reviews, we solve it by taking into account social network information as addressed in the subsection 4.2.",4137076,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"By referring from global topic model, movie preferred features are achieved.",Stating Movie Feature Preferences,Since our approach is mainly focusing on Recommendation for IMDb users by taking into account their reviews data.,Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 13/40
==========================================================================================
2. Stating Movie Feature Preferences -- 4/8
------------------------------------------------------------------------------------------
Since our approach is mainly focusing on Recommendation for IMDb users by taking into account their reviews data.",4137077,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"For the movie that has limited reviews, we solve it by taking into account social network information as addressed in the subsection 4.2.",Stating Movie Feature Preferences,Therefore our coldstart user in this case is the user that has insufficient review (e.g: User has one short review).,Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 14/40
==========================================================================================
2. Stating Movie Feature Preferences -- 5/8
------------------------------------------------------------------------------------------
Therefore our coldstart user in this case is the user that has insufficient review (e.g: User has one short review).",4137078,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Since our approach is mainly focusing on Recommendation for IMDb users by taking into account their reviews data.,Stating Movie Feature Preferences,"Basically, we adopt collaborative filtering method of user-item to user-feature for finding cross related-features between user.",Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 15/40
==========================================================================================
2. Stating Movie Feature Preferences -- 6/8
------------------------------------------------------------------------------------------
Basically, we adopt collaborative filtering method of user-item to user-feature for finding cross related-features between user.",4137079,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Therefore our coldstart user in this case is the user that has insufficient review (e.g: User has one short review).,Stating Movie Feature Preferences,So that the cold-start user will have sufficient information for recommendation.,Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 16/40
==========================================================================================
2. Stating Movie Feature Preferences -- 7/8
------------------------------------------------------------------------------------------
So that the cold-start user will have sufficient information for recommendation.",4137080,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Basically, we adopt collaborative filtering method of user-item to user-feature for finding cross related-features between user.",Stating Movie Feature Preferences,"In more detail, we will describe on the full paper.",Stating Movie Feature Preferences,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 17/40
==========================================================================================
2. Stating Movie Feature Preferences -- 8/8
------------------------------------------------------------------------------------------
In more detail, we will describe on the full paper.",4137081,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,So that the cold-start user will have sufficient information for recommendation.,Stating Movie Feature Preferences,"In our work, the problem of cold-start movie is for incoming movies that will be released in next few weeks/months.",Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 18/40
==========================================================================================
3. Cold-start Movie Recommendation -- 1/8
------------------------------------------------------------------------------------------
In our work, the problem of cold-start movie is for incoming movies that will be released in next few weeks/months.",4137082,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In more detail, we will describe on the full paper.",Stating Movie Feature Preferences,Since the review information is insufficient for applying recommendation.,Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 19/40
==========================================================================================
3. Cold-start Movie Recommendation -- 2/8
------------------------------------------------------------------------------------------
Since the review information is insufficient for applying recommendation.",4137083,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In our work, the problem of cold-start movie is for incoming movies that will be released in next few weeks/months.",Cold-start Movie Recommendation,"We found that with the incoming movies, the producers have been posted many information about the movies on Twitter.",Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 20/40
==========================================================================================
3. Cold-start Movie Recommendation -- 3/8
------------------------------------------------------------------------------------------
We found that with the incoming movies, the producers have been posted many information about the movies on Twitter.",4137084,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Since the review information is insufficient for applying recommendation.,Cold-start Movie Recommendation,Additionally there are many Twitter users follow the movies and give some Tweets on the movies.,Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 21/40
==========================================================================================
3. Cold-start Movie Recommendation -- 4/8
------------------------------------------------------------------------------------------
Additionally there are many Twitter users follow the movies and give some Tweets on the movies.",4137085,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"We found that with the incoming movies, the producers have been posted many information about the movies on Twitter.",Cold-start Movie Recommendation,"The table 1 shows the domination of Tweets information over IMDb reviews within one month from released date of the movie ""American Hustle"".",Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 22/40
==========================================================================================
3. Cold-start Movie Recommendation -- 5/8
------------------------------------------------------------------------------------------
The table 1 shows the domination of Tweets information over IMDb reviews within one month from released date of the movie ""American Hustle"".",4137086,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Additionally there are many Twitter users follow the movies and give some Tweets on the movies.,Cold-start Movie Recommendation,"In [4], Lin et al. also show that despite the coldstart, there is still information out there about the item, particularly on social networking services like Twitter.",Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 23/40
==========================================================================================
3. Cold-start Movie Recommendation -- 6/8
------------------------------------------------------------------------------------------
In [4], Lin et al. also show that despite the coldstart, there is still information out there about the item, particularly on social networking services like Twitter.",4137087,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"The table 1 shows the domination of Tweets information over IMDb reviews within one month from released date of the movie ""American Hustle"".",Cold-start Movie Recommendation,Therefore we collect user tweets about cold-movies on Twitter for suffi- cient movie reviews.,Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 24/40
==========================================================================================
3. Cold-start Movie Recommendation -- 7/8
------------------------------------------------------------------------------------------
Therefore we collect user tweets about cold-movies on Twitter for suffi- cient movie reviews.",4137088,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In [4], Lin et al. also show that despite the coldstart, there is still information out there about the item, particularly on social networking services like Twitter.",Cold-start Movie Recommendation,This step will be described in more detail in our full paper.,Cold-start Movie Recommendation,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 25/40
==========================================================================================
3. Cold-start Movie Recommendation -- 8/8
------------------------------------------------------------------------------------------
This step will be described in more detail in our full paper.",4137089,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,Therefore we collect user tweets about cold-movies on Twitter for suffi- cient movie reviews.,Cold-start Movie Recommendation,"In [5], Andrew et al. published the first large movie reviews dataset which contains 50,000 reviews with corresponded rating information. ",Dataset,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 26/40
==========================================================================================
4. Dataset -- 1/4
------------------------------------------------------------------------------------------
In [5], Andrew et al. published the first large movie reviews dataset which contains 50,000 reviews with corresponded rating information. ",4137090,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,This step will be described in more detail in our full paper.,Cold-start Movie Recommendation,"However, there is no information about holder (a person that wrote the review) for the related reviews.",Dataset,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 27/40
==========================================================================================
4. Dataset -- 2/4
------------------------------------------------------------------------------------------
However, there is no information about holder (a person that wrote the review) for the related reviews.",4137091,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In [5], Andrew et al. published the first large movie reviews dataset which contains 50,000 reviews with corresponded rating information. ",Dataset,"Therefore, by using the IMDb URLs that attached along with the corpus, we re-collected data information that including reviews, rating, and the holder identity.",Dataset,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 28/40
==========================================================================================
4. Dataset -- 3/4
------------------------------------------------------------------------------------------
Therefore, by using the IMDb URLs that attached along with the corpus, we re-collected data information that including reviews, rating, and the holder identity.",4137092,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"However, there is no information about holder (a person that wrote the review) for the related reviews.",Dataset,The statistic of the collected dataset is shown in figure 2.,Dataset,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 29/40
==========================================================================================
4. Dataset -- 4/4
------------------------------------------------------------------------------------------
The statistic of the collected dataset is shown in figure 2.",4137093,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, by using the IMDb URLs that attached along with the corpus, we re-collected data information that including reviews, rating, and the holder identity.",Dataset,The figure 3 shows three first preferred features of movie reviews in the dataset.,Exploiting Movie Preferred Features,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 30/40
==========================================================================================
5. Exploiting Movie Preferred Features -- 1/3
------------------------------------------------------------------------------------------
The figure 3 shows three first preferred features of movie reviews in the dataset.",4137094,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The statistic of the collected dataset is shown in figure 2.,Dataset,"In here, LDA with Gibb sampling of Phan et al.",Exploiting Movie Preferred Features,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 31/40
==========================================================================================
5. Exploiting Movie Preferred Features -- 2/3
------------------------------------------------------------------------------------------
In here, LDA with Gibb sampling of Phan et al.",4137095,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The figure 3 shows three first preferred features of movie reviews in the dataset.,Exploiting Movie Preferred Features,[6] is employed to exploit movie preferred features with 50 topics in 1000 iterations.,Exploiting Movie Preferred Features,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 32/40
==========================================================================================
5. Exploiting Movie Preferred Features -- 3/3
------------------------------------------------------------------------------------------
[6] is employed to exploit movie preferred features with 50 topics in 1000 iterations.",4137096,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In here, LDA with Gibb sampling of Phan et al.",Exploiting Movie Preferred Features,The dataset as described in5.1 is used to perform the experiment.,Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 33/40
==========================================================================================
6. Evaluation Methodology -- 1/8
------------------------------------------------------------------------------------------
The dataset as described in5.1 is used to perform the experiment.",4137097,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,[6] is employed to exploit movie preferred features with 50 topics in 1000 iterations.,Exploiting Movie Preferred Features,"Given that a user has a target movies to watch (which is her/his target choices), the experimental goal is to evaluate whether the target choices can be located in the recommendation list when being presented to her/him.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 34/40
==========================================================================================
6. Evaluation Methodology -- 2/8
------------------------------------------------------------------------------------------
Given that a user has a target movies to watch (which is her/his target choices), the experimental goal is to evaluate whether the target choices can be located in the recommendation list when being presented to her/him.",4137098,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,The dataset as described in5.1 is used to perform the experiment.,Evaluation Methodology,"For this goal, we concretely adopted the leave-one out evaluation scheme [3].",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 35/40
==========================================================================================
6. Evaluation Methodology -- 3/8
------------------------------------------------------------------------------------------
For this goal, we concretely adopted the leave-one out evaluation scheme [3].",4137099,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Given that a user has a target movies to watch (which is her/his target choices), the experimental goal is to evaluate whether the target choices can be located in the recommendation list when being presented to her/him.",Evaluation Methodology,"That is, during each round, we excluded one reviewer from the dataset and performed testing on it.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 36/40
==========================================================================================
6. Evaluation Methodology -- 4/8
------------------------------------------------------------------------------------------
That is, during each round, we excluded one reviewer from the dataset and performed testing on it.",4137100,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"For this goal, we concretely adopted the leave-one out evaluation scheme [3].",Evaluation Methodology,"As a matter of fact, the excluded reviewer must satisfy two ""new user"" selection criteria, so that the product purchased by the reviewer can be taken as the new user's target choice when measuring the algorithm's recommendation accuracy: 1) s/he has top number of reviews over the other users, and 2) her/his rating on the watched movies is positive marks (i.e., from 7 to 10), indicating that s/he likes the movies.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 37/40
==========================================================================================
6. Evaluation Methodology -- 5/8
------------------------------------------------------------------------------------------
As a matter of fact, the excluded reviewer must satisfy two ""new user"" selection criteria, so that the product purchased by the reviewer can be taken as the new user's target choice when measuring the algorithm's recommendation accuracy: 1) s/he has top number of reviews over the other users, and 2) her/his rating on the watched movies is positive marks (i.e., from 7 to 10), indicating that s/he likes the movies.",4137101,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"That is, during each round, we excluded one reviewer from the dataset and performed testing on it.",Evaluation Methodology,"In our dataset, we designed to select 1000 reviewers with the above criteria.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 38/40
==========================================================================================
6. Evaluation Methodology -- 6/8
------------------------------------------------------------------------------------------
In our dataset, we designed to select 1000 reviewers with the above criteria.",4137102,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"As a matter of fact, the excluded reviewer must satisfy two ""new user"" selection criteria, so that the product purchased by the reviewer can be taken as the new user's target choice when measuring the algorithm's recommendation accuracy: 1) s/he has top number of reviews over the other users, and 2) her/his rating on the watched movies is positive marks (i.e., from 7 to 10), indicating that s/he likes the movies.",Evaluation Methodology,"Therefore, at a time, one of them will be randomly chosen to behave as a new user.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 39/40
==========================================================================================
6. Evaluation Methodology -- 7/8
------------------------------------------------------------------------------------------
Therefore, at a time, one of them will be randomly chosen to behave as a new user.",4137103,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In our dataset, we designed to select 1000 reviewers with the above criteria.",Evaluation Methodology,"We further randomly select subsets of the reviewer's full feature preferences (i.e., 40%, 60%, 80% and 100%) to represent the new user's various preference completeness levels.",Evaluation Methodology,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
3. Proposed Framework -- 40/40
==========================================================================================
6. Evaluation Methodology -- 8/8
------------------------------------------------------------------------------------------
We further randomly select subsets of the reviewer's full feature preferences (i.e., 40%, 60%, 80% and 100%) to represent the new user's various preference completeness levels.",4137104,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Therefore, at a time, one of them will be randomly chosen to behave as a new user.",Evaluation Methodology,"In this paper, we have presented a thorough investigation of using user reviews for recommendation task.",Conclusion,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
4. Conclusion -- 1/5
==========================================================================================
In this paper, we have presented a thorough investigation of using user reviews for recommendation task.",4137105,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"We further randomly select subsets of the reviewer's full feature preferences (i.e., 40%, 60%, 80% and 100%) to represent the new user's various preference completeness levels.",Evaluation Methodology,"Our key contributions of the work is four-fold including (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb.",Conclusion,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
4. Conclusion -- 2/5
==========================================================================================
Our key contributions of the work is four-fold including (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb.",4137106,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"In this paper, we have presented a thorough investigation of using user reviews for recommendation task.",Conclusion,"About future study, firstly we will perform the evaluation method to proof our approach.",Conclusion,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
4. Conclusion -- 3/5
==========================================================================================
About future study, firstly we will perform the evaluation method to proof our approach.",4137107,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Our key contributions of the work is four-fold including (1) proposing a method to determine user opinion on aspects, (2) solving cold-start item recommendation by using Social Network Analysis to enrich item information, (3) applying collaborative filtering into cold-start user problem by finding the co-related opinion between massive users, and (4) publishing a first large review dataset with rating and comments of large anonymous reviewers on IMDb.",Conclusion,"Secondly, we manage to propose a generative model for exploiting user preferred features and movie preferred features at once.",Conclusion,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
4. Conclusion -- 4/5
==========================================================================================
Secondly, we manage to propose a generative model for exploiting user preferred features and movie preferred features at once.",4137108,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"About future study, firstly we will perform the evaluation method to proof our approach.",Conclusion,So that we can reduce the complexity of referring steps and exploit features in advance.,Conclusion,
"Mining User/Movie Preferred Features Based on Reviews for Video  Recommendation System
==========================================================================================
4. Conclusion -- 5/5
==========================================================================================
So that we can reduce the complexity of referring steps and exploit features in advance.",4137109,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","0. abstract
1. Introduction
2. Related Work
3. Proposed Framework
4. Conclusion
",2017,"Secondly, we manage to propose a generative model for exploiting user preferred features and movie preferred features at once.",Conclusion,,,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 1/6
==========================================================================================
  Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model.",5900935,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,,,"We focus on the problem of training these models on distantly-labeled data, which is generated by aligning entity mentions in a text corpus with their corresponding entity and relation types in a knowledge base.",abstract,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 2/6
==========================================================================================
We focus on the problem of training these models on distantly-labeled data, which is generated by aligning entity mentions in a text corpus with their corresponding entity and relation types in a knowledge base.",5900936,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,  Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model.,abstract,"One key challenge here is the presence of noisy labels, which arises from both entity and relation annotations, and significantly impair the effectiveness of supervised learning applications. ",abstract,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 3/6
==========================================================================================
One key challenge here is the presence of noisy labels, which arises from both entity and relation annotations, and significantly impair the effectiveness of supervised learning applications. ",5900937,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We focus on the problem of training these models on distantly-labeled data, which is generated by aligning entity mentions in a text corpus with their corresponding entity and relation types in a knowledge base.",abstract,"However, existing research primarily addresses only one type of noise, thereby limiting the effectiveness of noise reduction.",abstract,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 4/6
==========================================================================================
However, existing research primarily addresses only one type of noise, thereby limiting the effectiveness of noise reduction.",5900938,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"One key challenge here is the presence of noisy labels, which arises from both entity and relation annotations, and significantly impair the effectiveness of supervised learning applications. ",abstract,"To fill this gap, we introduce a new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a sequence tagging scheme for simultaneous entity and relation detection, and 2)~employs a noise-robust learning framework which includes a new loss function that penalizes inconsistency with both significant relation patterns and entity-relation dependencies, as well as a self-adaptive learning step that iteratively selects and trains on high-quality instances.",abstract,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 5/6
==========================================================================================
To fill this gap, we introduce a new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a sequence tagging scheme for simultaneous entity and relation detection, and 2)~employs a noise-robust learning framework which includes a new loss function that penalizes inconsistency with both significant relation patterns and entity-relation dependencies, as well as a self-adaptive learning step that iteratively selects and trains on high-quality instances.",5900939,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"However, existing research primarily addresses only one type of noise, thereby limiting the effectiveness of noise reduction.",abstract,Experiments on two datasets show that our method outperforms the existing state-of-the-art methods in both joint extraction performance and noise reduction effect. ,abstract,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
0. abstract -- 6/6
==========================================================================================
Experiments on two datasets show that our method outperforms the existing state-of-the-art methods in both joint extraction performance and noise reduction effect. ",5900940,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To fill this gap, we introduce a new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a sequence tagging scheme for simultaneous entity and relation detection, and 2)~employs a noise-robust learning framework which includes a new loss function that penalizes inconsistency with both significant relation patterns and entity-relation dependencies, as well as a self-adaptive learning step that iteratively selects and trains on high-quality instances.",abstract,"Joint extraction aims to detect entities along with their relations using a single model (see Figure 1), which is a critical step in automatic knowledge base construction (Yu et al., 2020).",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 1/23
==========================================================================================
Joint extraction aims to detect entities along with their relations using a single model (see Figure 1), which is a critical step in automatic knowledge base construction (Yu et al., 2020).",5900941,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,Experiments on two datasets show that our method outperforms the existing state-of-the-art methods in both joint extraction performance and noise reduction effect. ,abstract,"In order to cheaply acquire a large amount of labeled joint training data, distant supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning knowledge base (KB) with an unlabeled corpus.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 2/23
==========================================================================================
In order to cheaply acquire a large amount of labeled joint training data, distant supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning knowledge base (KB) with an unlabeled corpus.",5900942,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Joint extraction aims to detect entities along with their relations using a single model (see Figure 1), which is a critical step in automatic knowledge base construction (Yu et al., 2020).",Introduction,"It assumes that if an entity pair have a relationship in a KB, all sentences that contain this pair express the corresponding relation.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 3/23
==========================================================================================
It assumes that if an entity pair have a relationship in a KB, all sentences that contain this pair express the corresponding relation.",5900943,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In order to cheaply acquire a large amount of labeled joint training data, distant supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning knowledge base (KB) with an unlabeled corpus.",Introduction,"Nevertheless, DS brings plenty of noisy labels which significantly degrade the performance of the joint extraction models.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 4/23
==========================================================================================
Nevertheless, DS brings plenty of noisy labels which significantly degrade the performance of the joint extraction models.",5900944,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"It assumes that if an entity pair have a relationship in a KB, all sentences that contain this pair express the corresponding relation.",Introduction,"For example, given a sentence ""Bill Gates lived in Albuquerque"" and the sentence in Figure 1, DS may assign the relation type between ""Bill Gates"" and ""Albuquerque"" as Place_lived for both sentences.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 5/23
==========================================================================================
For example, given a sentence ""Bill Gates lived in Albuquerque"" and the sentence in Figure 1, DS may assign the relation type between ""Bill Gates"" and ""Albuquerque"" as Place_lived for both sentences.",5900945,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Nevertheless, DS brings plenty of noisy labels which significantly degrade the performance of the joint extraction models.",Introduction,"The words ""lived in"" in the first sentence is the pattern that explains the relation type, thus it is correctly labeled.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 6/23
==========================================================================================
The words ""lived in"" in the first sentence is the pattern that explains the relation type, thus it is correctly labeled.",5900946,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For example, given a sentence ""Bill Gates lived in Albuquerque"" and the sentence in Figure 1, DS may assign the relation type between ""Bill Gates"" and ""Albuquerque"" as Place_lived for both sentences.",Introduction,While the second sentence is noisy due to the lack of corresponding relation pattern.,Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 7/23
==========================================================================================
While the second sentence is noisy due to the lack of corresponding relation pattern.",5900947,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The words ""lived in"" in the first sentence is the pattern that explains the relation type, thus it is correctly labeled.",Introduction,"Moreover, due to the ambiguity and limited coverage over entities in open-domain KBs, DS also generates noisy and incomplete entity labels.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 8/23
==========================================================================================
Moreover, due to the ambiguity and limited coverage over entities in open-domain KBs, DS also generates noisy and incomplete entity labels.",5900948,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,While the second sentence is noisy due to the lack of corresponding relation pattern.,Introduction,"In some cases, DS may lead to over 30% noisy instances (Mintz et al., 2009), making it impossible to learn useful features. ",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 9/23
==========================================================================================
In some cases, DS may lead to over 30% noisy instances (Mintz et al., 2009), making it impossible to learn useful features. ",5900949,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Moreover, due to the ambiguity and limited coverage over entities in open-domain KBs, DS also generates noisy and incomplete entity labels.",Introduction,"Previous studies for handling such noisy labels consider either weakly-labeled entities, i.e., distantly-supervised named entity recognition (NER) (Shaalan, 2014), or noisy relation labels, i.e., distantly-supervised relation extraction (RE) (Rink and Harabagiu, 2010), where they focus on designing novel hand-crafted relation features (Yu et al., 2020), neural architectures (Chen et al., 2020), and tagging scheme (Dai et al., 2019) to improve relation extraction performance. ",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 10/23
==========================================================================================
Previous studies for handling such noisy labels consider either weakly-labeled entities, i.e., distantly-supervised named entity recognition (NER) (Shaalan, 2014), or noisy relation labels, i.e., distantly-supervised relation extraction (RE) (Rink and Harabagiu, 2010), where they focus on designing novel hand-crafted relation features (Yu et al., 2020), neural architectures (Chen et al., 2020), and tagging scheme (Dai et al., 2019) to improve relation extraction performance. ",5900950,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In some cases, DS may lead to over 30% noisy instances (Mintz et al., 2009), making it impossible to learn useful features. ",Introduction,"To alleviate both noise sources, we propose DENRL-Distantly-supervised joint Extraction with Noise-Robust Learning.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 11/23
==========================================================================================
To alleviate both noise sources, we propose DENRL-Distantly-supervised joint Extraction with Noise-Robust Learning.",5900951,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Previous studies for handling such noisy labels consider either weakly-labeled entities, i.e., distantly-supervised named entity recognition (NER) (Shaalan, 2014), or noisy relation labels, i.e., distantly-supervised relation extraction (RE) (Rink and Harabagiu, 2010), where they focus on designing novel hand-crafted relation features (Yu et al., 2020), neural architectures (Chen et al., 2020), and tagging scheme (Dai et al., 2019) to improve relation extraction performance. ",Introduction,"DENRL assumes that 1) reliable relation labels, whose relation pat-terns significantly indicate the relationship between entity pairs, should be explained by a model, and 2) reliable relation labels also implicitly indicate reliable entity tags of the corresponding entity pairs.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 12/23
==========================================================================================
DENRL assumes that 1) reliable relation labels, whose relation pat-terns significantly indicate the relationship between entity pairs, should be explained by a model, and 2) reliable relation labels also implicitly indicate reliable entity tags of the corresponding entity pairs.",5900952,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To alleviate both noise sources, we propose DENRL-Distantly-supervised joint Extraction with Noise-Robust Learning.",Introduction,"Specifically, DENRL applies Bag-Of-Word (BOW) regularization to guide a model to attend to significant relation patterns which explain correct relation labels, and logic fusion that teaches underlying entity-relation dependencies with open-domain Probabilistic Soft Logic (PSL) rules (Bach et al., 2017).",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 13/23
==========================================================================================
Specifically, DENRL applies Bag-Of-Word (BOW) regularization to guide a model to attend to significant relation patterns which explain correct relation labels, and logic fusion that teaches underlying entity-relation dependencies with open-domain Probabilistic Soft Logic (PSL) rules (Bach et al., 2017).",5900953,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"DENRL assumes that 1) reliable relation labels, whose relation pat-terns significantly indicate the relationship between entity pairs, should be explained by a model, and 2) reliable relation labels also implicitly indicate reliable entity tags of the corresponding entity pairs.",Introduction,"These two information sources are integrated to form a noise-robust loss, which regularizes a joint model to learn from high-quality instances with reliable entity and relation labels. ",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 14/23
==========================================================================================
These two information sources are integrated to form a noise-robust loss, which regularizes a joint model to learn from high-quality instances with reliable entity and relation labels. ",5900954,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Specifically, DENRL applies Bag-Of-Word (BOW) regularization to guide a model to attend to significant relation patterns which explain correct relation labels, and logic fusion that teaches underlying entity-relation dependencies with open-domain Probabilistic Soft Logic (PSL) rules (Bach et al., 2017).",Introduction,"Next, if a learned model clearly locates the relation patterns and understands entity-relation logic of candidate instances, it will select these instances and additional negative instances with corresponding head or tail entities of their recognized patterns for future adaptive learning.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 15/23
==========================================================================================
Next, if a learned model clearly locates the relation patterns and understands entity-relation logic of candidate instances, it will select these instances and additional negative instances with corresponding head or tail entities of their recognized patterns for future adaptive learning.",5900955,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"These two information sources are integrated to form a noise-robust loss, which regularizes a joint model to learn from high-quality instances with reliable entity and relation labels. ",Introduction,We iteratively learn an interpretable model and select high-quality instances.,Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 16/23
==========================================================================================
We iteratively learn an interpretable model and select high-quality instances.",5900956,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Next, if a learned model clearly locates the relation patterns and understands entity-relation logic of candidate instances, it will select these instances and additional negative instances with corresponding head or tail entities of their recognized patterns for future adaptive learning.",Introduction,These two-fold steps are mutually reinforced.,Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 17/23
==========================================================================================
These two-fold steps are mutually reinforced.",5900957,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We iteratively learn an interpretable model and select high-quality instances.,Introduction,"The more interpretable our model is, the better training instances are selected, and vice versa. ",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 18/23
==========================================================================================
The more interpretable our model is, the better training instances are selected, and vice versa. ",5900958,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,These two-fold steps are mutually reinforced.,Introduction,"Given the superiority of unified joint extraction methods, we introduce a sequence labeling (Zheng et al., 2017) method to tag entities and their relations simultaneously as token classification.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 19/23
==========================================================================================
Given the superiority of unified joint extraction methods, we introduce a sequence labeling (Zheng et al., 2017) method to tag entities and their relations simultaneously as token classification.",5900959,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The more interpretable our model is, the better training instances are selected, and vice versa. ",Introduction,"We incorporate a GPT-2 transformer (Radford et al., 2019) backbone that learns rich feature representations into the tagging scheme to benefit the information propagation between relations and entities.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 20/23
==========================================================================================
We incorporate a GPT-2 transformer (Radford et al., 2019) backbone that learns rich feature representations into the tagging scheme to benefit the information propagation between relations and entities.",5900960,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Given the superiority of unified joint extraction methods, we introduce a sequence labeling (Zheng et al., 2017) method to tag entities and their relations simultaneously as token classification.",Introduction,"The transformer attention mechanism builds direct connection between words and contributes to extracting long-range relations (Li et al., 2022(Li et al., , 2023c)).",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 21/23
==========================================================================================
The transformer attention mechanism builds direct connection between words and contributes to extracting long-range relations (Li et al., 2022(Li et al., , 2023c)).",5900961,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We incorporate a GPT-2 transformer (Radford et al., 2019) backbone that learns rich feature representations into the tagging scheme to benefit the information propagation between relations and entities.",Introduction,"Its multi-head attention weights indicate interactions between each pair of words, which is further leveraged by self-matching to produce position-aware representations.",Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 22/23
==========================================================================================
Its multi-head attention weights indicate interactions between each pair of words, which is further leveraged by self-matching to produce position-aware representations.",5900962,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The transformer attention mechanism builds direct connection between words and contributes to extracting long-range relations (Li et al., 2022(Li et al., , 2023c)).",Introduction,These representations are finally used to decode different tagging results and extract all entities together with their relations.,Introduction,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
1. Introduction -- 23/23
==========================================================================================
These representations are finally used to decode different tagging results and extract all entities together with their relations.",5900963,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Its multi-head attention weights indicate interactions between each pair of words, which is further leveraged by self-matching to produce position-aware representations.",Introduction,We incorporate a pre-trained GPT-2 backbone into our sequence tagging scheme to jointly extract entities and their relations (see Figure 3).,The DENRL Framework,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 1/125
==========================================================================================
We incorporate a pre-trained GPT-2 backbone into our sequence tagging scheme to jointly extract entities and their relations (see Figure 3).",5900964,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,These representations are finally used to decode different tagging results and extract all entities together with their relations.,Introduction,"To reduce the impact of noisy labels on tagging performance, we introduce BOW regularization and ontologybased logic fusion to guide the model to attend to confident relation patterns and entity-relation dependencies.",The DENRL Framework,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 2/125
==========================================================================================
To reduce the impact of noisy labels on tagging performance, we introduce BOW regularization and ontologybased logic fusion to guide the model to attend to confident relation patterns and entity-relation dependencies.",5900965,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We incorporate a pre-trained GPT-2 backbone into our sequence tagging scheme to jointly extract entities and their relations (see Figure 3).,The DENRL Framework,"Finally, we employ self-adaptive learning to iteratively train on instances that can be explained by the model.",The DENRL Framework,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 3/125
==========================================================================================
Finally, we employ self-adaptive learning to iteratively train on instances that can be explained by the model.",5900966,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To reduce the impact of noisy labels on tagging performance, we introduce BOW regularization and ontologybased logic fusion to guide the model to attend to confident relation patterns and entity-relation dependencies.",The DENRL Framework,"To extract both entities (mention and type) and relations, we tag quadruplets {e 1 , tag 1 , e 2 , re} for each start position p and define ""BIO"" signs to encode positions (see Figure 2).",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 4/125
==========================================================================================
0. Tagging Scheme -- 1/13
------------------------------------------------------------------------------------------
To extract both entities (mention and type) and relations, we tag quadruplets {e 1 , tag 1 , e 2 , re} for each start position p and define ""BIO"" signs to encode positions (see Figure 2).",5900967,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Finally, we employ self-adaptive learning to iteratively train on instances that can be explained by the model.",The DENRL Framework,"Here, e 1 is the detected entity at p (head entity), tag 1 is the entity type of e 1 , e 2 is other detected entity that has relationship with e 1 (tail entity), and re is the predicted relation type between e 1 and e 2 .",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 5/125
==========================================================================================
0. Tagging Scheme -- 2/13
------------------------------------------------------------------------------------------
Here, e 1 is the detected entity at p (head entity), tag 1 is the entity type of e 1 , e 2 is other detected entity that has relationship with e 1 (tail entity), and re is the predicted relation type between e 1 and e 2 .",5900968,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To extract both entities (mention and type) and relations, we tag quadruplets {e 1 , tag 1 , e 2 , re} for each start position p and define ""BIO"" signs to encode positions (see Figure 2).",Tagging Scheme,"For a T -token sentence, we annotate T different tag sequences according to different start positions. ",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 6/125
==========================================================================================
0. Tagging Scheme -- 3/13
------------------------------------------------------------------------------------------
For a T -token sentence, we annotate T different tag sequences according to different start positions. ",5900969,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Here, e 1 is the detected entity at p (head entity), tag 1 is the entity type of e 1 , e 2 is other detected entity that has relationship with e 1 (tail entity), and re is the predicted relation type between e 1 and e 2 .",Tagging Scheme,"For each tag sequence, if p is the start of an entity (this sequence is an instance), the entity type is labeled at p, other entities which have relationship to the entity at p are labeled with relation types.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 7/125
==========================================================================================
0. Tagging Scheme -- 4/13
------------------------------------------------------------------------------------------
For each tag sequence, if p is the start of an entity (this sequence is an instance), the entity type is labeled at p, other entities which have relationship to the entity at p are labeled with relation types.",5900970,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For a T -token sentence, we annotate T different tag sequences according to different start positions. ",Tagging Scheme,"The rest of tokens are labeled ""O"" (Outside), meaning they do not correspond to the head entity.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 8/125
==========================================================================================
0. Tagging Scheme -- 5/13
------------------------------------------------------------------------------------------
The rest of tokens are labeled ""O"" (Outside), meaning they do not correspond to the head entity.",5900971,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For each tag sequence, if p is the start of an entity (this sequence is an instance), the entity type is labeled at p, other entities which have relationship to the entity at p are labeled with relation types.",Tagging Scheme,"In this way, each tag sequence will produce a relation quadruplet.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 9/125
==========================================================================================
0. Tagging Scheme -- 6/13
------------------------------------------------------------------------------------------
In this way, each tag sequence will produce a relation quadruplet.",5900972,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The rest of tokens are labeled ""O"" (Outside), meaning they do not correspond to the head entity.",Tagging Scheme,"For example, if p is 7, the head entity is ""Microsoft"" and its tag is ORG.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 10/125
==========================================================================================
0. Tagging Scheme -- 7/13
------------------------------------------------------------------------------------------
For example, if p is 7, the head entity is ""Microsoft"" and its tag is ORG.",5900973,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In this way, each tag sequence will produce a relation quadruplet.",Tagging Scheme,"Other entities, such as ""Albuquerque"" and ""New Mexico"", are labeled as L_I and L_I indicating their (unidirectional) relations with ""Microsoft"".",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 11/125
==========================================================================================
0. Tagging Scheme -- 8/13
------------------------------------------------------------------------------------------
Other entities, such as ""Albuquerque"" and ""New Mexico"", are labeled as L_I and L_I indicating their (unidirectional) relations with ""Microsoft"".",5900974,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For example, if p is 7, the head entity is ""Microsoft"" and its tag is ORG.",Tagging Scheme,"If p is 9, the head entity ""Albuquerque"" has no relationship with other entities, thus only the entity type LOC is labeled.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 12/125
==========================================================================================
0. Tagging Scheme -- 9/13
------------------------------------------------------------------------------------------
If p is 9, the head entity ""Albuquerque"" has no relationship with other entities, thus only the entity type LOC is labeled.",5900975,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Other entities, such as ""Albuquerque"" and ""New Mexico"", are labeled as L_I and L_I indicating their (unidirectional) relations with ""Microsoft"".",Tagging Scheme,"If p is 13, all tokens are labeled as ""O"" because there is no entity at the head position to attend to. ",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 13/125
==========================================================================================
0. Tagging Scheme -- 10/13
------------------------------------------------------------------------------------------
If p is 13, all tokens are labeled as ""O"" because there is no entity at the head position to attend to. ",5900976,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"If p is 9, the head entity ""Albuquerque"" has no relationship with other entities, thus only the entity type LOC is labeled.",Tagging Scheme,"We define instances that contain at least one relation as positive instances (e.g., p is 7), and those without relations as negative instances (e.g., p is 9).",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 14/125
==========================================================================================
0. Tagging Scheme -- 11/13
------------------------------------------------------------------------------------------
We define instances that contain at least one relation as positive instances (e.g., p is 7), and those without relations as negative instances (e.g., p is 9).",5900977,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"If p is 13, all tokens are labeled as ""O"" because there is no entity at the head position to attend to. ",Tagging Scheme,"""BIO"" (Begin, Inside, Outside) signs are used to indicate the position information of tokens in each entity for both entity and relation type annotation to extract multi-word entities.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 15/125
==========================================================================================
0. Tagging Scheme -- 12/13
------------------------------------------------------------------------------------------
""BIO"" (Begin, Inside, Outside) signs are used to indicate the position information of tokens in each entity for both entity and relation type annotation to extract multi-word entities.",5900978,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We define instances that contain at least one relation as positive instances (e.g., p is 7), and those without relations as negative instances (e.g., p is 9).",Tagging Scheme,"Note that we do not  need the tail entity type, because every entity will be queried and we are able to obtain all entity types as well as their relations from the T tag sequences.",Tagging Scheme,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 16/125
==========================================================================================
0. Tagging Scheme -- 13/13
------------------------------------------------------------------------------------------
Note that we do not  need the tail entity type, because every entity will be queried and we are able to obtain all entity types as well as their relations from the T tag sequences.",5900979,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"""BIO"" (Begin, Inside, Outside) signs are used to indicate the position information of tokens in each entity for both entity and relation type annotation to extract multi-word entities.",Tagging Scheme,"GPT-2 Decoder with Self-Matching We follow GPT-2 (Radford et al., 2019) to use a multilayer transformer (Vaswani et al., 2017) that takes an input sequence S = {w 1 , ...w T } and converts it into token-level representations h 0",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 17/125
==========================================================================================
1. Joint Extraction Model -- 1/22
------------------------------------------------------------------------------------------
GPT-2 Decoder with Self-Matching We follow GPT-2 (Radford et al., 2019) to use a multilayer transformer (Vaswani et al., 2017) that takes an input sequence S = {w 1 , ...w T } and converts it into token-level representations h 0",5900980,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Note that we do not  need the tail entity type, because every entity will be queried and we are able to obtain all entity types as well as their relations from the T tag sequences.",Tagging Scheme,"= {h t } T t=1 , where h t ∈ R d is a d-dimensional vector corresponding to the t-th token in S.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 18/125
==========================================================================================
1. Joint Extraction Model -- 2/22
------------------------------------------------------------------------------------------
= {h t } T t=1 , where h t ∈ R d is a d-dimensional vector corresponding to the t-th token in S.",5900981,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"GPT-2 Decoder with Self-Matching We follow GPT-2 (Radford et al., 2019) to use a multilayer transformer (Vaswani et al., 2017) that takes an input sequence S = {w 1 , ...w T } and converts it into token-level representations h 0",Joint Extraction Model,"The model applies L transformer layers over the hidden vectors to produce contextual representations h l = Transformer l (h l−1 ), l ∈ [1, L].",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 19/125
==========================================================================================
1. Joint Extraction Model -- 3/22
------------------------------------------------------------------------------------------
The model applies L transformer layers over the hidden vectors to produce contextual representations h l = Transformer l (h l−1 ), l ∈ [1, L].",5900982,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"= {h t } T t=1 , where h t ∈ R d is a d-dimensional vector corresponding to the t-th token in S.",Joint Extraction Model,Each layer contains a multihead self-attention operation followed by a feed-forward network (FFN) over previous hidden vector h l−1 .,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 20/125
==========================================================================================
1. Joint Extraction Model -- 4/22
------------------------------------------------------------------------------------------
Each layer contains a multihead self-attention operation followed by a feed-forward network (FFN) over previous hidden vector h l−1 .",5900983,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The model applies L transformer layers over the hidden vectors to produce contextual representations h l = Transformer l (h l−1 ), l ∈ [1, L].",Joint Extraction Model,"The final representations h L ∈ R T ×d generally integrate the contextual information of the whole sentence but are inadequate for decoding a T -tag sequence, because for each position p we also need to encode e 1 and its overlapping relations re with other entities e 2 . ",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 21/125
==========================================================================================
1. Joint Extraction Model -- 5/22
------------------------------------------------------------------------------------------
The final representations h L ∈ R T ×d generally integrate the contextual information of the whole sentence but are inadequate for decoding a T -tag sequence, because for each position p we also need to encode e 1 and its overlapping relations re with other entities e 2 . ",5900984,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,Each layer contains a multihead self-attention operation followed by a feed-forward network (FFN) over previous hidden vector h l−1 .,Joint Extraction Model,"We define self-matching (Tan et al., 2018) that calculates position-attention a t between tokens at start position p as well as each target position t: a t = softmax( a t j T j=1 ) s.t.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 22/125
==========================================================================================
1. Joint Extraction Model -- 6/22
------------------------------------------------------------------------------------------
We define self-matching (Tan et al., 2018) that calculates position-attention a t between tokens at start position p as well as each target position t: a t = softmax( a t j T j=1 ) s.t.",5900985,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The final representations h L ∈ R T ×d generally integrate the contextual information of the whole sentence but are inadequate for decoding a T -tag sequence, because for each position p we also need to encode e 1 and its overlapping relations re with other entities e 2 . ",Joint Extraction Model,a t j = w ⊤ (h L p + h L t + h L j ) where w ∈ R d is a parameter to be learned,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 23/125
==========================================================================================
1. Joint Extraction Model -- 7/22
------------------------------------------------------------------------------------------
a t j = w ⊤ (h L p + h L t + h L j ) where w ∈ R d is a parameter to be learned",5900986,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We define self-matching (Tan et al., 2018) that calculates position-attention a t between tokens at start position p as well as each target position t: a t = softmax( a t j T j=1 ) s.t.",Joint Extraction Model,", h p , h t , h j ∈ R d are hidden states at position p, t, j, re-spectively.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 24/125
==========================================================================================
1. Joint Extraction Model -- 8/22
------------------------------------------------------------------------------------------
, h p , h t , h j ∈ R d are hidden states at position p, t, j, re-spectively.",5900987,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,a t j = w ⊤ (h L p + h L t + h L j ) where w ∈ R d is a parameter to be learned,Joint Extraction Model,a t j is the score computed by comparing h p and h t with each hidden state h j .,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 25/125
==========================================================================================
1. Joint Extraction Model -- 9/22
------------------------------------------------------------------------------------------
a t j is the score computed by comparing h p and h t with each hidden state h j .",5900988,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,", h p , h t , h j ∈ R d are hidden states at position p, t, j, re-spectively.",Joint Extraction Model,a t ∈ R T is the softmax attention produced by normalizing a t j .,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 26/125
==========================================================================================
1. Joint Extraction Model -- 10/22
------------------------------------------------------------------------------------------
a t ∈ R T is the softmax attention produced by normalizing a t j .",5900989,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,a t j is the score computed by comparing h p and h t with each hidden state h j .,Joint Extraction Model,"The start hidden state h p serves as comparing with the sentence representations to encode position information, and h t matches the sentence representations against itself to collect context information.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 27/125
==========================================================================================
1. Joint Extraction Model -- 11/22
------------------------------------------------------------------------------------------
The start hidden state h p serves as comparing with the sentence representations to encode position information, and h t matches the sentence representations against itself to collect context information.",5900990,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,a t ∈ R T is the softmax attention produced by normalizing a t j .,Joint Extraction Model,The position-aware representation m t ∈ R T ×d is an attention-weighted sentence vector: ,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 28/125
==========================================================================================
1. Joint Extraction Model -- 12/22
------------------------------------------------------------------------------------------
The position-aware representation m t ∈ R T ×d is an attention-weighted sentence vector: ",5900991,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The start hidden state h p serves as comparing with the sentence representations to encode position information, and h t matches the sentence representations against itself to collect context information.",Joint Extraction Model,We concatenate h t and m t to generate positionaware and context-aware representations {x t } T t=1 : x t = [h t ; m t ] ,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 29/125
==========================================================================================
1. Joint Extraction Model -- 13/22
------------------------------------------------------------------------------------------
We concatenate h t and m t to generate positionaware and context-aware representations {x t } T t=1 : x t = [h t ; m t ] ",5900992,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,The position-aware representation m t ∈ R T ×d is an attention-weighted sentence vector: ,Joint Extraction Model,"For each start position, self-matching produces different sentence representations and thus can model different tag sequences of a sentence. ",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 30/125
==========================================================================================
1. Joint Extraction Model -- 14/22
------------------------------------------------------------------------------------------
For each start position, self-matching produces different sentence representations and thus can model different tag sequences of a sentence. ",5900993,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We concatenate h t and m t to generate positionaware and context-aware representations {x t } T t=1 : x t = [h t ; m t ] ,Joint Extraction Model,"CRF Decoder CRF (Lafferty et al., 2001) considers the correlations between labels in neighborhoods and jointly decodes the best chain of labels, which benefits sequence labeling models.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 31/125
==========================================================================================
1. Joint Extraction Model -- 15/22
------------------------------------------------------------------------------------------
CRF Decoder CRF (Lafferty et al., 2001) considers the correlations between labels in neighborhoods and jointly decodes the best chain of labels, which benefits sequence labeling models.",5900994,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For each start position, self-matching produces different sentence representations and thus can model different tag sequences of a sentence. ",Joint Extraction Model,"For each position-aware representation x t , the input sequence scores Z = {z t } T t=1 is generated by: z t = W x x t (4) where z t ∈ R V is tag score of the t-th token, V is the number of distinct tags, and z j t is the score of the j-th tag at position t. ",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 32/125
==========================================================================================
1. Joint Extraction Model -- 16/22
------------------------------------------------------------------------------------------
For each position-aware representation x t , the input sequence scores Z = {z t } T t=1 is generated by: z t = W x x t (4) where z t ∈ R V is tag score of the t-th token, V is the number of distinct tags, and z j t is the score of the j-th tag at position t. ",5900995,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"CRF Decoder CRF (Lafferty et al., 2001) considers the correlations between labels in neighborhoods and jointly decodes the best chain of labels, which benefits sequence labeling models.",Joint Extraction Model,"For a sequence of labels y = {y 1 , ..., y T }, the decoding score score(Z, y) is the sum of transition score from tag y t to tag y t+1 , plus the input score z yt t for each token position t.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 33/125
==========================================================================================
1. Joint Extraction Model -- 17/22
------------------------------------------------------------------------------------------
For a sequence of labels y = {y 1 , ..., y T }, the decoding score score(Z, y) is the sum of transition score from tag y t to tag y t+1 , plus the input score z yt t for each token position t.",5900996,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For each position-aware representation x t , the input sequence scores Z = {z t } T t=1 is generated by: z t = W x x t (4) where z t ∈ R V is tag score of the t-th token, V is the number of distinct tags, and z j t is the score of the j-th tag at position t. ",Joint Extraction Model,"The conditional probability p(y|Z) is the softmax of score(Z, y) over all possible label sequences y ′ for Z.",Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 34/125
==========================================================================================
1. Joint Extraction Model -- 18/22
------------------------------------------------------------------------------------------
The conditional probability p(y|Z) is the softmax of score(Z, y) over all possible label sequences y ′ for Z.",5900997,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For a sequence of labels y = {y 1 , ..., y T }, the decoding score score(Z, y) is the sum of transition score from tag y t to tag y t+1 , plus the input score z yt t for each token position t.",Joint Extraction Model,We maximize the log-likelihood of correct tag sequences during training: L c,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 35/125
==========================================================================================
1. Joint Extraction Model -- 19/22
------------------------------------------------------------------------------------------
We maximize the log-likelihood of correct tag sequences during training: L c",5900998,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The conditional probability p(y|Z) is the softmax of score(Z, y) over all possible label sequences y ′ for Z.",Joint Extraction Model,Decoding searches for the tag sequence y *,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 36/125
==========================================================================================
1. Joint Extraction Model -- 20/22
------------------------------------------------------------------------------------------
Decoding searches for the tag sequence y *",5900999,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We maximize the log-likelihood of correct tag sequences during training: L c,Joint Extraction Model,that maximizes the decoding score.,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 37/125
==========================================================================================
1. Joint Extraction Model -- 21/22
------------------------------------------------------------------------------------------
that maximizes the decoding score.",5901000,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,Decoding searches for the tag sequence y *,Joint Extraction Model,The best tag sequence y * is computed using the Viterbi algorithm.,Joint Extraction Model,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 38/125
==========================================================================================
1. Joint Extraction Model -- 22/22
------------------------------------------------------------------------------------------
The best tag sequence y * is computed using the Viterbi algorithm.",5901001,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,that maximizes the decoding score.,Joint Extraction Model,"Bag-of-word Regularization (BR) Assuming reliable relation patterns are explainable to a model itself, we propose average BOW frequency as an instance-level pattern oracle to guide the model's position-attention for joint tagging.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 39/125
==========================================================================================
2. Noise-Robust Learning -- 1/41
------------------------------------------------------------------------------------------
Bag-of-word Regularization (BR) Assuming reliable relation patterns are explainable to a model itself, we propose average BOW frequency as an instance-level pattern oracle to guide the model's position-attention for joint tagging.",5901002,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,The best tag sequence y * is computed using the Viterbi algorithm.,Joint Extraction Model,"For an input sentence S, an entity pair (e 1 , e 2 ) in S, a relation label re, and a relation pattern p that explains the relation re of e 1 and e 2 , we define BOW frequency as the corresponding guidance score a p , i.e., Pattern Significance, conditional on pattern p.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 40/125
==========================================================================================
2. Noise-Robust Learning -- 2/41
------------------------------------------------------------------------------------------
For an input sentence S, an entity pair (e 1 , e 2 ) in S, a relation label re, and a relation pattern p that explains the relation re of e 1 and e 2 , we define BOW frequency as the corresponding guidance score a p , i.e., Pattern Significance, conditional on pattern p.",5901003,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Bag-of-word Regularization (BR) Assuming reliable relation patterns are explainable to a model itself, we propose average BOW frequency as an instance-level pattern oracle to guide the model's position-attention for joint tagging.",Noise-Robust Learning,"Take the relation Contains as an example, its BOW is a set of tokens {""capital"", ""section"", ""of"", ""areas"", ""in"", ...} which appear in a corresponding pattern set {""capital of"", ""section in"", ""areas of"", ...}.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 41/125
==========================================================================================
2. Noise-Robust Learning -- 3/41
------------------------------------------------------------------------------------------
Take the relation Contains as an example, its BOW is a set of tokens {""capital"", ""section"", ""of"", ""areas"", ""in"", ...} which appear in a corresponding pattern set {""capital of"", ""section in"", ""areas of"", ...}.",5901004,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For an input sentence S, an entity pair (e 1 , e 2 ) in S, a relation label re, and a relation pattern p that explains the relation re of e 1 and e 2 , we define BOW frequency as the corresponding guidance score a p , i.e., Pattern Significance, conditional on pattern p.",Noise-Robust Learning,"The motivation is to guide the model to explore new high-quality patterns such as ""section of "", ""areas in"", etc.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 42/125
==========================================================================================
2. Noise-Robust Learning -- 4/41
------------------------------------------------------------------------------------------
The motivation is to guide the model to explore new high-quality patterns such as ""section of "", ""areas in"", etc.",5901005,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Take the relation Contains as an example, its BOW is a set of tokens {""capital"", ""section"", ""of"", ""areas"", ""in"", ...} which appear in a corresponding pattern set {""capital of"", ""section in"", ""areas of"", ...}.",Noise-Robust Learning,The guidance a I for an instance I is the average of a p regarding all patterns m corresponding to each relation re in S: a p = softmax({bow t } T t=1 ),Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 43/125
==========================================================================================
2. Noise-Robust Learning -- 5/41
------------------------------------------------------------------------------------------
The guidance a I for an instance I is the average of a p regarding all patterns m corresponding to each relation re in S: a p = softmax({bow t } T t=1 )",5901006,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The motivation is to guide the model to explore new high-quality patterns such as ""section of "", ""areas in"", etc.",Noise-Robust Learning,"(6) where bow t represents the BOW frequency of w t under relation re if w t belongs to entity words or corresponding relation pattern words, e.g., f (""of""|Contains)",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 44/125
==========================================================================================
2. Noise-Robust Learning -- 6/41
------------------------------------------------------------------------------------------
(6) where bow t represents the BOW frequency of w t under relation re if w t belongs to entity words or corresponding relation pattern words, e.g., f (""of""|Contains)",5901007,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,The guidance a I for an instance I is the average of a p regarding all patterns m corresponding to each relation re in S: a p = softmax({bow t } T t=1 ),Noise-Robust Learning,I | is the number of distinct relation types in instance I. ,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 45/125
==========================================================================================
2. Noise-Robust Learning -- 7/41
------------------------------------------------------------------------------------------
I | is the number of distinct relation types in instance I. ",5901008,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"(6) where bow t represents the BOW frequency of w t under relation re if w t belongs to entity words or corresponding relation pattern words, e.g., f (""of""|Contains)",Noise-Robust Learning,"We expect a joint tagger to approximate its position-attention a S to a I , where a S = AvgPooling (a 1 , . .",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 46/125
==========================================================================================
2. Noise-Robust Learning -- 8/41
------------------------------------------------------------------------------------------
We expect a joint tagger to approximate its position-attention a S to a I , where a S = AvgPooling (a 1 , . .",5901009,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,I | is the number of distinct relation types in instance I. ,Noise-Robust Learning,", a T ) is the average pooling of model's position-attention a t defined in Equation (1) for each position j in S. We apply Mean Squared Error (MSE) as the optimized function: L BR = MSE(a I , a S )",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 47/125
==========================================================================================
2. Noise-Robust Learning -- 9/41
------------------------------------------------------------------------------------------
, a T ) is the average pooling of model's position-attention a t defined in Equation (1) for each position j in S. We apply Mean Squared Error (MSE) as the optimized function: L BR = MSE(a I , a S )",5901010,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We expect a joint tagger to approximate its position-attention a S to a I , where a S = AvgPooling (a 1 , . .",Noise-Robust Learning,"= (a I − a S ) 2 (7) Ontology-Based Logic Fusion (OLF) Probabilistic Soft Logic (PSL) (Bach et al., 2017) uses soft truth values for predicates in an interval between [0, 1], which represents our token classification probability p(y t |w t ) as a convex optimization problem.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 48/125
==========================================================================================
2. Noise-Robust Learning -- 10/41
------------------------------------------------------------------------------------------
= (a I − a S ) 2 (7) Ontology-Based Logic Fusion (OLF) Probabilistic Soft Logic (PSL) (Bach et al., 2017) uses soft truth values for predicates in an interval between [0, 1], which represents our token classification probability p(y t |w t ) as a convex optimization problem.",5901011,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,", a T ) is the average pooling of model's position-attention a t defined in Equation (1) for each position j in S. We apply Mean Squared Error (MSE) as the optimized function: L BR = MSE(a I , a S )",Noise-Robust Learning,We adapt PSL to entity-relation dependency rules according to data ontology.,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 49/125
==========================================================================================
2. Noise-Robust Learning -- 11/41
------------------------------------------------------------------------------------------
We adapt PSL to entity-relation dependency rules according to data ontology.",5901012,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"= (a I − a S ) 2 (7) Ontology-Based Logic Fusion (OLF) Probabilistic Soft Logic (PSL) (Bach et al., 2017) uses soft truth values for predicates in an interval between [0, 1], which represents our token classification probability p(y t |w t ) as a convex optimization problem.",Noise-Robust Learning,"For example, if the predicted relation type is Founder_of, the head entity type is expected to be PERSON.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 50/125
==========================================================================================
2. Noise-Robust Learning -- 12/41
------------------------------------------------------------------------------------------
For example, if the predicted relation type is Founder_of, the head entity type is expected to be PERSON.",5901013,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We adapt PSL to entity-relation dependency rules according to data ontology.,Noise-Robust Learning,Training instances that violate any of these rules are penalized to enhance comprehension of entityrelation coherence.,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 51/125
==========================================================================================
2. Noise-Robust Learning -- 13/41
------------------------------------------------------------------------------------------
Training instances that violate any of these rules are penalized to enhance comprehension of entityrelation coherence.",5901014,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For example, if the predicted relation type is Founder_of, the head entity type is expected to be PERSON.",Noise-Robust Learning,"Suppose BR guides a model to recognize confident relations, OLF further helps explore instances with reliable entity labels, especially when no relations exist in them. ",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 52/125
==========================================================================================
2. Noise-Robust Learning -- 14/41
------------------------------------------------------------------------------------------
Suppose BR guides a model to recognize confident relations, OLF further helps explore instances with reliable entity labels, especially when no relations exist in them. ",5901015,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,Training instances that violate any of these rules are penalized to enhance comprehension of entityrelation coherence.,Noise-Robust Learning,"Particularly, we define Logic Distance based on a model's softmax scores over the head entity given its predicted relation type to measure how severely it violates logic rules.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 53/125
==========================================================================================
2. Noise-Robust Learning -- 15/41
------------------------------------------------------------------------------------------
Particularly, we define Logic Distance based on a model's softmax scores over the head entity given its predicted relation type to measure how severely it violates logic rules.",5901016,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Suppose BR guides a model to recognize confident relations, OLF further helps explore instances with reliable entity labels, especially when no relations exist in them. ",Noise-Robust Learning,"For a training instance, we define an atom l as each tag and the interpretation I(l) as soft truth value for the atom.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 54/125
==========================================================================================
2. Noise-Robust Learning -- 16/41
------------------------------------------------------------------------------------------
For a training instance, we define an atom l as each tag and the interpretation I(l) as soft truth value for the atom.",5901017,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Particularly, we define Logic Distance based on a model's softmax scores over the head entity given its predicted relation type to measure how severely it violates logic rules.",Noise-Robust Learning,"For each rule r : relation → entity, the distance to satisfaction d",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 55/125
==========================================================================================
2. Noise-Robust Learning -- 17/41
------------------------------------------------------------------------------------------
For each rule r : relation → entity, the distance to satisfaction d",5901018,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For a training instance, we define an atom l as each tag and the interpretation I(l) as soft truth value for the atom.",Noise-Robust Learning,"(I) under the interpretation I is: d r (I) = max {0, I(l re ) − I(l ent )} (8) PSL determines a rule r as satisfied when the truth value of I(l re ) − I(l ent ) ≥ 0.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 56/125
==========================================================================================
2. Noise-Robust Learning -- 18/41
------------------------------------------------------------------------------------------
(I) under the interpretation I is: d r (I) = max {0, I(l re ) − I(l ent )} (8) PSL determines a rule r as satisfied when the truth value of I(l re ) − I(l ent ) ≥ 0.",5901019,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For each rule r : relation → entity, the distance to satisfaction d",Noise-Robust Learning,"For each instance I, we set l ent as (head) entity type and l re as relation type.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 57/125
==========================================================================================
2. Noise-Robust Learning -- 19/41
------------------------------------------------------------------------------------------
For each instance I, we set l ent as (head) entity type and l re as relation type.",5901020,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"(I) under the interpretation I is: d r (I) = max {0, I(l re ) − I(l ent )} (8) PSL determines a rule r as satisfied when the truth value of I(l re ) − I(l ent ) ≥ 0.",Noise-Robust Learning,"This equation indicates that the smaller I(l ent ) is, the larger penalty it has.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 58/125
==========================================================================================
2. Noise-Robust Learning -- 20/41
------------------------------------------------------------------------------------------
This equation indicates that the smaller I(l ent ) is, the larger penalty it has.",5901021,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For each instance I, we set l ent as (head) entity type and l re as relation type.",Noise-Robust Learning,We compute the distance to satisfaction for each rule r and use the smallest one as penalty because at least one rule needs to be satisfied. ,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 59/125
==========================================================================================
2. Noise-Robust Learning -- 21/41
------------------------------------------------------------------------------------------
We compute the distance to satisfaction for each rule r and use the smallest one as penalty because at least one rule needs to be satisfied. ",5901022,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"This equation indicates that the smaller I(l ent ) is, the larger penalty it has.",Noise-Robust Learning,"We learn a distance function D(•, •) that minimizes all possible PSL rule grounding results, as described in Algorithm 1. D(•, •) should return 0 if at least one PSL rule is satisfied.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 60/125
==========================================================================================
2. Noise-Robust Learning -- 22/41
------------------------------------------------------------------------------------------
We learn a distance function D(•, •) that minimizes all possible PSL rule grounding results, as described in Algorithm 1. D(•, •) should return 0 if at least one PSL rule is satisfied.",5901023,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We compute the distance to satisfaction for each rule r and use the smallest one as penalty because at least one rule needs to be satisfied. ,Noise-Robust Learning,"The prediction probability p(y|e 1 ) over head entity e 1 is regarded as the interpretation I(l ent ) of ground atom l ent , so as p(y|e 2 ) over tail entity e 2 for I(l re ) of l re .",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 61/125
==========================================================================================
2. Noise-Robust Learning -- 23/41
------------------------------------------------------------------------------------------
The prediction probability p(y|e 1 ) over head entity e 1 is regarded as the interpretation I(l ent ) of ground atom l ent , so as p(y|e 2 ) over tail entity e 2 for I(l re ) of l re .",5901024,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We learn a distance function D(•, •) that minimizes all possible PSL rule grounding results, as described in Algorithm 1. D(•, •) should return 0 if at least one PSL rule is satisfied.",Noise-Robust Learning,"If no rules is satisfied, the distance is set as 0.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 62/125
==========================================================================================
2. Noise-Robust Learning -- 24/41
------------------------------------------------------------------------------------------
If no rules is satisfied, the distance is set as 0.",5901025,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The prediction probability p(y|e 1 ) over head entity e 1 is regarded as the interpretation I(l ent ) of ground atom l ent , so as p(y|e 2 ) over tail entity e 2 for I(l re ) of l re .",Noise-Robust Learning,"We formulate the distance to satisfaction as a regularization term to penalize inconsistent predictions: L OLF = D(R; {(p(y|e i ), ŷi )}) where p(y|e i ) is the softmax probability of z t i in Equation ( 4  d ← 0. ",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 63/125
==========================================================================================
2. Noise-Robust Learning -- 25/41
------------------------------------------------------------------------------------------
We formulate the distance to satisfaction as a regularization term to penalize inconsistent predictions: L OLF = D(R; {(p(y|e i ), ŷi )}) where p(y|e i ) is the softmax probability of z t i in Equation ( 4  d ← 0. ",5901026,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"If no rules is satisfied, the distance is set as 0.",Noise-Robust Learning,"is the sum of D(•, •) over all entity-relation pairs (e 1 , e 2 ) in instance I. We finalize a noise-robust loss function by summing up ( 5), ( 7) and ( 9): L = L c + αL BR + βL OLF ( ) where α, β are weights for balancing the two losses. ",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 64/125
==========================================================================================
2. Noise-Robust Learning -- 26/41
------------------------------------------------------------------------------------------
is the sum of D(•, •) over all entity-relation pairs (e 1 , e 2 ) in instance I. We finalize a noise-robust loss function by summing up ( 5), ( 7) and ( 9): L = L c + αL BR + βL OLF ( ) where α, β are weights for balancing the two losses. ",5901027,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We formulate the distance to satisfaction as a regularization term to penalize inconsistent predictions: L OLF = D(R; {(p(y|e i ), ŷi )}) where p(y|e i ) is the softmax probability of z t i in Equation ( 4  d ← 0. ",Noise-Robust Learning,Self-Adaptive Learning (SAL) Self-adaptive learning aims to iteratively select high-quality instances with informative relation patterns p and entity tags.,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 65/125
==========================================================================================
2. Noise-Robust Learning -- 27/41
------------------------------------------------------------------------------------------
Self-Adaptive Learning (SAL) Self-adaptive learning aims to iteratively select high-quality instances with informative relation patterns p and entity tags.",5901028,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"is the sum of D(•, •) over all entity-relation pairs (e 1 , e 2 ) in instance I. We finalize a noise-robust loss function by summing up ( 5), ( 7) and ( 9): L = L c + αL BR + βL OLF ( ) where α, β are weights for balancing the two losses. ",Noise-Robust Learning,"In each training epoch, more preciselylabeled instance are needed to guide a model to attend to informative evidence for joint extraction.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 66/125
==========================================================================================
2. Noise-Robust Learning -- 28/41
------------------------------------------------------------------------------------------
In each training epoch, more preciselylabeled instance are needed to guide a model to attend to informative evidence for joint extraction.",5901029,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,Self-Adaptive Learning (SAL) Self-adaptive learning aims to iteratively select high-quality instances with informative relation patterns p and entity tags.,Noise-Robust Learning,"For instance selection, more versatile patterns are required to select trustable data and to discover more confident relation patterns.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 67/125
==========================================================================================
2. Noise-Robust Learning -- 29/41
------------------------------------------------------------------------------------------
For instance selection, more versatile patterns are required to select trustable data and to discover more confident relation patterns.",5901030,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In each training epoch, more preciselylabeled instance are needed to guide a model to attend to informative evidence for joint extraction.",Noise-Robust Learning,"According to the attention mechanism and entity-relation logic, a trained tagger can tell the importance of each word for identifying the entity pair along with their relationship, and predict reasonable entity-relation label pairs.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 68/125
==========================================================================================
2. Noise-Robust Learning -- 30/41
------------------------------------------------------------------------------------------
According to the attention mechanism and entity-relation logic, a trained tagger can tell the importance of each word for identifying the entity pair along with their relationship, and predict reasonable entity-relation label pairs.",5901031,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For instance selection, more versatile patterns are required to select trustable data and to discover more confident relation patterns.",Noise-Robust Learning,"For an instance I, if 1) the model's attention weights do not match the target attention that explains the relation types in I, or 2) its confidence distribution over entity and relation tags violates the logic dependencies, this instance is likely a false alarm.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 69/125
==========================================================================================
2. Noise-Robust Learning -- 31/41
------------------------------------------------------------------------------------------
For an instance I, if 1) the model's attention weights do not match the target attention that explains the relation types in I, or 2) its confidence distribution over entity and relation tags violates the logic dependencies, this instance is likely a false alarm.",5901032,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"According to the attention mechanism and entity-relation logic, a trained tagger can tell the importance of each word for identifying the entity pair along with their relationship, and predict reasonable entity-relation label pairs.",Noise-Robust Learning,"We add up both BR and OLF loss for an instance I to measure its fitness u(I), i.e., how likely it is correctly labeled: u = σ(MSE(a",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 70/125
==========================================================================================
2. Noise-Robust Learning -- 32/41
------------------------------------------------------------------------------------------
We add up both BR and OLF loss for an instance I to measure its fitness u(I), i.e., how likely it is correctly labeled: u = σ(MSE(a",5901033,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For an instance I, if 1) the model's attention weights do not match the target attention that explains the relation types in I, or 2) its confidence distribution over entity and relation tags violates the logic dependencies, this instance is likely a false alarm.",Noise-Robust Learning,"I , a S ) − D(R; I)) ( ) where σ is the sigmoid function that bounds u in the range [0, 1].",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 71/125
==========================================================================================
2. Noise-Robust Learning -- 33/41
------------------------------------------------------------------------------------------
I , a S ) − D(R; I)) ( ) where σ is the sigmoid function that bounds u in the range [0, 1].",5901034,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We add up both BR and OLF loss for an instance I to measure its fitness u(I), i.e., how likely it is correctly labeled: u = σ(MSE(a",Noise-Robust Learning,"The higher u is, the more confident an instance I is.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 72/125
==========================================================================================
2. Noise-Robust Learning -- 34/41
------------------------------------------------------------------------------------------
The higher u is, the more confident an instance I is.",5901035,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"I , a S ) − D(R; I)) ( ) where σ is the sigmoid function that bounds u in the range [0, 1].",Noise-Robust Learning,We compute fitness scores for all training instances and select those whose score is larger than a predefined threshold τ . ,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 73/125
==========================================================================================
2. Noise-Robust Learning -- 35/41
------------------------------------------------------------------------------------------
We compute fitness scores for all training instances and select those whose score is larger than a predefined threshold τ . ",5901036,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The higher u is, the more confident an instance I is.",Noise-Robust Learning,"Because trustable relation labels also indicate trustable entity tags, we further consider Entity Se-lection (ES), i.e., selecting negative instances containing either the head or tail entity corresponding to each relation pattern in the selected positive candidates.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 74/125
==========================================================================================
2. Noise-Robust Learning -- 36/41
------------------------------------------------------------------------------------------
Because trustable relation labels also indicate trustable entity tags, we further consider Entity Se-lection (ES), i.e., selecting negative instances containing either the head or tail entity corresponding to each relation pattern in the selected positive candidates.",5901037,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We compute fitness scores for all training instances and select those whose score is larger than a predefined threshold τ . ,Noise-Robust Learning,"Specifically, we consider relation pattern p as the text between two entities in an instance.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 75/125
==========================================================================================
2. Noise-Robust Learning -- 37/41
------------------------------------------------------------------------------------------
Specifically, we consider relation pattern p as the text between two entities in an instance.",5901038,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Because trustable relation labels also indicate trustable entity tags, we further consider Entity Se-lection (ES), i.e., selecting negative instances containing either the head or tail entity corresponding to each relation pattern in the selected positive candidates.",Noise-Robust Learning,We build an initial trustable pattern set P by counting all patterns up and selecting the top 10% frequent (maximum 20) patterns for each relation type.,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 76/125
==========================================================================================
2. Noise-Robust Learning -- 38/41
------------------------------------------------------------------------------------------
We build an initial trustable pattern set P by counting all patterns up and selecting the top 10% frequent (maximum 20) patterns for each relation type.",5901039,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Specifically, we consider relation pattern p as the text between two entities in an instance.",Noise-Robust Learning,"Next, we redistribute the training dataset D based on P, where all positive instances that match patterns in P as well as negative instances that contain the head entity or tail entity of these patterns are retained to train the model for several epochs.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 77/125
==========================================================================================
2. Noise-Robust Learning -- 39/41
------------------------------------------------------------------------------------------
Next, we redistribute the training dataset D based on P, where all positive instances that match patterns in P as well as negative instances that contain the head entity or tail entity of these patterns are retained to train the model for several epochs.",5901040,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We build an initial trustable pattern set P by counting all patterns up and selecting the top 10% frequent (maximum 20) patterns for each relation type.,Noise-Robust Learning,"Finally, we select more diverse reliable instances according to the fitness scores over D, from which we also extract new trustable patterns and use them to enrich P. These new confident instances are used for the next training epoch.",Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 78/125
==========================================================================================
2. Noise-Robust Learning -- 40/41
------------------------------------------------------------------------------------------
Finally, we select more diverse reliable instances according to the fitness scores over D, from which we also extract new trustable patterns and use them to enrich P. These new confident instances are used for the next training epoch.",5901041,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Next, we redistribute the training dataset D based on P, where all positive instances that match patterns in P as well as negative instances that contain the head entity or tail entity of these patterns are retained to train the model for several epochs.",Noise-Robust Learning,We repeat the above procedure until the validation F1 converges.,Noise-Robust Learning,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 79/125
==========================================================================================
2. Noise-Robust Learning -- 41/41
------------------------------------------------------------------------------------------
We repeat the above procedure until the validation F1 converges.",5901042,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Finally, we select more diverse reliable instances according to the fitness scores over D, from which we also extract new trustable patterns and use them to enrich P. These new confident instances are used for the next training epoch.",Noise-Robust Learning,"We evaluate the performance of DENRL on two public datasets: (1) NYT (Riedel et al., 2010).",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 80/125
==========================================================================================
3. Datasets and Evaluation -- 1/10
------------------------------------------------------------------------------------------
We evaluate the performance of DENRL on two public datasets: (1) NYT (Riedel et al., 2010).",5901043,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We repeat the above procedure until the validation F1 converges.,Noise-Robust Learning,"We use the human-annotated test dataset (Jia et al., 2019) including 1,024 sentences with 3,280 instances and 3,880 quadruplets.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 81/125
==========================================================================================
3. Datasets and Evaluation -- 2/10
------------------------------------------------------------------------------------------
We use the human-annotated test dataset (Jia et al., 2019) including 1,024 sentences with 3,280 instances and 3,880 quadruplets.",5901044,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We evaluate the performance of DENRL on two public datasets: (1) NYT (Riedel et al., 2010).",Datasets and Evaluation,"The training data is automatically generated by DS (aligning entity pairs from Freebase with handcrafted rules), including 235k sentences with 692k instances and 353k quadruplets.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 82/125
==========================================================================================
3. Datasets and Evaluation -- 3/10
------------------------------------------------------------------------------------------
The training data is automatically generated by DS (aligning entity pairs from Freebase with handcrafted rules), including 235k sentences with 692k instances and 353k quadruplets.",5901045,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We use the human-annotated test dataset (Jia et al., 2019) including 1,024 sentences with 3,280 instances and 3,880 quadruplets.",Datasets and Evaluation,"(2) Wiki-KBP (Ling and Weld, 2012).",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 83/125
==========================================================================================
3. Datasets and Evaluation -- 4/10
------------------------------------------------------------------------------------------
(2) Wiki-KBP (Ling and Weld, 2012).",5901046,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The training data is automatically generated by DS (aligning entity pairs from Freebase with handcrafted rules), including 235k sentences with 692k instances and 353k quadruplets.",Datasets and Evaluation,"Its test set is manually annotated in 2013 KBP slot filling assessment results (Ellis et al., 2013) containing 289 sentences with 919 instances and 1092 quadruplets.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 84/125
==========================================================================================
3. Datasets and Evaluation -- 5/10
------------------------------------------------------------------------------------------
Its test set is manually annotated in 2013 KBP slot filling assessment results (Ellis et al., 2013) containing 289 sentences with 919 instances and 1092 quadruplets.",5901047,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"(2) Wiki-KBP (Ling and Weld, 2012).",Datasets and Evaluation,"The training data is generated by DS (Liu et al., 2017) including 75k sentences with 145k instances and 115k quadruplets. ",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 85/125
==========================================================================================
3. Datasets and Evaluation -- 6/10
------------------------------------------------------------------------------------------
The training data is generated by DS (Liu et al., 2017) including 75k sentences with 145k instances and 115k quadruplets. ",5901048,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Its test set is manually annotated in 2013 KBP slot filling assessment results (Ellis et al., 2013) containing 289 sentences with 919 instances and 1092 quadruplets.",Datasets and Evaluation,"We evaluate the extracted quadruplets for each sentence in terms of Precision (Prec.), Recall (Rec.), and F1.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 86/125
==========================================================================================
3. Datasets and Evaluation -- 7/10
------------------------------------------------------------------------------------------
We evaluate the extracted quadruplets for each sentence in terms of Precision (Prec.), Recall (Rec.), and F1.",5901049,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The training data is generated by DS (Liu et al., 2017) including 75k sentences with 145k instances and 115k quadruplets. ",Datasets and Evaluation,"A quadruplet {e 1 , tag 1 , e 2 , re} is marked correct if the relation type re, two entities e 1 , e 2 , and head entity type tag 1 are all matched.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 87/125
==========================================================================================
3. Datasets and Evaluation -- 8/10
------------------------------------------------------------------------------------------
A quadruplet {e 1 , tag 1 , e 2 , re} is marked correct if the relation type re, two entities e 1 , e 2 , and head entity type tag 1 are all matched.",5901050,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We evaluate the extracted quadruplets for each sentence in terms of Precision (Prec.), Recall (Rec.), and F1.",Datasets and Evaluation,"Note that negative quadruplets with ""None"" relation are also considered for evaluating prediction accuracy.",Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 88/125
==========================================================================================
3. Datasets and Evaluation -- 9/10
------------------------------------------------------------------------------------------
Note that negative quadruplets with ""None"" relation are also considered for evaluating prediction accuracy.",5901051,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"A quadruplet {e 1 , tag 1 , e 2 , re} is marked correct if the relation type re, two entities e 1 , e 2 , and head entity type tag 1 are all matched.",Datasets and Evaluation,We build a validation set by randomly sampling 10% sentences from the test set.,Datasets and Evaluation,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 89/125
==========================================================================================
3. Datasets and Evaluation -- 10/10
------------------------------------------------------------------------------------------
We build a validation set by randomly sampling 10% sentences from the test set.",5901052,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Note that negative quadruplets with ""None"" relation are also considered for evaluating prediction accuracy.",Datasets and Evaluation,We compare DENRL with the following baselines:,Baselines,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 90/125
==========================================================================================
4. Baselines -- 1/5
------------------------------------------------------------------------------------------
We compare DENRL with the following baselines:",5901053,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We build a validation set by randomly sampling 10% sentences from the test set.,Datasets and Evaluation,"LSTM-CRF (Zheng et al., 2017)  CNN+RL (Feng et al., 2018) that trains an instance selector and a CNN classifier using reinforcement learning.",Baselines,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 91/125
==========================================================================================
4. Baselines -- 2/5
------------------------------------------------------------------------------------------
LSTM-CRF (Zheng et al., 2017)  CNN+RL (Feng et al., 2018) that trains an instance selector and a CNN classifier using reinforcement learning.",5901054,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We compare DENRL with the following baselines:,Baselines,"ARNOR (Jia et al., 2019) which uses attention regularization and bootstrap learning to reduce noise for distantly-supervised RE.",Baselines,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 92/125
==========================================================================================
4. Baselines -- 3/5
------------------------------------------------------------------------------------------
ARNOR (Jia et al., 2019) which uses attention regularization and bootstrap learning to reduce noise for distantly-supervised RE.",5901055,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"LSTM-CRF (Zheng et al., 2017)  CNN+RL (Feng et al., 2018) that trains an instance selector and a CNN classifier using reinforcement learning.",Baselines,"FAN (Hao et al., 2021), an adversarial method including a transformer encoder to reduce noise for distantlysupervised RE.",Baselines,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 93/125
==========================================================================================
4. Baselines -- 4/5
------------------------------------------------------------------------------------------
FAN (Hao et al., 2021), an adversarial method including a transformer encoder to reduce noise for distantlysupervised RE.",5901056,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"ARNOR (Jia et al., 2019) which uses attention regularization and bootstrap learning to reduce noise for distantly-supervised RE.",Baselines,"SENT (Ma et al., 2021), a negative training method that selects complementary labels and re-labels the noisy instances for distantlysupervised RE.",Baselines,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 94/125
==========================================================================================
4. Baselines -- 5/5
------------------------------------------------------------------------------------------
SENT (Ma et al., 2021), a negative training method that selects complementary labels and re-labels the noisy instances for distantlysupervised RE.",5901057,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"FAN (Hao et al., 2021), an adversarial method including a transformer encoder to reduce noise for distantlysupervised RE.",Baselines,"For DENRL, we use the gpt2-medium as the sentence decoder.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 95/125
==========================================================================================
5. Implementation Details -- 1/10
------------------------------------------------------------------------------------------
For DENRL, we use the gpt2-medium as the sentence decoder.",5901058,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"SENT (Ma et al., 2021), a negative training method that selects complementary labels and re-labels the noisy instances for distantlysupervised RE.",Baselines,"For baselines using LSTM, we consider a single layer with a hidden size of 256.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 96/125
==========================================================================================
5. Implementation Details -- 2/10
------------------------------------------------------------------------------------------
For baselines using LSTM, we consider a single layer with a hidden size of 256.",5901059,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For DENRL, we use the gpt2-medium as the sentence decoder.",Implementation Details,"For baselines using pre-trained transformers, we follow their original settings.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 97/125
==========================================================================================
5. Implementation Details -- 3/10
------------------------------------------------------------------------------------------
For baselines using pre-trained transformers, we follow their original settings.",5901060,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For baselines using LSTM, we consider a single layer with a hidden size of 256.",Implementation Details,We tune hyperparameters on the validation set via grid search.,Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 98/125
==========================================================================================
5. Implementation Details -- 4/10
------------------------------------------------------------------------------------------
We tune hyperparameters on the validation set via grid search.",5901061,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For baselines using pre-trained transformers, we follow their original settings.",Implementation Details,"Specifically in regularization training, we find optimal parameters α and β as 1 and 0.5 for our considered datasets. ",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 99/125
==========================================================================================
5. Implementation Details -- 5/10
------------------------------------------------------------------------------------------
Specifically in regularization training, we find optimal parameters α and β as 1 and 0.5 for our considered datasets. ",5901062,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We tune hyperparameters on the validation set via grid search.,Implementation Details,"We implement DENRL and all baselines in Py-Torch, using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of 5e-4, a dropout rate of 0.2, and a batch size of 8.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 100/125
==========================================================================================
5. Implementation Details -- 6/10
------------------------------------------------------------------------------------------
We implement DENRL and all baselines in Py-Torch, using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of 5e-4, a dropout rate of 0.2, and a batch size of 8.",5901063,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Specifically in regularization training, we find optimal parameters α and β as 1 and 0.5 for our considered datasets. ",Implementation Details,"For instance selection, an empirical fitness threshold is set to 0.5 with the best validation F1.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 101/125
==========================================================================================
5. Implementation Details -- 7/10
------------------------------------------------------------------------------------------
For instance selection, an empirical fitness threshold is set to 0.5 with the best validation F1.",5901064,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We implement DENRL and all baselines in Py-Torch, using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of 5e-4, a dropout rate of 0.2, and a batch size of 8.",Implementation Details,We take a maximum of 5 new patterns in a loop for each relation type.,Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 102/125
==========================================================================================
5. Implementation Details -- 8/10
------------------------------------------------------------------------------------------
We take a maximum of 5 new patterns in a loop for each relation type.",5901065,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"For instance selection, an empirical fitness threshold is set to 0.5 with the best validation F1.",Implementation Details,"In the self-adaptive procedure, we run 5 epochs in the first loop, and 1 epoch in every rest loop until the validation performance converges.",Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 103/125
==========================================================================================
5. Implementation Details -- 9/10
------------------------------------------------------------------------------------------
In the self-adaptive procedure, we run 5 epochs in the first loop, and 1 epoch in every rest loop until the validation performance converges.",5901066,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We take a maximum of 5 new patterns in a loop for each relation type.,Implementation Details,adaptive learning effectively reduces the impact of mislabeled instances during training.,Implementation Details,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 104/125
==========================================================================================
5. Implementation Details -- 10/10
------------------------------------------------------------------------------------------
adaptive learning effectively reduces the impact of mislabeled instances during training.",5901067,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In the self-adaptive procedure, we run 5 epochs in the first loop, and 1 epoch in every rest loop until the validation performance converges.",Implementation Details,"We investigate the effectiveness of several components of DENRL on NYT dataset, as shown in Table 2.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 105/125
==========================================================================================
6. Ablation Study -- 1/10
------------------------------------------------------------------------------------------
We investigate the effectiveness of several components of DENRL on NYT dataset, as shown in Table 2.",5901068,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,adaptive learning effectively reduces the impact of mislabeled instances during training.,Implementation Details,"Before noise reduction, we first evaluate the impact of CRF layer by substituting it with a FC layer.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 106/125
==========================================================================================
6. Ablation Study -- 2/10
------------------------------------------------------------------------------------------
Before noise reduction, we first evaluate the impact of CRF layer by substituting it with a FC layer.",5901069,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We investigate the effectiveness of several components of DENRL on NYT dataset, as shown in Table 2.",Ablation Study,We found it improves the final performance by over 1% F1.,Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 107/125
==========================================================================================
6. Ablation Study -- 3/10
------------------------------------------------------------------------------------------
We found it improves the final performance by over 1% F1.",5901070,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Before noise reduction, we first evaluate the impact of CRF layer by substituting it with a FC layer.",Ablation Study,"We then build an initial redistributed dataset (via IDR), which helps joint model earn over 2% improvement in F1 and a sharp 28% precision increase compared to GPT-2+CRF.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 108/125
==========================================================================================
6. Ablation Study -- 4/10
------------------------------------------------------------------------------------------
We then build an initial redistributed dataset (via IDR), which helps joint model earn over 2% improvement in F1 and a sharp 28% precision increase compared to GPT-2+CRF.",5901071,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We found it improves the final performance by over 1% F1.,Ablation Study,"This demonstrates that the original DS dataset contains plenty of noise, thus a simple filtering method would effectively improve the performance.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 109/125
==========================================================================================
6. Ablation Study -- 5/10
------------------------------------------------------------------------------------------
This demonstrates that the original DS dataset contains plenty of noise, thus a simple filtering method would effectively improve the performance.",5901072,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We then build an initial redistributed dataset (via IDR), which helps joint model earn over 2% improvement in F1 and a sharp 28% precision increase compared to GPT-2+CRF.",Ablation Study,"However, this initial data induces poor recall performance, which means a large proportion of true positives with long-tail patterns are mistakenly regarded as false negatives.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 110/125
==========================================================================================
6. Ablation Study -- 6/10
------------------------------------------------------------------------------------------
However, this initial data induces poor recall performance, which means a large proportion of true positives with long-tail patterns are mistakenly regarded as false negatives.",5901073,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"This demonstrates that the original DS dataset contains plenty of noise, thus a simple filtering method would effectively improve the performance.",Ablation Study,"Assuming that some relation patterns in the training data are too rare to guide the model learn to attend them, we employ BR to training and achieves 5% recall increases with a slight decline in precision, inducing another 2% F1 improvement.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 111/125
==========================================================================================
6. Ablation Study -- 7/10
------------------------------------------------------------------------------------------
Assuming that some relation patterns in the training data are too rare to guide the model learn to attend them, we employ BR to training and achieves 5% recall increases with a slight decline in precision, inducing another 2% F1 improvement.",5901074,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"However, this initial data induces poor recall performance, which means a large proportion of true positives with long-tail patterns are mistakenly regarded as false negatives.",Ablation Study,This shows the effect of guiding the model to understand important feature words for identifying relations.,Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 112/125
==========================================================================================
6. Ablation Study -- 8/10
------------------------------------------------------------------------------------------
This shows the effect of guiding the model to understand important feature words for identifying relations.",5901075,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Assuming that some relation patterns in the training data are too rare to guide the model learn to attend them, we employ BR to training and achieves 5% recall increases with a slight decline in precision, inducing another 2% F1 improvement.",Ablation Study,"After we introduce OLF to training, both precision and recall improves about 2%, leading to another 2% F1 improvement, proving that logic rules guide a model to learn the entity-relation dependencies and further reduce entity labeling noise.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 113/125
==========================================================================================
6. Ablation Study -- 9/10
------------------------------------------------------------------------------------------
After we introduce OLF to training, both precision and recall improves about 2%, leading to another 2% F1 improvement, proving that logic rules guide a model to learn the entity-relation dependencies and further reduce entity labeling noise.",5901076,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,This shows the effect of guiding the model to understand important feature words for identifying relations.,Ablation Study,"After we obtain an initial model trained by BR and OLF, we continue SAL where DENRL collects more confident long-tail patterns to boost the recall performance and finally achieves 6% F1 improvement.",Ablation Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 114/125
==========================================================================================
6. Ablation Study -- 10/10
------------------------------------------------------------------------------------------
After we obtain an initial model trained by BR and OLF, we continue SAL where DENRL collects more confident long-tail patterns to boost the recall performance and finally achieves 6% F1 improvement.",5901077,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"After we introduce OLF to training, both precision and recall improves about 2%, leading to another 2% F1 improvement, proving that logic rules guide a model to learn the entity-relation dependencies and further reduce entity labeling noise.",Ablation Study,"To verify the effect of attention and logic guidance, we select some instances from the test set and visualize their attention weights, as well as the model's softmax probability distribution over all labels.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 115/125
==========================================================================================
7. Case Study -- 1/11
------------------------------------------------------------------------------------------
To verify the effect of attention and logic guidance, we select some instances from the test set and visualize their attention weights, as well as the model's softmax probability distribution over all labels.",5901078,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"After we obtain an initial model trained by BR and OLF, we continue SAL where DENRL collects more confident long-tail patterns to boost the recall performance and finally achieves 6% F1 improvement.",Ablation Study,"As shown in Figure 5, GPT-2+CRF, which is trained on original noisy data without BR or OLF, only focuses on entity pairs and makes wrong predictions.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 116/125
==========================================================================================
7. Case Study -- 2/11
------------------------------------------------------------------------------------------
As shown in Figure 5, GPT-2+CRF, which is trained on original noisy data without BR or OLF, only focuses on entity pairs and makes wrong predictions.",5901079,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To verify the effect of attention and logic guidance, we select some instances from the test set and visualize their attention weights, as well as the model's softmax probability distribution over all labels.",Case Study,"Its logic distance for r : Founder_of → PERSON is d r (I) = max {0, 0.7 − 0.4} = 0.3.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 117/125
==========================================================================================
7. Case Study -- 3/11
------------------------------------------------------------------------------------------
Its logic distance for r : Founder_of → PERSON is d r (I) = max {0, 0.7 − 0.4} = 0.3.",5901080,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"As shown in Figure 5, GPT-2+CRF, which is trained on original noisy data without BR or OLF, only focuses on entity pairs and makes wrong predictions.",Case Study,While DENRL precisely captures important words and correctly predicts the relation.,Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 118/125
==========================================================================================
7. Case Study -- 4/11
------------------------------------------------------------------------------------------
While DENRL precisely captures important words and correctly predicts the relation.",5901081,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Its logic distance for r : Founder_of → PERSON is d r (I) = max {0, 0.7 − 0.4} = 0.3.",Case Study,"The logic distance for r : Company_worked → PERSON is d r (I) = max {0, 0.8 − 0.8} = 0 < 0.3, which demonstrates the effect of OLF. ",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 119/125
==========================================================================================
7. Case Study -- 5/11
------------------------------------------------------------------------------------------
The logic distance for r : Company_worked → PERSON is d r (I) = max {0, 0.8 − 0.8} = 0 < 0.3, which demonstrates the effect of OLF. ",5901082,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,While DENRL precisely captures important words and correctly predicts the relation.,Case Study,"To show that BR explores versatile patterns to enrich pattern set P, we summarize both highfrequency patterns obtained by IDR and meaningful long-tail patterns discovered during SAL, and statistic their average fitness (see Table 3).",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 120/125
==========================================================================================
7. Case Study -- 6/11
------------------------------------------------------------------------------------------
To show that BR explores versatile patterns to enrich pattern set P, we summarize both highfrequency patterns obtained by IDR and meaningful long-tail patterns discovered during SAL, and statistic their average fitness (see Table 3).",5901083,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"The logic distance for r : Company_worked → PERSON is d r (I) = max {0, 0.8 − 0.8} = 0 < 0.3, which demonstrates the effect of OLF. ",Case Study,"Some long-tail patterns are not similar syntactically but still have over 0.5 average fitness scores, meaning the model learns useful semantic correlations between related feature words. ",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 121/125
==========================================================================================
7. Case Study -- 7/11
------------------------------------------------------------------------------------------
Some long-tail patterns are not similar syntactically but still have over 0.5 average fitness scores, meaning the model learns useful semantic correlations between related feature words. ",5901084,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"To show that BR explores versatile patterns to enrich pattern set P, we summarize both highfrequency patterns obtained by IDR and meaningful long-tail patterns discovered during SAL, and statistic their average fitness (see Table 3).",Case Study,We further check the performance of DENRL  on negative test cases that do not contain relations from NYT dataset.,Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 122/125
==========================================================================================
7. Case Study -- 8/11
------------------------------------------------------------------------------------------
We further check the performance of DENRL  on negative test cases that do not contain relations from NYT dataset.",5901085,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"Some long-tail patterns are not similar syntactically but still have over 0.5 average fitness scores, meaning the model learns useful semantic correlations between related feature words. ",Case Study,"After selecting confident candidates in each epoch, we further choose additional trustable negative instances that contain either the head or tail entity corresponding to each relation pattern in the selected positive candidates during bootstrap.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 123/125
==========================================================================================
7. Case Study -- 9/11
------------------------------------------------------------------------------------------
After selecting confident candidates in each epoch, we further choose additional trustable negative instances that contain either the head or tail entity corresponding to each relation pattern in the selected positive candidates during bootstrap.",5901086,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We further check the performance of DENRL  on negative test cases that do not contain relations from NYT dataset.,Case Study,"We compare the results between methods with and without entity selection, as shown in Figure 4.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 124/125
==========================================================================================
7. Case Study -- 10/11
------------------------------------------------------------------------------------------
We compare the results between methods with and without entity selection, as shown in Figure 4.",5901087,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"After selecting confident candidates in each epoch, we further choose additional trustable negative instances that contain either the head or tail entity corresponding to each relation pattern in the selected positive candidates during bootstrap.",Case Study,"The improved performance with entity selection demonstrates that a trustable relation pattern also indicates reliable entity labels, and partially explains the overall superiority of DENRL.",Case Study,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
2. The DENRL Framework -- 125/125
==========================================================================================
7. Case Study -- 11/11
------------------------------------------------------------------------------------------
The improved performance with entity selection demonstrates that a trustable relation pattern also indicates reliable entity labels, and partially explains the overall superiority of DENRL.",5901088,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"We compare the results between methods with and without entity selection, as shown in Figure 4.",Case Study,Entities and relations extraction is important to construct a KB.,Related Work,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 1/6
==========================================================================================
In this work we incorporate a GPT-2 backbone into a sequence tagging scheme for distantly-supervised joint extraction.",5901089,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,We will also evaluate our framework on other tasks such as event extraction and open information extraction.,Related Work,"While our current framework considers GPT-2, it's designed with flexibility in mind.",Limitations,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 2/6
==========================================================================================
While our current framework considers GPT-2, it's designed with flexibility in mind.",5901090,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,In this work we incorporate a GPT-2 backbone into a sequence tagging scheme for distantly-supervised joint extraction.,Limitations,"It can be easily adapted to other transformers such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and even newer large LMs, as the only difference is the computation of the transformer final representations, which is the very first step before our architecture designs.",Limitations,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 3/6
==========================================================================================
It can be easily adapted to other transformers such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and even newer large LMs, as the only difference is the computation of the transformer final representations, which is the very first step before our architecture designs.",5901091,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"While our current framework considers GPT-2, it's designed with flexibility in mind.",Limitations,"On the other hand, we focus on relations within a sentence and regard words between an entity pair as relation patterns.",Limitations,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 4/6
==========================================================================================
On the other hand, we focus on relations within a sentence and regard words between an entity pair as relation patterns.",5901092,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"It can be easily adapted to other transformers such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and even newer large LMs, as the only difference is the computation of the transformer final representations, which is the very first step before our architecture designs.",Limitations,"In our future work, we aim to consider relations beyond the sentence boundary for distantly-supervised joint extraction to better adapt to real-world information extraction scenarios.",Limitations,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 5/6
==========================================================================================
In our future work, we aim to consider relations beyond the sentence boundary for distantly-supervised joint extraction to better adapt to real-world information extraction scenarios.",5901093,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"On the other hand, we focus on relations within a sentence and regard words between an entity pair as relation patterns.",Limitations,"Last but not least, although our OLF is a one-time effort and can benefit future training, it is still hand-crafted based on ontology, and we aim to design a probabilistic method such as model uncertainty to quantify more comprehensive underlying relation-entity dependencies in the future.",Limitations,
"Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
==========================================================================================
4. Limitations -- 6/6
==========================================================================================
Last but not least, although our OLF is a one-time effort and can benefit future training, it is still hand-crafted based on ontology, and we aim to design a probabilistic method such as model uncertainty to quantify more comprehensive underlying relation-entity dependencies in the future.",5901094,"Distantly-Supervised Joint Entity and Relation Extraction with
  Noise-Robust Learning","0. abstract
1. Introduction
2. The DENRL Framework
3. Related Work
4. Limitations
",2023,"In our future work, we aim to consider relations beyond the sentence boundary for distantly-supervised joint extraction to better adapt to real-world information extraction scenarios.",Limitations,,,
