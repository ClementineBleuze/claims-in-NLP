text,maj_label,doccano_art_id,sentence_id,label_anno1,comments_anno1,label_anno2,comments_anno2,label_anno3,comments_anno3,label_anno4,comments_anno4,previous_sentence_section,previous_sentence,next_sentence_section,next_sentence
"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",FACT,0,1,FACT,,FACT,,FACT,,NC,,abstract,In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos.,abstract,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%."
"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",POS,0,2,POS,j'hésite entre POS et FACT,POS,,POS,,POS,,abstract,"Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software.",abstract,"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing."
"Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.",FACT,0,3,FACT,,FACT,,POS,,POS,,abstract,"The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%.",Introduction,"One of the problems relating to sign language recognition is the lack of appropriate datasets for algorithm training, since most datasets are recorded for academic purposes and as such, they concentrate in human learning rather than machine learning."
"Thus, we developed a system in the direction of ""phonological"" features recognition.",FACT,0,7,FACT,,FACT,,FACT,,POS,,Introduction,This characteristic makes it very unlikely for these datasets to be used as training sets for classification algorithms in sign recognition level.,Introduction,"This way we can extract a dataset with a lot of examples for every handshape, palm orientation and hand location out of the video collections."
"This way we can extract a dataset with a lot of examples for every handshape, palm orientation and hand location out of the video collections.",POS,0,8,POS,pas sûre,NC,,POS,,,,Introduction,"Thus, we developed a system in the direction of ""phonological"" features recognition.",Datasets,For the purposes of the project two collections of single gloss videos were used as datasets.
It is used as a tool of human body keypoints extraction from a single image or video frame.,NC,0,23,NC,,NC,,NC,,FACT,,Openpose,"OpenPose is a software freely distributed by Carnegie Melon University, Perceptual Computing Lab (Cao et al., 2018).",Openpose,"It offers an estimation of 25 body/foot keypoints, 2x21 hand keypoints and 70 face keypoints."
"It offers an estimation of 25 body/foot keypoints, 2x21 hand keypoints and 70 face keypoints.",NC,0,24,NC,,NC,,NC,,FACT,,Openpose,It is used as a tool of human body keypoints extraction from a single image or video frame.,Openpose,"In the case of a 2D video input, for each keypoint it returns a vector containing 3 elements."
The novelty behind OpenPose relies on the fact that it works for more than one person per image but more importantly the keypoint analysis is not affected when part of the individual's body is out of frame.,NC,0,28,,,NC,,NC,,POS,,Openpose,"The third is a value in the range [0,1] which is quantification of the confidence given by the program that the specific keypoint is correctly located in the frame.",Openpose,This last feature is crucial for applications on sign language videos where the signer appears above the waist level (Figure 1).
This last feature is crucial for applications on sign language videos where the signer appears above the waist level (Figure 1).,NC,0,29,,,NC,,NC,,POS,,Openpose,The novelty behind OpenPose relies on the fact that it works for more than one person per image but more importantly the keypoint analysis is not affected when part of the individual's body is out of frame.,Our Method,The first step in our method is transforming each video frame into keypoints using the OpenPose software.
Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English.,NC,1,0,NC,,NC,,NC,,POS,,,,abstract,"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation."
"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.",NC,1,1,NC,,NC,,NC,,POS,,abstract,Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English.,abstract,"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences."
"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.",NC,1,2,NC,,NC,,NC,,NEG,,abstract,"A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.",abstract,This degrades translation accuracy.
This degrades translation accuracy.,NC,1,3,NC,,NC,,NC,,NEG,,abstract,"However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.",abstract,"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations."
"In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.",FACT,1,4,FACT,,FACT,,POS,,POS,,abstract,This degrades translation accuracy.,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points."
1,NC,1,6,NC,problème phrase,NC,,NC,,NC,,abstract,"Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.",Introduction,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008)."
"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008).",NC,1,7,NC,,NC,,NC,,POS,,abstract,1,Introduction,"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English."
"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English.",NC,1,8,NC,,NC,,NC,,NEG,,Introduction,"In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008).",Introduction,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006)."
"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",NC,1,9,NC,,NC,,NC,,POS,,Introduction,"Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English.",Introduction,"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model."
"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",NC,1,10,NC,,NC,,NC,"Ce n'est pas un claim as such, mais je me rends compte qu'en citant de cette manière, on influence la suite alors qu'on ne parle pas de la recherche menée, mais de l'état de l'art",,,Introduction,"One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral-1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 lel data with the source and target languages exists (de Gispert and Mariño, 2006).",Introduction,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015)."
"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015).",NC,1,11,NC,,NC,,NC,,POS,,Introduction,"Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007;Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model.",Introduction,"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora."
"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora.",NC,1,12,NC,,NC,,NC,,POS,,Introduction,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003;Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007;Miura et al., 2015).",Introduction,"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists."
"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists.",NC,1,13,NC,,NC,,NC,,FACT,,Introduction,"However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models trained on direct parallel corpora.",Introduction,"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms."
"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms.",NC,1,14,NC,,NC,,NC,,POS,,Introduction,"In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase exists.",Introduction,"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context."
"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context.",NC,1,15,NC,,NC,,NC,,POS,,Introduction,"In Figure 1 (a), we show an example of standard triangulation on Hiero TMs that combines hierarchical rules of phrase pairs by matching pivot phrases with equivalent surface forms.",Introduction,"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given."
"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given.",NC,1,16,NC,,NC,,NC,,NEG,,Introduction,"This example also demonstrates problems of ambiguity: the English word ""record"" can correspond to several different parts-of-speech according to the context.",Introduction,This varying syntactic structure will affect translation.
This varying syntactic structure will affect translation.,NC,1,17,NC,,NC,,NC,,FACT,,Introduction,"More broadly, phrases including this word also have different possible grammatical structures, but it is impossible to uniquely identify this structure unless information about the surrounding context is given.",Introduction,"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun."
"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun.",NC,1,18,NC,,NC,,NC,,FACT,,Introduction,This varying syntactic structure will affect translation.,Introduction,"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word."
"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word.",NC,1,19,NC,,NC,,NC,,FACT,,Introduction,"For example, the French verb ""enregistrer"" corresponds to the English verb ""record"", but the French noun ""dossier"" also corresponds to ""record"" -as a noun.",Introduction,"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change."
"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change.",NC,1,20,NC,,NC,,NC,,FACT,,Introduction,"As a more extreme example, Chinese is a languages that does not have inflections according to the part-of-speech of the word.",Introduction,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录""."
"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",NC,1,21,NC,,NC,,NC,,NEG,,Introduction,"As a result, even in the contexts where ""record"" is used with different parts-of-speech, the Chinese word ""记录"" will be used, although the word order will change.",Introduction,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences."
"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",NC,1,22,NC,,NC,"analyse tirée d'un exemple (example = not a claim), mais servant de base à une hypothèse de travail",NC,,POS,,Introduction,"These facts might result in an incorrect connection of ""[X1] enregistrer [X2]"" and ""[X2] [X1] 记录"" even though proper correspondence of ""[X1] enregistrer [X2]"" and ""[X1] dossier [X2]"" would be ""[X1] 记 录 [X2]"" and ""[X2] [X1] 记 录"".",Introduction,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting."
"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",POS,1,23,POS,,POS,,PROSP,Hypothèse =? prospective ?,POS,,Introduction,"Hence a superficial phrase matching method based solely on the surface form of the pivot will often combine incorrect phrase pairs, causing translation errors if their translation scores are estimated to be higher than the proper correspondences.",Introduction,"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b)."
"To incorporate this intuition into our models, we propose a method that considers syntactic information of the pivot phrase, as shown in Figure 1  (b).",FACT,1,24,FACT,,FACT,,FACT,,POS,,Introduction,"Given this background, we hypothesize that disambiguation of these cases would be easier if the necessary syntactic information such as phrase structures are considered during pivoting.",Introduction,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase."
"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",FACT,1,26,FACT,,NC,"précisions sur la méthode développée, déjà introduite précédemment, mais on reste dans l'introduction ...",FACT,,POS,,Introduction,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",Introduction,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4)."
"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",NC,1,27,FACT,,NC,,NC,,POS,,Introduction,"Specifically, we propose a method based on Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969;Chiang, 2007), which are widely used in tree-based machine translation frameworks ( §2).",Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2)."
"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",NC,1,28,NC,,NC,,NC,,POS,,Introduction,"After describing the baseline triangulation method ( §3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching ( §4).",Introduction,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5)."
"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",POS,1,29,FACT#POS,"""we perform expe..."" = FACT (?)",FACT#POS,,POS,,POS,,Introduction,"The first places a hard restriction on exact matching of parse trees ( §4.1) included in translation rules, while the second places a softer restriction allowing partial matches ( §4.2).",Introduction,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario."
"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",POS,1,30,FACT#POS,,FACT#POS,,POS,,POS,,Introduction,"To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language ( §5).",Synchronous Context-Free Grammars,"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007)."
"In this section, first we cover SCFGs, which are widely used in machine translation, particularly hierarchical phrase-based translation (Hiero) (Chiang, 2007).",NC,1,31,,,NC,,NC,,POS,,Introduction,"In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario.",Synchronous Context-Free Grammars,"In SCFGs, the elementary structures used in translation are synchronous rewrite rules with aligned pairs of source and target symbols on the right-hand side: X → ⟨ s, t ⟩ (1) where X is the head symbol of the rewrite rule, and s and t are both strings of terminals and nonterminals on the source and target side respectively."
Synchronous rules can be extracted based on parallel sentences and automatically obtained word alignments.,NC,1,35,NC,,NC,,NC,,FACT,,Synchronous Context-Free Grammars,"For example, a synchronous rule could take the form of: X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",Synchronous Context-Free Grammars,"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1."
"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1.",NC,1,36,NC,,NC,,NC,,FACT,,Synchronous Context-Free Grammars,Synchronous rules can be extracted based on parallel sentences and automatically obtained word alignments.,Synchronous Context-Free Grammars,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings."
"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",NC,1,37,NC,,NC,,NC,,FACT,,Synchronous Context-Free Grammars,"Each extracted rule is scored with phrase translation probabilities in both directions ϕ(s|t) and ϕ(t|s), lexical translation probabilities in both directions ϕ lex (s|t) and ϕ lex (t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1.",Synchronous Context-Free Grammars,"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998)."
"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",NC,1,38,NC#NEG,,NC,,NC,,POS,,Synchronous Context-Free Grammars,"At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model (LM) probability over the target strings.",Synchronous Context-Free Grammars,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007)."
"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007).",NC,1,39,NC,,NC,,NC,,POS,,Synchronous Context-Free Grammars,"When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998).",Hierarchical Rules,"In this section, we specifically cover the rules used in Hiero."
"In this section, we specifically cover the rules used in Hiero.",NC,1,40,NC,,NC,,NC,,POS,,Synchronous Context-Free Grammars,"When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007).",Hierarchical Rules,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X."
"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X.",NC,1,41,NC,,NC,,NC,,FACT,,Hierarchical Rules,"In this section, we specifically cover the rules used in Hiero.",Hierarchical Rules,"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007)."
"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007).",NC,1,42,NC,,NC,,NC,,FACT,,Hierarchical Rules,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X.",Hierarchical Rules,"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ ."
"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",NC,1,43,NC,,NC,,NC,,FACT,,Hierarchical Rules,"2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al., 2003) based on word alignments, followed by a step that performs recursive extraction of hierarchical phrases (Chiang, 2007).",Hierarchical Rules,"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing."
"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing.",NC,1,44,NC,,NC,,NC,,POS,,Hierarchical Rules,"For example, hierarchical rules could take the form of: X → ⟨Officers, 主席团 成員⟩ (3) X → ⟨the Committee, 委员会⟩ (4) X → ⟨X 0 of X 1 , X 1 的 X 0 ⟩ .",Hierarchical Rules,"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations."
"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations.",NC,1,45,NC,,NC,,NEG,,POS,,Hierarchical Rules,"( ) From these rules, we can translate the input sentence by derivation: S → ⟨X 0 , X 0 ⟩ ⇒ ⟨X 1 of X 2 , X 2 的 X 1 ⟩ ⇒ ⟨Officers of X 2 , X 2 主席团 成員⟩ ⇒ ⟨Officers of the Committee, 委员会 的 主席团 成員⟩ The advantage of Hiero is that it is able to achieve relatively high word re-ordering accuracy (compared to other symbolic SMT alternatives such as standard phrase-based MT) without language-dependent processing.",Explicitly Syntactic Rules,"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules)."
"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules).",NC,1,46,NC,,NC,,NC,,FACT,,Hierarchical Rules,"On the other hand, since it does not use syntactic information and tries to extract all possible combinations of rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations.",Explicitly Syntactic Rules,"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree."
"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree.",NC,1,47,NC,,NC,,NC,,FACT,,Explicitly Syntactic Rules,"An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules).",Explicitly Syntactic Rules,"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ ."
"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ .",NC,1,48,NC,,NC,,NC,,FACT,,Explicitly Syntactic Rules,"Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree.",Explicitly Syntactic Rules,"Here, parse subtrees of the source language rules are given in the form of S-expressions."
"Here, parse subtrees of the source language rules are given in the form of S-expressions.",NC,1,49,NC,,NC,,NC,,FACT,,Explicitly Syntactic Rules,"For example, T2S rules could take the form of: X NP → ⟨(NP (NNS Officers)), 主席团 成員⟩ (6) X NP → ⟨(NP (DT the) (NNP Committee)), 委员会⟩ (7) X PP → ⟨ (PP (IN of) X NP,0 ), X0 的 ⟩ (8) X NP → ⟨ (NP X NP,0 X PP,1 ), X1 X0 ⟩ .",Explicitly Syntactic Rules,"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar."
"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar.",NC,1,50,NC,,NC,,POS,,POS,,Explicitly Syntactic Rules,"Here, parse subtrees of the source language rules are given in the form of S-expressions.",Explicitly Syntactic Rules,"This method also has the advantage the number of less-useful synchronous rules extracted by syntax-agnostic methods such as Hiero are reduced, making it possible to learn more compact rule tables and allowing for faster translation."
"This method also has the advantage the number of less-useful synchronous rules extracted by syntax-agnostic methods such as Hiero are reduced, making it possible to learn more compact rule tables and allowing for faster translation.",NC,1,51,NC,,NC,,POS,,POS,,Explicitly Syntactic Rules,"From these rules, we can translate from the parse tree of the input sentence by derivation: X ROOT → ⟨ X NP,0 , X0 ⟩ ⇒ ⟨ (NP X NP,1 X PP,2 ), X2 X1 ⟩ ⇒ ⟨ (NP (NP (NNS Officers) X PP,2 )), X2 主席团 成員 ⟩ * ⇒ ⟨ (NP (NP (NNS Officers)) (PP (IN of) (NP (DT the) (NNP Committee)))) , 委员会 的 主席团 成員 ⟩ In this way, it is possible in T2S translation to obtain a result conforming to the source language's grammar.",Standard Triangulation Method,"In the triangulation method by Cohn and Lapata (2007), we first train source-pivot and pivot-target rule tables as T SP and T P T respectively."
"In the previous section, we explained about the standard triangulation method and mentioned that the pivot-side ambiguity causes incorrect estimation of translation probability and the translation accuracy might decrease.",NC,1,59,NC,,NC,,NC,,POS,,Standard Triangulation Method,"Specifically, if there are multiple interpretations of the pivot phrase as shown in the example of Figure 1, source and target phrases that do not correspond to each other semantically might be connected, and over-estimation by summing products of the translation probabilities is likely to cause failed translations.",Triangulation with Syntactic Matching,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent."
"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",NC,1,60,NC,difficile de savoir si c'est une connaissance établie ou une nouvelle hypothèse de leur part (sans connaissance du domaine),POS,claim implicite ?,NC,,POS,,Triangulation with Syntactic Matching,"In the previous section, we explained about the standard triangulation method and mentioned that the pivot-side ambiguity causes incorrect estimation of translation probability and the translation accuracy might decrease.",Triangulation with Syntactic Matching,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching."
"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching.",NC,1,61,NC,,NC,,NC,,FACT,,Triangulation with Syntactic Matching,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",Exact Matching of Parse Subtrees,"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively."
"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively.",NC,1,62,NC,,NC,,NC,,FACT,,Triangulation with Syntactic Matching,"In the following two sections, we describe two methods to distinguish pivot phrases that have syntactically different roles, one based on exact matching of parse trees, and one based on soft matching.",Exact Matching of Parse Subtrees,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings."
"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",NC,1,63,NC,,NC,,NC,,FACT,,Exact Matching of Parse Subtrees,"In the exact matching method, we first train pivotsource and pivot-target T2S TMs by parsing the pivot side of parallel corpora, and store them into rule tables as T P S and T P T respectively.",Exact Matching of Parse Subtrees,"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2."
"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2.",NC,1,64,NC,,NC,,NC,,POS,,Exact Matching of Parse Subtrees,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",Exact Matching of Parse Subtrees,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM."
"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",NC,1,65,NC,,NC,,NC,,POS,,Exact Matching of Parse Subtrees,"The procedure of synthesizing source-target synchronous rules essentially follows equations ( 11)-( 14), except using T P S instead of T SP (direction of probability features is reversed) and pivot subtree p instead of pivot phrase p. Here s and t do not have syntactic information, therefore the synthesized synchronous rules should be hierarchical rules explained in §2.2.",Exact Matching of Parse Subtrees,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced."
"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",POS,1,66,POS,,NC,,NC,,POS,,Exact Matching of Parse Subtrees,"The matching condition of this method has harder constraints than matching of superficial symbols in standard triangulation, and has the potential to reduce incorrect connections of phrase pairs, resulting in a more reliable triangulated TM.",Exact Matching of Parse Subtrees,Therefore it is important to create TMs that are both reliabile and have high coverage.
Therefore it is important to create TMs that are both reliabile and have high coverage.,POS,1,67,POS,,NC,,NC,,POS,,Exact Matching of Parse Subtrees,"On the other hand, the number of connected rules decreases as well in this restricted triangulation, and the coverage of the triangulated model might be reduced.",Partial Matching of Parse Subtrees,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees."
"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees.",FACT,1,68,FACT,,FACT,,NC,,POS,,Exact Matching of Parse Subtrees,Therefore it is important to create TMs that are both reliabile and have high coverage.,Partial Matching of Parse Subtrees,"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively."
"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively.",NC,1,69,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"To prevent the problem of the reduction of coverage in the exact matching method, we also propose a partial matching method that keeps coverage just like standard triangulation by allowing connection of incompletely equivalent pivot subtrees.",Partial Matching of Parse Subtrees,"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections."
"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections.",NC,1,70,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"To estimate translation probabilities in partial matching, we first define weighted triangulation generalizing the equations ( 11)-( 14) of standard triangulation with weight function ψ(•): ϕ ( t|s ) = ∑ pT ∑ pS ϕ ( t| pT ) ψ ( pT | pS ) ϕ ( pS |s) , ϕ ( s|t ) = ∑ pS ∑ pT ϕ (s| pS ) ψ ( pS | pT ) ϕ ( pT |t ) , ϕ lex ( t|s ) = ∑ pT ∑ pS ϕ lex ( t| pT ) ψ ( pT | pS ) ϕ lex ( pS |s) , (19) ϕ lex ( s|t ) = ∑ pS ∑ pT ϕ lex (s| pS ) ψ ( pS | pT ) ϕ lex ( pT |t ) where pS ∈ T SP and pT ∈ P P T are pivot parse subtrees of source-pivot and pivot-target synchronous rules respectively.",Partial Matching of Parse Subtrees,"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14)."
"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14).",NC,1,71,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"By adjusting ψ(•), we can control the magnitude of the penalty for the case of incompletely matched connections.",Partial Matching of Parse Subtrees,"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available."
"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available.",NC,1,72,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"If we define ψ( pT | pS ) = 1 when pT is equal to pS and ψ( pT | pS ) = 0 otherwise, equations ( 17)-( 20) are equivalent with equations ( 11)-( 14).",Partial Matching of Parse Subtrees,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998)."
"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998).",NC,1,73,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"Better estimating ψ(•) is not trivial, and cooccurrence counts of pS and pT are not available.",Partial Matching of Parse Subtrees,"According to equations ( 21)-( 24), we can assure that incomplete match of pivot subtrees leads d(•) ≥ 1 and penalizes such that ψ(•) ≤ 1/e d ≤ 1/e, while exact match of subtrees leads to a value of ψ(•) at least e ≈ 2.718 times larger than when using partially matched subtrees."
"According to equations ( 21)-( 24), we can assure that incomplete match of pivot subtrees leads d(•) ≥ 1 and penalizes such that ψ(•) ≤ 1/e d ≤ 1/e, while exact match of subtrees leads to a value of ψ(•) at least e ≈ 2.718 times larger than when using partially matched subtrees.",NC,1,74,NC,,NC,,NC,,POS,,Partial Matching of Parse Subtrees,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998).",Experimental Set-Up,"To investigate the effect of our proposed approach, we evaluate the translation accuracy through pivot translation experiments on the United Nations Parallel Corpus (UN6Way) (Ziemski et al., 2016)."
Translating with a Hiero TM directly trained on the source-target parallel corpus without using pivot language (as an oracle).,NC,1,92,NC,,NC,,NC,,POS,,Experimental Set-Up,We evaluate 6 translation methods:,Tri. Hiero:,"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3)."
"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3).",NC,1,93,NC,,NC,,NC,,FACT,,Direct:,Translating with a Hiero TM directly trained on the source-target parallel corpus without using pivot language (as an oracle).,Tri. TreeExact,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed exact matching of pivot subtrees (proposed 1, §4.1)."
"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed exact matching of pivot subtrees (proposed 1, §4.1).",NC,1,94,NC,,NC,,NC,,POS,,Tri. Hiero:,"Triangulating source-pivot and pivot-target Hiero TMs into a source-target Hiero TM using the traditional method (baseline, §3).",Tri. TreePartial,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed partial matching of pivot subtrees (proposed 2, §4.2)."
The result of experiments using all combinations of pivot translation tasks for 5 languages via English is shown in Table 1.,NC,1,96,NC,,NC,,NC,,FACT,,Tri. TreePartial,"Triangulating pivot-source and pivot-target T2S TMs into a source-target Hiero TM using the proposed partial matching of pivot subtrees (proposed 2, §4.2).",Experimental Results,"From the results, we can see that the proposed partial matching method of pivot subtrees in triangulation outperforms the standard triangulation method for all language pairs and achieves higher or almost equal scores than proposed exact matching method."
In Table 2 we show the comparison of coverage of each proposed triangulated method.,NC,1,99,NC,,NC,,NC,,FACT,,Experimental Results,"The exact matching method also outperforms the standard triangulation method in the majority of the language pairs, but has a lesser improvement than partial matching method.",Experimental Results,"From this table, we can see that the exact matching method reduces several percent in number of unique phrases while the partial matching method keeps the same coverage with surfaceform matching."
"From this table, we can see that the exact matching method reduces several percent in number of unique phrases while the partial matching method keeps the same coverage with surfaceform matching.",POS,1,100,POS,,POS,,FACT,,POS,,Experimental Results,In Table 2 we show the comparison of coverage of each proposed triangulated method.,Experimental Results,We can consider that it is one of the reasons of the difference in improvement stability between the partial and exact matching methods.
We show an example of a translated sentences for which pivot-side ambiguity is resolved in the the syntactic matching methods:,NC,1,102,NC,,NC,,NC,,POS,,Experimental Results,We can consider that it is one of the reasons of the difference in improvement stability between the partial and exact matching methods.,Reference in Spanish:,Suiza alienta a todos los Estados partes : a ::: que ::::::: apoyen ::: la :::::: actual ::::: labor :::::::::: conceptual ::: de :: la ::::::::: Secretaría .
"Recent results (Firat et al., 2016;Johnson et al., 2016) have found that neural machine translation systems can gain the ability to perform translation with zero parallel resources by training on multiple sets of bilingual data.",NC,1,107,NC,,NC,,NC,,POS,,Reference in Spanish:,We can confirm that the derivation improves word-selection and word-reordering by using this rule.,Comparison with Neural MT:,"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero."
"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero.",NC,1,108,NC,,NC,,NC,,NEG,,Comparison with Neural MT:,"Recent results (Firat et al., 2016;Johnson et al., 2016) have found that neural machine translation systems can gain the ability to perform translation with zero parallel resources by training on multiple sets of bilingual data.",Comparison with Neural MT:,"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al."
"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al.",NC,1,109,NC,,NC,,FACT,,FACT,,Comparison with Neural MT:,"However, previous work has not examined the competitiveness of these methods with pivot-based symbolic SMT frameworks such as PBMT or Hiero.",Comparison with Neural MT:,"To train and evaluate NMT models, we adopt NMTKit."
"To train and evaluate NMT models, we adopt NMTKit.",NC,1,110,NC,,NC,,NC,,FACT,,Comparison with Neural MT:,"In this section, we compare a zero-shot NMT model (detailed parameters in  Johnson et al.",Comparison with Neural MT:,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training."
"Cascade is one immediate method connecting two TMs, and NMT cascade translation shows the medium performance in this experiment.",POS,1,112,POS,,NC,,,,POS,,Comparison with Neural MT:,"7 From the results we see the tendency of NMT that directly trained model achieves high translation accuracy even for translation between languages of different families, on the other hand, the accuracy is drastically reduced in the situation when there is no sourcetarget parallel corpora for training.",Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker."
"This may be because we used only 1 LSTM layer for encoder/decoder, or because the amount of parallel corpora or language pairs were not sufficient.",NEG,1,114,NEG,,POS,"cause d'un résultat, et non sa conséquence (conséquence directe d'un résultat = claim), quand même considéré comme une analyse directe de résultat ?",,,NEG,,Comparison with Neural MT:,"In our setting, while bilingually trained NMT systems were competitive or outperformed Hiero-based models, zeroshot translation is uniformly weaker.",Comparison with Neural MT:,"Thus, we can posit that while zero-shot translation has demonstrated reasonable results in some settings, successful zero-shot translation systems are far from trivial to build, and pivot-based symbolic MT systems such as PBMT or Hiero may still be a competitive alternative."
"To estimate translation probabilities, we introduced heuristic that has no guarantee to be optimal.",FACT,1,118,FACT,,FACT#NEG,,,,,,Conclusion,"In experiments, we found that these triangulated models are effective in particular when allowing partial matching.",Conclusion,"Therefore in the future, we plan to explore more refined estimation methods that utilize machine learning."
"The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture.",NC,2,3,NC,,POS,working hypothesis,,,,,abstract,"This graph is used to prune the action space, enabling more efficient exploration.",abstract,"In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives."
We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.,NC,2,5,NC,,FACT,link to code/data -> to keep in FACT ? could be useful for an overview of the contributions,,,,,abstract,"In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives.",Introduction,Natural language communication can be used to affect change in the real world.
We introduce three contributions to text-based game playing to deal with the combinatorially large state and action spaces.,NC,2,18,NC,,FACT,,,,,,Introduction,"Such games have historically proven to be difficult to play for AI agents, and the more complex variants such as Zork still remain firmly out of the reach of existing approaches.",Introduction,"First, we show that a state representation in the form of a knowledge graph gives us the ability to effectively prune an action space."
The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.,NC,2,21,NC,,POS,,,,,,Introduction,A knowledge graph captures the relationships between entities as a directed graph.,Introduction,"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair."
"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair.",FACT,2,22,FACT,,FACT#POS,,,,,,Introduction,The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.,Introduction,"This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018;Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs."
"This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018;Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs.",NC,2,23,NC,,POS,,,,,,Introduction,"Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-resentation to estimate the Q-value for a stateaction pair.",Introduction,"Finally, we take initial steps toward framing the POMDP as a questionanswering (QA) problem wherein a knowledgegraph can be used to not only prune actions but to answer the question of what action is most appropriate."
We provide results on ablative experiments comparing our knowledge-graph based approach approaches to strong baselines.,NC,2,27,NC,,FACT,,,,,,Introduction,We show how pre-training certain parts of our KG-DQN network using existing QA methods improves performance and allows knowledge to be transferred from different games.,Introduction,Results show that incorporating a knowledge-graph into a reinforcement learning agent results in converges to the highest reward more than 40% faster than the best baseline.
"In our approach, our agent learns a knowledge graph, stored as a set of RDF triples, i.e.",NC,2,50,NC,phrase incomplète,NC,,,,,,Knowledge Graph DQN,"In this section we introduce our knowledge graph representation, action pruning and deep Qnetwork architecture.",Knowledge Graph Representation,"3-tuples of subject, relation, object ."
"For example, from a phrase such as ""There is an exit to the north"" one can infer a has relation between the current The knowledge graph is updated after every agent action (see Figure 1).",NC,2,54,NC,1re partie est une phrase coupée ?,NC,,,,,,Knowledge Graph Representation,OpenIE is not optimized to the regularities of text adventure games and there are a lot of relations that can be inferred from the typical structure of descriptive texts.,Knowledge Graph Representation,The update rules are defined such that there are portions of the graph offering short and long-term context.
We intend for the update rules to be applied to text-based games in different domains and so only hand-craft a minimal set of rules that we believe apply generally.,POS,2,58,POS,,PROSP,,,,,,Knowledge Graph Representation,Other relations persist after each action.,Knowledge Graph Representation,They are: • Linking the current room type (e.g.
Treating the problem as question-answering will not replace the need for exploration in text-adventure games.,POS,2,106,POS,,NEG,,,,,,Game Play as Question Answering,"When appropriately trained, the agent may be able to answer the question for itself and select a good next move to execute.",Game Play as Question Answering,"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration."
"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.",POS,2,107,POS,,PROSP,,,,,,Game Play as Question Answering,Treating the problem as question-answering will not replace the need for exploration in text-adventure games.,Game Play as Question Answering,"To teach the agent to answer the question of what action is best to take given an observation, we use an offline, pre-training approach."
"To teach the agent to answer the question of what action is best to take given an observation, we use an offline, pre-training approach.",NC,2,108,NC,,FACT,,,,,,Game Play as Question Answering,"However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.",Game Play as Question Answering,"The data for the pre-training approach is generated using an oracle, an agent capable of finishing a game perfectly in the least number of steps possible."
"We then use the DrQA (Chen et al., 2017) question-answering technique to train a paired question encoder and an answer encoder that together predict the answer (action) from the question (text observation).",NC,2,113,NC,,FACT,,,,,,Game Play as Question Answering,and the oracle's correct action is the answer.,Game Play as Question Answering,The weights from the SB-LSTM in the document encoder in the DrQA system are then used to initialize the weights of the SB-LSTM.
Our approach to question-answering in the context of text adventure game playing thus represents a form of transfer learning.,NC,2,122,NC,,FACT,,,,,,Game Play as Question Answering,"For pre-training to work, the agent must develop a general question-answering competence that can transfer to new quests.",Experiments,"We conducted experiments in the TextWorld framework (Côté et al., 2018) using their ""home"" theme."
Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map.,NC,2,167,NC,,POS,,,,,,Results and Discussion,"To help benchmark our agent's performance, we observed people unaffiliated with the research playing through the same TextWorld ""home"" quests as the other models.",Results and Discussion,"When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible."
"When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible.",NC,2,168,NC,,POS,,,,,,Results and Discussion,Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map.,Results and Discussion,Also note that none of the deep reinforcement learning agents received instructions.
We don't show BOW-DQN because it is strictly inferior to LSTM-DQN in all situations).,POS,2,171,POS,,NC,,,,,,Results and Discussion,"On both small and large maps, all versions of KG-DQN tested converge faster than baselines (see Figure 3 for the small game and Figure 4 for the large game).",Results and Discussion,KG-DQN converges 40% faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five.
"On the large game, no Table 3: Average number of steps (and standard deviation) taken to complete the small game.",POS,2,173,POS,légende de table en plein milieu/coupe phrase,NC,,,,,,Results and Discussion,KG-DQN converges 40% faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five.,Results and Discussion,"agents achieve the maximum reward of 10, and the LSTM-DQN requires more than 300 episodes to converge at the same level as KG-DQN."
"Differences between LSTM-DQN and full KG-DQN are not statistically significant, p = 0.199 on an independent Ttest.",POS,2,178,POS,,NEG#POS,,,,,,Results and Discussion,Full KG-DQN requires an equivalent number of steps in the small game (Table 3) and in the large game (Table 4).,Results and Discussion,The ablated versions of KG-DQN-unpruned KG-DQN and non-pre-trained KG-DQN-require many more steps to complete quests.
TextWorld's reward function allows for a lot of exploration of the environment without penalty so it is possible for a model that has converged on reward to complete quests in as few as five steps or in many hundreds of steps.,NC,2,180,NC,,POS,,,,,,Results and Discussion,The ablated versions of KG-DQN-unpruned KG-DQN and non-pre-trained KG-DQN-require many more steps to complete quests.,Results and Discussion,"From these results, we conclude that the pre-training using our questionanswering paradigm is allowing the agent to find a general understanding of how to pick good actions even when the agent has never seen the final"
"While the knowledge graph allows the agent to reach optimal reward more quickly, it doesn't ensure a high quality solution to quests.",POS,2,184,POS,,NEG,,,,,,Conclusions,We speculate that this is because the knowledge graph provides a persistent memory of the world as it is being explored.,Conclusions,Action pruning using the knowledge graph and pre-training of the embeddings used in the deep Q-network result in shorter action sequences needed to complete quests.
The insight into pre-training portions of the agent's architecture is based on converting textadventure game playing into a question-answering activity.,NC,2,186,NC,,POS,,,,,,Conclusions,Action pruning using the knowledge graph and pre-training of the embeddings used in the deep Q-network result in shorter action sequences needed to complete quests.,Conclusions,"That is, at every step, the agent is asking-and trying to answer-what is the most important thing to try."
"The pre-training acts as a form of transfer learning from different, but re-lated games.",NC,2,188,NC,,POS,,,,,,Conclusions,"That is, at every step, the agent is asking-and trying to answer-what is the most important thing to try.",Conclusions,"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required."
"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.",POS,2,189,POS,,NEG,,,,,,Conclusions,"The pre-training acts as a form of transfer learning from different, but re-lated games.",Conclusions,"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language."
"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language.",POS,2,190,POS,,PROSP,,,,,,Conclusions,"However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.",Conclusions,"Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language."
"Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language.",POS,2,191,POS,,PROSP,,,,,,Conclusions,"By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language.",,
"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",POS,3,0,POS,,NC,,,,,,,,abstract,"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation."
"However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation.",POS,3,1,POS,,NC,,,,,,abstract,"  Despite the feature of real-time decoding, Monotonic Multihead Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks.",abstract,"In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time."
"We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.",FACT,3,4,FACT,,POS,,,,,,abstract,"Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process.",INTRODUCTION,"Online automatic speech recognition (ASR), which immediately recognizes incomplete speeches as humans do, is emerging as a core element of diverse ASR-based services such as teleconferences, AI secretaries, or AI booking services."
"Published in ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 6-11 June 2021 in Toronto, Ontario, Canada.",NC,3,8,NC,ligne de conf,NC,,,,,,INTRODUCTION,"However, of course, online ASR models [1,2] targeting real-time inference have concerns about performance degradation compared to traditional Copyright 2021 IEEE.",INTRODUCTION,Personal use of this material is permitted.
The performance degradation with small occurs since accessible input information is very limited and training models with small restricts head diversity severely.,NC,3,97,NC,,NC,explication d'un résultat,,,,,Trade-off between Performance and Latency,Our model outperforms baselines and is still faster than MMA without HSD even though there are small increases in relative latency compared to HeadDrop except for the case with extremely small text .,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small ."
We suggest the method to learn alignments with considering other heads' alignments by modifying expected alignments for all the heads of each layer to select an input frame within a fixed size window.,POS,3,99,POS,,FACT,,,,,,Trade-off between Performance and Latency,"Thus, this result suggests that the practitioners should avoid choosing small .",CONCLUSION,Our approach improves performance with only a small increase in latency by regularizing the intra-layer difference of boundaries effectively from the training phase.
"However, Tanaka (2011) ar gues against Merchant's dichotomy in voice mis match between VP ellipsis and Pseudogapping, reporting that voice mismatch in both types of e llipsis is permissible or not while interacting wi th what Kehler (2000) calls discourse coherence relations between ellipsis and antecedent clause s. Departing from Kehler's (2000) insight, we s uggest that vP undergoes ellipsis in a resemblan ce discourse relation, but VP does so in a cause/ effect discourse relation.",NC,4,3,NC,,POS,caractères espace --> pb au niveau du XML ?,,,,,abstract,"reports that VP ellipsis as an elision o f smaller size VP allows voice mismatch, but Ps eudogapping and Sluicing as an elision of bigge r size vP/TP do not.",abstract,"Given the asymmetry i n the size of ellipsis in tandem with discourse re lations, we argue that since Accusative as well as Nominative Case is checked outside VP, the VP to be elided can meet the identity condition on ellipsis with its antecedent VP as the object element in the former and the subject one in the latter or vice versus have not been Case-checke d yet, thus being identical in terms of Case-feat ure at the point of derivation building a VP."
This paper examines the very issue of voice mismatch in the above three types of ellipsis in English.,NC,4,14,NC,,FACT,,,,,,Introduction,"(3) Pseudogapping *Roses were brought by some, and others did bring lilies.",Introduction,"The next section reviews Merchant's (2007Merchant's ( , 2008) ) analysis of voice mismatch in ellipsis by postulating the functional category of Voice in the syntactic structure of a clause, and the subsequent rebuttal of Merchant's analysis by Tanaka (2011)."
"It seems clear that voice mismatch is disallowed only in some of elliptical structures like VP ellipsis, Pseudogapping and Sluicing.",NC,4,21,NC,,POS,claim tiré d'un exemple mais servant de point de départ aux auteurs,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,Consider the examples in (4) and (5).,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Unlike in the ellipsis structure of (4), voice mismatch is permissible in the non-elliptical structure of ( 5)."
"These results of the experiment show that voice mismatch in VP ellipsis is not always permissible, unlike what Merchant (2008) argues.",NC,4,71,NC,,NC,rapport détaillé des résultats d'un autre papier,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"No effect of coherence (i.e., discourse relation) is found in the big elliptical conditions (conditions (a-d) above).",No asymmetry in voice match between VP ellipsis and Pseudogapping,"Instead, discourse relations are a determining factor in ruling in or out voice mismatch in VP ellipsis."
"The conclusion drawn from the review of Merchant (2007Merchant ( , 2008) ) and Tanaka (2011) is that the former analysis based on the different sizes of ellipsis for VP ellipsis and Pseudogapping overgenerates and under-generates.",NC,4,73,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Instead, discourse relations are a determining factor in ruling in or out voice mismatch in VP ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"It over-predicts that all the examples involving mismatch in VP are acceptable, and at the same time it cannot predict that some of those involving voice mismatch in VP ellipsis are unacceptable."
"It over-predicts that all the examples involving mismatch in VP are acceptable, and at the same time it cannot predict that some of those involving voice mismatch in VP ellipsis are unacceptable.",NC,4,74,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"The conclusion drawn from the review of Merchant (2007Merchant ( , 2008) ) and Tanaka (2011) is that the former analysis based on the different sizes of ellipsis for VP ellipsis and Pseudogapping overgenerates and under-generates.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"In the next section, building on Kehler's (2000) insight into discourse relations between ellipsis and antecedent clauses, we argue that sizes of ellipsis for both VP ellipsis and Pseudogapping interact with such discourse relations."
"We depart from Kehler (2000), suggesting that a cause/effect relation as well as a resemblance relation requires syntactic identity in ellipsis, but that they are distinguished in terms of the category that undergoes ellipsis.",NC,4,99,NC,,POS,claim principal d'une argumentation qui continue sur plusieurs lignes ... est-ce qu'on garde seulement le claim initial ?,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"(Kehler 2000: 551, example 34) Kehler (2000: 543-46) ascribes this contrast to the fact that cause/effect relations require only semantic identity, which tolerates voice mismatch, while resemblance relations require syntactic identity in addition to semantic identity.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis."
"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis.",NC,4,100,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"We depart from Kehler (2000), suggesting that a cause/effect relation as well as a resemblance relation requires syntactic identity in ellipsis, but that they are distinguished in terms of the category that undergoes ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"By contrast, when a causeeffect relation holds, the smaller category VP can be elided, as schematized below: (28) a. vP ellipsis in ""parallel resemblance (or contrast) relations"" [ TP < vP [ VP ] >]... [ TP [ vP [ VP ] ]] b. VP ellipsis in ""non-parallel cause-effect relations"" [ TP [ vP < VP > ]]... [ TP [ vP [ VP ] ]] The difference between the two types of relations in terms of the category of ellipsis is justified on the basis of the following reasoning."
"By contrast, when a causeeffect relation holds, the smaller category VP can be elided, as schematized below: (28) a. vP ellipsis in ""parallel resemblance (or contrast) relations"" [ TP < vP [ VP ] >]... [ TP [ vP [ VP ] ]] b. VP ellipsis in ""non-parallel cause-effect relations"" [ TP [ vP < VP > ]]... [ TP [ vP [ VP ] ]] The difference between the two types of relations in terms of the category of ellipsis is justified on the basis of the following reasoning.",NC,4,101,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"In particular, when a resemblance relation holds, the bigger category vP is a target of ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"First, a parallel resemblance relation relates two clauses/sentences; the ellipsis clause and its antecedent clause."
The ideas we rely on are summarized below: (29) Identity condition on VP or vP ellipsis: a. Case/case mismatch (between the copy of the survivor/remnant and its correlate) is not allowed for ellipsis (as part of syntactic isomorphism in ellipsis).,NC,4,115,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Given the asymmetry between resemblance and cause/effect relations in terms of the size of ellipsis, we are now in a position to account for their contrast in voice mismatch when a verbal domain (VP or vP) undergoes ellipsis.",No asymmetry in voice match between VP ellipsis and Pseudogapping,"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP."
"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP.",NC,4,116,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,The ideas we rely on are summarized below: (29) Identity condition on VP or vP ellipsis: a. Case/case mismatch (between the copy of the survivor/remnant and its correlate) is not allowed for ellipsis (as part of syntactic isomorphism in ellipsis).,No asymmetry in voice match between VP ellipsis and Pseudogapping,c. vP undergoes 'VP ellipsis' in a resemblance relation.
c. vP undergoes 'VP ellipsis' in a resemblance relation.,NC,4,117,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,"b. Nominative and Accusative Case are checked outside VP, whereas inherent case is checked inside VP.",No asymmetry in voice match between VP ellipsis and Pseudogapping,The key ingredient we rely on in this analysis is Case/case (mis)match in ellipsis.
The key ingredient we rely on in this analysis is Case/case (mis)match in ellipsis.,NC,4,118,NC,,POS,,,,,,No asymmetry in voice match between VP ellipsis and Pseudogapping,c. vP undergoes 'VP ellipsis' in a resemblance relation.,No asymmetry in voice match between VP ellipsis and Pseudogapping,"Simply stated, Case/case mismatch is not allowed between a survivor/remnant and its antecedent constituent (or correlate)."
"In other words, voice mismatch for vP ellipsis in a resemblance relation is not permissible, because it always invites Case mismatch between an object element and its corresponding subject or vice versus, ultimately infringing on the syntactic isomorphism on ellipsis.",NC,4,135,NC,,POS,,,,,,(30) ..[ antecedent constituent A' ] ...[ ellipsis constituent,"Therefore, there is bound to arise a Case mismatch in both Pseudogapping and VP ellipsis in a resemblance relation that holds for (33) and (34).",(30) ..[ antecedent constituent A' ] ...[ ellipsis constituent,We now turn to the examples where a VPinternal element is assigned not structural Case but inherent case.
"We saw that passive-active alternation (i.e., voice mismatch) in the antecedent and ellipsis pair is permissible in a cause/effect relation.",POS,4,158,POS,,NC,,,,,,Consequences,"Chomsky (1995) Why is there a contrast between passives, on the one hand, and unaccusatives and middles, on the other hand?",Consequences,"However, neither causative-unaccusative nor transitive-middle alternation in the antecedent and ellipsis pair is allowed."
"However, neither causative-unaccusative nor transitive-middle alternation in the antecedent and ellipsis pair is allowed.",POS,4,159,POS,,NC,,,,,,Consequences,"We saw that passive-active alternation (i.e., voice mismatch) in the antecedent and ellipsis pair is permissible in a cause/effect relation.",Consequences,"We suggest on the basis of the following do so replacement that in English, passives involve syntactic movement, but neither unaccusatives nor middles do so."
"This account implies that passive verbs are potentially transitive verbs, thus being able to meet the identity condition on ellipsis with transitive verbs.",POS,4,166,POS,,NC,,,,,,Consequences,"Stroik (2001), among others) cannot replace a VP that contains a gap left behind by A or A'-movement.",Consequences,"However, unaccusative and middle verbs are in fact intransitive verbs, thus not being able to meet the identity condition on ellipsis with causative or transitive verbs."
"However, unaccusative and middle verbs are in fact intransitive verbs, thus not being able to meet the identity condition on ellipsis with causative or transitive verbs.",POS,4,167,POS,,NC,,,,,,Consequences,"This account implies that passive verbs are potentially transitive verbs, thus being able to meet the identity condition on ellipsis with transitive verbs.",Consequences,"This is how we account for the unacceptability of ( 38), (42), and (43)."
We took Tanaka's rebuttal of Merchant's dichotomy in voice mismatch between VP and Pseudogapping to be valid.,NC,4,177,NC,,POS,,,,,,Conclusion,"In this paper, we first started with reviewing Merchant's (2008) analysis of voice mismatch in ellipsis constructions and Tanaka's (2011) reply to this analysis.",Conclusion,"Departing from Kehler's (2000) insight that the distinction between resemblance vs. cause/effect discourse coherence relations rather than between VP and Pseudogapping come into place in apparent voice mismatch, we argued that VP undergoes ellipsis in a resemblance relation, whereas vP does so in a cause/effect relation."
"On the basis, we employ a recurrent network to eliminate the fakes.",NC,5,3,NC,,FACT,,,,,,abstract,"In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features.",abstract,Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.
"To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy.",POS,5,18,POS,,FACT,,,,,,Introduction,"However, it is difficult to determine which words are pseudo-related in a specific case, and when they will ""jump out"" to mislead the generation of latent features during testing.",Introduction,"In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network Figure 1: Self-regulated learning scheme is equipped with a memory suppressor to eliminate the fakes."
"In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network Figure 1: Self-regulated learning scheme is equipped with a memory suppressor to eliminate the fakes.",NC,5,19,NC,,FACT,,,,,,Introduction,"To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy.",Introduction,"Detailed experiments on event detection show that our proposed method achieves a substantial performance gain, and is capable of robust domain adaptation."
"To be honest, the models use two different kinds of recurrent units.",NC,5,126,NC,,NEG,,,,,,Trigger identification,This is proven by the fact that SELF (Bi-LSTM+GAN) outperforms Nguyen et al (2016)'s Bi-RNN.,Trigger identification,"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM."
"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM.",NC,5,127,NC,,NEG,,,,,,Trigger identification,"To be honest, the models use two different kinds of recurrent units.",Trigger identification,"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015)."
"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015).",POS,5,128,POS,,NC,,,,,,Trigger identification,"Bi-RNN uses GRUs, but SELF uses the units that possess LSTM.",Trigger identification,This allows a fair comparison between Bi-RNN and SELF.
This allows a fair comparison between Bi-RNN and SELF.,NC,5,129,NC,,POS,,,,,,Trigger identification,"Nevertheless, GRU has been experimentally proven to be comparable in performance to LSTM (Chung et al., 2014;Jozefowicz et al., 2015).",Event classification,Table 2 shows the performance of multi-class classification.
"SELF inherits the merits of the RNN models, classifying the events with higher recall.",NC,5,139,NC,,POS,,,,,,Event classification,"Let us turn to the structurally more complicated models, SELF and Hybrid.",Event classification,"Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features."
"Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features.",NC,5,140,NC,,POS,,,,,,Event classification,"SELF inherits the merits of the RNN models, classifying the events with higher recall.",Event classification,"So that it outperforms other RNNs, with improvements of no less than 4.5% precision and 1.7% recall."
"However it is built using a single-channel architecture, concatenating the RNN and the CNN.",NC,5,146,NC,,NEG,,,,,,Event classification,Multi-angled cognition enables Hybrid to be more precise.,Event classification,"This results in twofold accumulation of feature information, causing a serious overfitting problem."
"This results in twofold accumulation of feature information, causing a serious overfitting problem.",POS,5,147,POS,,NEG,,,,,,Event classification,"However it is built using a single-channel architecture, concatenating the RNN and the CNN.",Event classification,"Therefore, Hybrid is localized to much higher precision but substantially lower recall."
"For Hybrid, as illustrated in  Figure 2, the gap becomes much wider (from 9% to 19.7%) when the binary classification task (trigger identification) is shifted to multi-class classification (event detection).",POS,5,150,POS,,NEG,,,,,,Event classification,Overfitting results in enlargement of the gap between precision and recall when the task changes to be more difficult.,Event classification,"By contrast, other work shows a nearly constant gap."
"By contrast, other work shows a nearly constant gap.",NC,5,151,NC,,NEG,,,,,,Event classification,"For Hybrid, as illustrated in  Figure 2, the gap becomes much wider (from 9% to 19.7%) when the binary classification task (trigger identification) is shifted to multi-class classification (event detection).",Event classification,"In particular, SELF yields a minimum gap in each task, which changes negligibly from 3.5% to 3.4%."
"However, SELF fails to recall the pronouns that act as a trigger.",POS,5,202,POS,,NEG,,,,,,Recall and Missing,We observe that Bi-RNN and Hybrid seldom pick them up.,Recall and Missing,This is because they occur in spoken language much more frequently than they occur in written language.
This is because they occur in spoken language much more frequently than they occur in written language.,NC,5,203,NC,,POS,,,,,,Recall and Missing,"However, SELF fails to recall the pronouns that act as a trigger.",Recall and Missing,The lack of narrative content makes it difficult to learn the relationship between the pronouns and the events.
The lack of narrative content makes it difficult to learn the relationship between the pronouns and the events.,NC,5,204,NC,,NEG,,,,,,Recall and Missing,This is because they occur in spoken language much more frequently than they occur in written language.,Recall and Missing,Some real examples collected from ACE are shown in Table 7.
We use a self-regulated learning approach to improve event detection.,NC,5,220,NC,,FACT,,,,,,Related Work,"We follow the work to create spurious features, but use them to regulate the self-learning process in a single-task situation.",Conclusion,"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space."
"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space.",NC,5,221,NC,,FACT,,,,,,Conclusion,We use a self-regulated learning approach to improve event detection.,Conclusion,"In this study, the performance of the discriminator in the adversarial network is left to be evaluated."
"In this study, the performance of the discriminator in the adversarial network is left to be evaluated.",PROSP,5,222,PROSP,,NEG,,,,,,Conclusion,"In the learning process, the adversarial and cooperative models are utilized in decontaminating the latent feature space.",Conclusion,"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition."
"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition.",PROSP,5,223,PROSP,,POS,,,,,,Conclusion,"In this study, the performance of the discriminator in the adversarial network is left to be evaluated.",Conclusion,"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other."
"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other.",PROSP,5,224,PROSP,,POS,,,,,,Conclusion,"Most probably, the discriminator also performs well because it is gradually enhanced by fierce competition.",Conclusion,"Besides, the global features extracted in Li et al (2013)'s work are potentially useful for detecting the event instances referred by pronouns, although involve noises."
"Besides, the global features extracted in Li et al (2013)'s work are potentially useful for detecting the event instances referred by pronouns, although involve noises.",PROSP,5,225,PROSP,,NC,,,,,,Conclusion,"Considering this possibility, we suggest to drive the two discriminators in our self-regulation framework to cooperate with each other.",Conclusion,"Therefore, in the future, we will encode the global information by neural networks and use the self-regulation strategy to reduce the negative influence of noises."
"We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling.",FACT,6,3,FACT#POS,,FACT,,,,,,abstract,Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations.,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.
These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.,NC,6,5,NC,,POS,,,,,,abstract,Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures.,Introduction,The need for effective regularization methods for RNNs has seen extensive focus in recent years.
"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",NC,6,25,NC,,FACT,,,,,,Introduction,"Norm stabilization (Krueger & Memisevic, 2015) penalizes the model when the norm of an RNN's hidden state changes substantially between timesteps, achieving strong results in character language modeling on and phoneme recognition.",Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results."
"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",NC,6,26,NC,,POS,,,,,,Introduction,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model."
"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",NC,6,27,NC,,POS,,,,,,Introduction,"When applied to modern baselines that do not contain recurrent dropout or normalization techniques, AR and TAR achieve comparable or superior results.",Compared to other invasive regularization techniques,This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.
This enables AR and TAR to be applied to optimized RNN implementations such as the cuDNN LSTM which can be many times faster than naïve but flexible LSTM implementations.,NC,6,28,NC,,POS,,,,,,Compared to other invasive regularization techniques,"which may require modifications to the RNN cell itself or complex model changes, both AR and TAR require no substantial modifications to the RNN or model.",Activation Regularization,"L 2 activation regularization (AR) While L 2 regularization is traditionally used on the weights of machine learning models (L 2 weight decay), it could also be used on the activations."
"To understand the potential of AR and TAR, we investigate their impact on language model perplexity when used independently in Table 1 (AR) and Table 2 (TAR).",NC,6,56,NC,,FACT,,,,,,Model,"All models use weight tying between the embedding and softmax layer (Inan et al., 2016;Press & Wolf, 2016).",Evaluating AR and TAR independently on PTB:,"While both result in a substantial reduction in perplexity, AR results in the strongest improvement of 5.3, while TAR only achieves 4.3."
"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",POS,6,65,NC#POS,,POS,,,,,,Evaluating AR and TAR independently on PTB:,Comparing to state-of-the-art PTB: In Table 5 we summarize the current state of the art models in language modeling over the Penn Treebank.,Evaluating AR and TAR independently on PTB:,"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation."
"Although the LSTM uses twice as many parameters, the RHN runs a cell 10 times per timestep (referred to as recurrence depth), resulting in far more computation.",POS,6,66,POS,,NC,,,,,,Evaluating AR and TAR independently on PTB:,"The largest LSTM we train (h = 1500) achieves comparable results to the Recurrent Highway Network (RHN) (Zilly et al., 2016), a human developed custom RNN architecture, but with approximately double the number of parameters.",Evaluating AR and TAR independently on PTB:,"This would likely result in the RHN being slower than the larger LSTM model during both training and prediction, especially when factoring in optimized LSTM implementations such as NVIDIA's cuDNN LSTM."
It is possible that a large contributor to their improved result was in these tuned hyperparameters as they did not compare their NAS cell results to a standard or variational LSTM cell that was subjected to the same extensive hyperparameter search.,POS,6,70,POS,,NC,,,,,,Evaluating AR and TAR independently on PTB:,"While Zoph & Le (2016) do not report any of the hyperparameters or what type of dropout they used for their Penn Treebank result, they do note that they performed an extensive hyperparameter search over learning rate, weight initialization, dropout rates, and decay epoch in order to produce their best performing model.",Evaluating AR and TAR independently on PTB:,"Our largest LSTM results are 3 perplexity higher in comparison but have not undergone extensive hyperparameter search, do not use additional regularization techniques such as recurrent or embedding dropout, and do not use a custom RNN cell."
"While simple to implement, activity regularization and temporal activity regularization are com-",NC,6,89,NC,phrase incomplète,NC,,,,,,Conclusion,"In this work, we revisit L 2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR).",Sample generated text,"For generating text samples, words were sampled using the standard generation script contained in the PyTorch word level language modeling example."
Unsupervised parsing solutions are simultaneously an attractive yet troublesome method for handling low-data scenarios.,POS,7,5,POS,,NC,,,,,,abstract,"With less than 24 hours of total annotation, we obtain 7% and 17% absolute improvement in unlabeled dependency scores for English and Spanish, respectively, compared to the same parser using only universal grammar constraints.",Introduction,"The performance of unsupervised parsers has increased dramatically in recent years (Klein and Manning, 2004;Naseem et al., 2010), making them a potentially viable option for constructing labeled corpora on limited budgets."
"Allowing partial annotations dramatically increases the speed at which annotators can work, while simultaneously reducing error rates.",POS,7,31,POS,,NC,,,,,,Graph Fragment Language,Many words and phrases are underspecified.,Graph Fragment Language,These two effects both arise from being able to leave difficult or tedious portions of a sentence unspecified.
These two effects both arise from being able to leave difficult or tedious portions of a sentence unspecified.,POS,7,32,POS,,NC,,,,,,Graph Fragment Language,"Allowing partial annotations dramatically increases the speed at which annotators can work, while simultaneously reducing error rates.",Filling in Partial Dependencies,A partial annotation produces a set of dependency tree fragments.
"Indeed, Fill+Parse method produces better results for our datasets than Fill-then-Parse (see Section 4.2).",NC,7,41,NC,,POS,,,,,,Filling in Partial Dependencies,Fill-then-Parse obscures this distinction and not knowing how trustworthy an arc is can lead to additional errors.,Simulated Cost Comparison,Many factors influence the cost of creating a corpus.
"The actual cost of finding and paying annotators is the most obvious factor, and it will typically be higher for a lowresource language or highly specialized domain.",POS,7,44,POS,,NC,,,,,,Simulated Cost Comparison,Our goal is to minimize cost relative to the performance of a parser trained with the corpus.,Simulated Cost Comparison,Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.
Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.,PROSP,7,45,PROSP,,NC,,,,,,Simulated Cost Comparison,"The actual cost of finding and paying annotators is the most obvious factor, and it will typically be higher for a lowresource language or highly specialized domain.",Simulated Cost Comparison,"Given a partial annotation scheme like GFL, an additional cost factor is that of obtaining a particular level of completion for each sentence."
"Given a partial annotation scheme like GFL, an additional cost factor is that of obtaining a particular level of completion for each sentence.",POS,7,46,POS,,NC,,,,,,Simulated Cost Comparison,Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.,Simulated Cost Comparison,"Consider that for any sentence there are both 'low-hanging fruit' dependencies such as determiner attachment, and more difficult dependencies such as preposition attachment and long-distance relations."
"To this end, we trained our own POS taggers using type label annotations (Garrette and Baldridge, 2013)  We trained taggers for all languages using a limited amount of the available gold data-ensuring that the accuracy is comparable with low-resource human-sourced taggers.",FACT,7,84,FACT,,NC,,,,,,POS-Tagging,Our goal is to minimize real-world costs associated with producing a finished parsing model.,POS-Tagging,"We extract types from the corpus, rank them by frequency, and take the most frequent types to train the tagger."
Results based on actual annotation are the most important as they provide our best measure of performance under a realistic annotation setting.,POS,7,115,POS,,NC,,,,,,Experiments and Discussion,We consider both simulated and actual partial annotations.,Experiments and Discussion,"However, our Spanish annotators had only six hours each, and there was no inter-annotator communication or creation of annotation conventions, and no attempt to have them adopt the conventions in the gold-standard AnCora dependencies we evaluate against."
"To investigate the impact of POS taggers on parsing results, we conducted two series of experiments using POS tags trained by our own tagger as discussed in Section 2.6 (Predicted Tag) and gold POS tags extracted from treebank (Gold Tag).",FACT,7,134,FACT,,NC,,,,,,Annotator-sourced partial dependencies,Table 5 gives semi-supervised parsing results on the English and Spanish treebanks for sentences with 10 or fewer words.,Annotator-sourced partial dependencies,We compare against a right-branching baseline and the Gibbs parser of Mielens et al.
All the parsing methods handily beat the rightbranching baseline.,NC,7,136,NC,,POS,,,,,,Annotator-sourced partial dependencies,We compare against a right-branching baseline and the Gibbs parser of Mielens et al.,Annotator-sourced partial dependencies,"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags."
"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags.",NC,7,137,NC,,POS,,,,,,Annotator-sourced partial dependencies,All the parsing methods handily beat the rightbranching baseline.,Annotator-sourced partial dependencies,"This shows the effectiveness of ConvexMST, but highlights its brittleness with respect to tagging errors: bad tags lead to poor guidance from language universals."
"This shows the effectiveness of ConvexMST, but highlights its brittleness with respect to tagging errors: bad tags lead to poor guidance from language universals.",POS,7,138,POS,,NEG#POS,,,,,,Annotator-sourced partial dependencies,"ConvexMST-UG (the model of Grave and Elhadad ( 2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags.",Annotator-sourced partial dependencies,ConvexMST-GFL easily beats both these approaches: it exploits partial annotations much more effectively than the Gibbs parser and learns effectively without language universals.
"Our goal was to understand the overall behavior of different methods given the same free-wheeling, diverse annotations; it is likely that higher numbers would have been achieved had we guided annotators to use corpus conventions, or used full annotations provided by our annotators as the evaluation set.",PROSP,7,146,PROSP,,POS,,,,,,Annotator-sourced partial dependencies,It is important to recall that the GFL annotations have no specific conformity to the gold standards of either original corpus.,Annotator-sourced partial dependencies,"The former defeats the spirit of our exercise, and we did not have sufficient budget for the latter."
"(Again, keep in mind that we are considering a ""cold start"" to this process, so there can be no gold standard for checking annotator quality.)",NC,7,154,NC,,NEG,,,,,,Annotator-sourced partial dependencies,The next obvious steps would be to use active learning and to detect disagreement in annotators to either drop some or intervene improve their quality.,Annotator-sourced partial dependencies,"Comparison to Full Annotation To this point, all performance comparisons have been between different parse feature sets; we have demonstrated that the GFL features are complimentary to the UG features, and that when standing alone the GFL features are stronger than the UG features."
"However, the theoretical wall clock time of the group of annotators could be as low as two hours if the sessions were run in parallel.",NEG,7,160,NEG,,POS,,,,,,Annotator-sourced partial dependencies,"In comparison, the other annotators were able to partially annotate the same section in roughly two hours each -a total of 24 hours.",Annotator-sourced partial dependencies,These different training sets were once again used to train ConvexMST models that were evaluated on a held out test set.
"In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario).",PROSP,7,164,PROSP,,NEG,,,,,,Annotator-sourced partial dependencies,It should be noted that this comparison does not weight the results using the extrinsic costs associated with the production of the training data.,Annotator-sourced partial dependencies,This makes the performance per unit cost for partial annotators even higher than of these extrinsic cost effects.
This makes the performance per unit cost for partial annotators even higher than of these extrinsic cost effects.,POS,7,165,POS,,NEG,,,,,,Annotator-sourced partial dependencies,"In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario).",Longer Sentences,We also evaluated ConvexMST with longer sentences: those with 20 words or less.
"For this, the right-branching baseline is 25.8%.",POS,7,167,POS,,NC,,,,,,Longer Sentences,We also evaluated ConvexMST with longer sentences: those with 20 words or less.,Longer Sentences,"When using all the annotations on the common set for all annotators, the scores for ConvexMST with UG, GFL, and GFL+UG are 47.6%, 54.4%, and 55.3%, respectively."
POS-Tagging Impact We thought it important to consider imperfect POS-taggings because this entire framework is based off of the assumption that the user is working from essentially no pre-existing resources.,POS,7,170,POS,,NC,,,,,,Longer Sentences,"The values are worse than for shorter sentences, as expected, but the pattern observed in Table 5 still holds: GFL annotations best UG alone, and their combination is the best of all.",Discussion & Error Analysis,"Assuming the availability of gold-standard POS tags is antithetical to this idea, and is one way in which direct supervision can show up in otherwise unsupervised (or indirectly supervised) systems."
"However, more unlikely errors can cause more dramatic effects, as shown in Figure 6.",POS,7,173,POS,,NEG,,,,,,Discussion & Error Analysis,"Many tagger errors are not likely to cause major problems during parsing; for instance mislabeling pronouns as nouns, or adverbs as adjectives, is unlikely to lead to major structural issues.",Discussion & Error Analysis,"Here, the phrase 'beating politically' (gold tags 'NOUN ADV') is mistagged as 'ADJ VERB', leading to the attachment of 'politically' to the root word and the reorganization of a substantial chunk of the sentence."
"Weighting Constraint Violations For feature sets with both GFL and UG-based constraints, a weighting factor can bias the parser towards being more likely to respect either GFL or UG constraints.",POS,7,175,POS,,NEG,,,,,,Discussion & Error Analysis,"Here, the phrase 'beating politically' (gold tags 'NOUN ADV') is mistagged as 'ADJ VERB', leading to the attachment of 'politically' to the root word and the reorganization of a substantial chunk of the sentence.",Discussion & Error Analysis,"We experimented with this, and found that for the  datasets we considered, the best results were obtained when we weighted violations of GFL constraints as worse than violations of UG constraints."
"We experimented with this, and found that for the  datasets we considered, the best results were obtained when we weighted violations of GFL constraints as worse than violations of UG constraints.",POS,7,176,FACT#POS,,POS,,,,,,Discussion & Error Analysis,"Weighting Constraint Violations For feature sets with both GFL and UG-based constraints, a weighting factor can bias the parser towards being more likely to respect either GFL or UG constraints.",Discussion & Error Analysis,"This result is not entirely unexpected given the relative performances of the constraints on their own, but it provides more evidence that direct supervision even in small amounts can beat indirect supervision."
"We demonstrate this with actual annotations produced for English and Spanish, using annotators with a range of experience.",POS,7,180,POS,,FACT,,,,,,5,"The ConvexMST method we adapt from Grave and Elhadad easily combines constraints from both language universals and partial annotations, providing greater robustness from starting annotation until one runs out of budget or time.",5,"Overall, we present a case for working in realistic settings by paying close attention the various sources of annotation and tracking the real costs associated with that supervision."
"Overall, we present a case for working in realistic settings by paying close attention the various sources of annotation and tracking the real costs associated with that supervision.",POS,7,181,POS,,FACT,,,,,,5,"We demonstrate this with actual annotations produced for English and Spanish, using annotators with a range of experience.",5,"We believe that overreliance on creeping supervision of this type may lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models, and are encouraged by recent work on character-based models by Gillick et al."
"This approach has the advantage that it does not require explicit feature engineering, alignments, and a sound transition matrix.",FACT,8,40,FACT,,NC,should we consider it a fact ?,,,,,Convolutional networks,This article is the first to apply convolutional networks (ConvNets) to phonemes by treating each phoneme as a vector of binary valued phonetic features.,Convolutional networks,The approach requires cognacy statements and phonetic descriptions of sounds used to transcribe the words.
The approach requires cognacy statements and phonetic descriptions of sounds used to transcribe the words.,FACT,8,41,FACT,,NC,,,,,,Convolutional networks,"This approach has the advantage that it does not require explicit feature engineering, alignments, and a sound transition matrix.",Convolutional networks,The cognacy statements can be obtained from etymological dictionaries and the quality of the phonemes can be obtained from Ladefoged and Maddieson (1998).
The Manhattan ConvNet competes with PMI and orthographic models at cross-concept cognate identification task.,NC,8,141,NC,,POS,,,,,,Cross-Family experiments,The results of our experiments are given in table 5,Discussion,The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all the three language families.
The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all the three language families.,NC,8,142,NC,,POS,,,,,,Discussion,The Manhattan ConvNet competes with PMI and orthographic models at cross-concept cognate identification task.,Discussion,"In terms of averaged F-scores, Manhattan ConvNet performs slightly better than orthographic model and only performs worse than the other models at Austronesian language family."
The Manhattan ConvNet does not turn up as the best system across all the evaluation metrics in a single language family.,POS,8,145,POS,,POS,exemple de résultat négatif (ici catégorisé en POS),,,,,Discussion,The Manhattan ConvNet shows mixed performance at the task of cross-family cognate identification.,Discussion,The ConvNet performs better than PMI but is not as good as Orthographic measures at Indo-European language family.
"However, ConvNets do not perform as well as orthographic and PMI systems.",POS,8,152,POS,,POS,exemple de résultat négatif (ici catégorisé comme POS),,,,,Discussion,The Orthographic system and PMI system show similar performance at the Austronesian crossconcept task.,Discussion,The reason for this could be due to the differential transcriptions in the database.
The reason for this could be due to the differential transcriptions in the database.,POS,8,153,POS,,POS,explication de résultat,,,,,Discussion,"However, ConvNets do not perform as well as orthographic and PMI systems.",Conclusion,"In this article, we explored the use of phonetic feature convolutional networks for the task of pairwise cognate identification."
We are currently working towards building a larger database of word lists in IPA transcription.,PROSP,8,157,PROSP,,PROSP,PROSP mais déjà commencé dans le présent,,,,,Conclusion,"In the future, we intend to work directly with speech recordings and include language relatedness information into ConvNets to improve the performance.",,
"Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.",NC,9,4,NC,,FACT,,,,,,abstract,"In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents.",abstract,"We evaluate our method on question answering, obtaining state-of-the-art results."
"We evaluate our method on question answering, obtaining state-of-the-art results.",POS,9,5,FACT#POS,,POS,,,,,,abstract,"Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.",INTRODUCTION,"Information retrieval is an important component for many natural language processing tasks, such as question answering (Voorhees et al., 1999) or fact checking (Thorne et al., 2018)."
"Said otherwise, we assume that attention activations are a good proxy for the relevance of documents.",NC,9,27,NC,,POS,,,,,,INTRODUCTION,"More precisely, we use a sequence-to-sequence model as the reader, and use the attention activations over the input documents as synthetic labels to train the retriever.",INTRODUCTION,We then train the retriever to reproduce the ranking of documents corresponding to that metric.
"We make the following contributions: • First, we show that attention scores from a sequence-to-sequence reader model are a good measure of document relevance (Sec.",POS,9,29,FACT#POS,,POS,,,,,,INTRODUCTION,We then train the retriever to reproduce the ranking of documents corresponding to that metric.,INTRODUCTION,"3.2) ; • Second, inspired by knowledge distillation, we propose to iteratively train the retriever from these activations, and compare different loss functions (Sec."
"3.4) ; • Finally, we evaluate our method on three question-answering benchmarks, obtaining stateof-the-art results (Sec.",POS,9,31,FACT#POS,,POS,,,,,,INTRODUCTION,"3.2) ; • Second, inspired by knowledge distillation, we propose to iteratively train the retriever from these activations, and compare different loss functions (Sec.",INTRODUCTION,Our code is available at: github.com/facebookresearch/FiD.
Our code is available at: github.com/facebookresearch/FiD.,NC,9,32,NC,,FACT,,,,,,INTRODUCTION,"3.4) ; • Finally, we evaluate our method on three question-answering benchmarks, obtaining stateof-the-art results (Sec.",RELATED WORK,We briefly review information retrieval based on machine learning.
"In addition to initializing our method with documents retrieved with BM25 and BERT, we also train a system by starting from DPR documents.",FACT,9,206,FACT,,NC,,,,,,RESULTS,"In Table 2, we report the performance of our approach, as well as existing state-of-the-art systems on TriviaQA and NaturalQuestions.",RESULTS,"First, we observe that our method improve the performance over the state-of-the-art, even when starting from BM25 documents."
